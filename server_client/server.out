server side epoch loop 0
algo.train executed
agent_timesteps_total: 2
connector_metrics: {}
counters:
  num_agent_steps_sampled: 2
  num_agent_steps_trained: 2
  num_env_steps_sampled: 2
  num_env_steps_trained: 2
custom_metrics: {}
date: 2023-10-14_23-01-28
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.19999999999999993
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9429693003495534
        entropy_coeff: 0.0
        grad_gnorm: 15.387430667877197
        kl: 0.0028251249900677067
        policy_loss: -0.11404707233111064
        total_loss: 1.7188353180885314
        vf_explained_var: .nan
        vf_loss: 1.8323173627257348
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 30.5
  num_agent_steps_sampled: 2
  num_agent_steps_trained: 2
  num_env_steps_sampled: 2
  num_env_steps_trained: 2
iterations_since_restore: 1
node_ip: 10.27.41.23
num_agent_steps_sampled: 2
num_agent_steps_trained: 2
num_env_steps_sampled: 2
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005565758134559973
num_env_steps_trained: 2
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005565758134559973
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.822027290448345
  ram_util_percent: 45.88226120857699
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 359.3403694629669
time_this_iter_s: 359.3403694629669
time_total_s: 359.3403694629669
timers:
  learn_throughput: 3.883
  learn_time_ms: 515.021
  load_throughput: 6150.006
  load_time_ms: 0.325
  sample_time_ms: 358823.814
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 359340.034
timestamp: 1697313688
timesteps_total: 2
training_iteration: 1
trial_id: default

Last checkpoint 0 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000001
server side epoch loop 1
algo.train executed
agent_timesteps_total: 4
connector_metrics: {}
counters:
  num_agent_steps_sampled: 4
  num_agent_steps_trained: 4
  num_env_steps_sampled: 4
  num_env_steps_trained: 4
custom_metrics: {}
date: 2023-10-14_23-07-28
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.09999999999999996
        cur_lr: 5.0000000000000016e-05
        entropy: 1.924806179602941
        entropy_coeff: 0.0
        grad_gnorm: 17.309453399976096
        kl: 0.011831115215318277
        policy_loss: -0.17218573292096456
        total_loss: 1.3274928192297617
        vf_explained_var: .nan
        vf_loss: 1.4984954406817754
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 90.5
  num_agent_steps_sampled: 4
  num_agent_steps_trained: 4
  num_env_steps_sampled: 4
  num_env_steps_trained: 4
iterations_since_restore: 2
node_ip: 10.27.41.23
num_agent_steps_sampled: 4
num_agent_steps_trained: 4
num_env_steps_sampled: 4
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0055631852148056065
num_env_steps_trained: 4
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0055631852148056065
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 12.682651072124758
  ram_util_percent: 47.180701754385964
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 718.8468573093414
time_this_iter_s: 359.5064878463745
time_total_s: 718.8468573093414
timers:
  learn_throughput: 3.748
  learn_time_ms: 533.634
  load_throughput: 5270.882
  load_time_ms: 0.379
  sample_time_ms: 358888.407
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 359423.14
timestamp: 1697314048
timesteps_total: 4
training_iteration: 2
trial_id: default

Last checkpoint 1 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000002
server side epoch loop 2
algo.train executed
agent_timesteps_total: 6
connector_metrics: {}
counters:
  num_agent_steps_sampled: 6
  num_agent_steps_trained: 6
  num_env_steps_sampled: 6
  num_env_steps_trained: 6
custom_metrics: {}
date: 2023-10-14_23-13-28
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.09999999999999996
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9373939434687297
        entropy_coeff: 0.0
        grad_gnorm: 19.130591042836507
        kl: 0.006546449134960616
        policy_loss: -0.11774353583653768
        total_loss: 1.395007484142358
        vf_explained_var: .nan
        vf_loss: 1.512096364547809
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 150.5
  num_agent_steps_sampled: 6
  num_agent_steps_trained: 6
  num_env_steps_sampled: 6
  num_env_steps_trained: 6
iterations_since_restore: 3
node_ip: 10.27.41.23
num_agent_steps_sampled: 6
num_agent_steps_trained: 6
num_env_steps_sampled: 6
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0055565867785149214
num_env_steps_trained: 6
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0055565867785149214
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.678793774319065
  ram_util_percent: 47.1568093385214
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 1078.7802476882935
time_this_iter_s: 359.933390378952
time_total_s: 1078.7802476882935
timers:
  learn_throughput: 3.806
  learn_time_ms: 525.45
  load_throughput: 5070.688
  load_time_ms: 0.394
  sample_time_ms: 359066.617
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 359593.144
timestamp: 1697314408
timesteps_total: 6
training_iteration: 3
trial_id: default

Last checkpoint 2 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000003
server side epoch loop 3
algo.train executed
agent_timesteps_total: 8
connector_metrics: {}
counters:
  num_agent_steps_sampled: 8
  num_agent_steps_trained: 8
  num_env_steps_sampled: 8
  num_env_steps_trained: 8
custom_metrics: {}
date: 2023-10-14_23-18-28
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.09999999999999996
        cur_lr: 5.0000000000000016e-05
        entropy: 1.937092504898707
        entropy_coeff: 0.0
        grad_gnorm: 13.285435231526693
        kl: 0.01419847554085815
        policy_loss: -0.1532023-10-14 23:21:27,772	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:21:27] "POST / HTTP/1.1" 200 -
2023-10-14 23:22:28,103	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:22:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:22:28,140	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:22:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:23:28,401	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:23:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:24:28,669	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:24:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:24:28,700	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:24:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:27:28,419	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:27:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:28:28,731	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:28:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:28:28,768	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:28:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:30:27,799	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:30:27] "POST / HTTP/1.1" 200 -
2023-10-14 23:31:28,144	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:31:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:31:28,173	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:31:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:34:28,675	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:34:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:37:28,353	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:37:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:37:28,389	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:37:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:40:28,492	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:40:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:41:28,804	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:41:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:41:28,837	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:41:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:43:27,671	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:43:27] "POST / HTTP/1.1" 200 -
2023-10-14 23:45:28,395	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:45:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:45:28,431	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:45:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:46:28,754	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:46:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:49:28,480	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:49:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:49:28,510	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:49:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:51:28,603	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:51:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:52:28,920	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:52:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:52:28,968	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:52:28] "POST / HTTP/1.1" 200 -
2023-10-14 23:53:29,315	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:53:29] "POST / HTTP/1.1" 200 -
2023-10-14 23:54:29,603	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:54:29] "POST / HTTP/1.1" 200 -
2023-10-14 23:54:29,643	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [14/Oct/2023 23:54:29] "POST / HTTP/1.1" 200 -
2023-10-14 23:57:28,005	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [14/Oct/2023 23:57:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:00:27,971	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:00:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:00:28,023	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:00:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:01:28,273	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:01:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:03:28,426	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:03:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:03:28,460	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:03:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:04:28,824	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:04:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:07:27,965	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:07:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:07:28,002	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:07:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:10:27,850	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:10:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:13:28,335	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:13:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:13:28,381	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:13:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:14:28,734	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:14:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:16:27,606	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:16:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:16:27,639	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:16:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:19:28,433	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:19:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:20:28,755	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:20:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:20:28,790	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:20:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:21:29,175	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:21:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:24:27,939	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:24:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:24:27,977	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:24:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:25:28,257	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:25:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:28:27,894	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:28:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:28:27,930	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:28:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:31:27,671	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:31:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:32:28,060	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:32:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:32:28,101	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:32:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:33:28,423	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:33:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:34:28,791	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:34:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:34:28,834	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:34:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:35:29,205	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:35:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:36:29,554	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:36:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:36:29,590	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:36:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:37:29,882	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:37:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:38:30,192	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:38:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:38:30,226	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:38:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:39:30,532	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:39:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:40:30,954	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:40:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:40:30,996	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:40:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:41:31,296	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:41:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:42:31,599	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:42:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:42:31,642	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:42:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:44:27,560	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:44:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:45:27,876	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:45:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:45:27,920	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:45:27] "POST / HTTP/1.1" 200 -
2023-10-15 00:46:28,214	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:46:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:47:28,506	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:47:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:47:28,536	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:47:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:48:28,809	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:48:28] "POST / HTTP/1.1" 200 -
2023-10-15 00:49:29,247	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:49:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:49:29,287	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:49:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:50:29,551	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:50:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:51:29,867	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:51:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:51:29,910	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:51:29] "POST / HTTP/1.1" 200 -
2023-10-15 00:52:30,169	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:52:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:53:30,458	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:53:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:53:30,484	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:53:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:54:30,759	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:54:30] "POST / HTTP/1.1" 200 -
2023-10-15 00:55:31,073	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:55:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:55:31,102	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:55:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:56:31,401	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:56:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:57:31,671	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:57:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:57:31,703	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:57:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:58:31,975	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:58:31] "POST / HTTP/1.1" 200 -
2023-10-15 00:59:32,190	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 00:59:32] "POST / HTTP/1.1" 200 -
2023-10-15 00:59:32,222	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 00:59:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:00:32,480	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:00:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:01:32,739	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:01:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:01:32,772	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:01:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:02:33,065	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:02:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:03:33,271	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:03:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:03:33,299	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:03:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:04:33,567	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:04:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:05:33,864	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:05:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:05:33,893	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:05:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:06:34,148	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:06:34] "POST / HTTP/1.1" 200 -
2023-10-15 01:07:34,431	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:07:34] "POST / HTTP/1.1" 200 -
2023-10-15 01:07:34,459	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:07:34] "POST / HTTP/1.1" 200 -
2023-10-15 01:08:34,791	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:08:34] "POST / HTTP/1.1" 200 -
2023-10-15 01:09:57,536	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:09:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:09:57,566	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:09:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:10:57,845	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:10:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:11:58,163	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:11:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:11:58,202	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:11:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:12:58,505	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:12:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:13:58,828	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:13:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:13:58,860	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:13:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:15:52,849	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:15:52] "POST / HTTP/1.1" 200 -
2023-10-15 01:16:57,545	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:16:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:16:57,574	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:16:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:18:27,707	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:18:27] "POST / HTTP/1.1" 200 -
2023-10-15 01:19:28,027	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:19:28] "POST / HTTP/1.1" 200 -
2023-10-15 01:19:28,058	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:19:28] "POST / HTTP/1.1" 200 -
2023-10-15 01:20:28,353	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:20:28] "POST / HTTP/1.1" 200 -
2023-10-15 01:21:28,667	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:21:28] "POST / HTTP/1.1" 200 -
2023-10-15 01:21:28,707	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:21:28] "POST / HTTP/1.1" 200 -
2023-10-15 01:22:29,023	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:22:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:23:29,334	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:23:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:23:29,364	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:23:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:24:29,729	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:24:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:25:30,054	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:25:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:25:30,097	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:25:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:26:30,398	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:26:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:27:30,740	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:27:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:27:30,778	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:27:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:28:31,080	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:28:31] "POST / HTTP/1.1" 200 -
2023-10-15 01:29:31,399	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:29:31] "POST / HTTP/1.1" 200 -
2023-10-15 01:29:31,446	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:29:31] "POST / HTTP/1.1" 200 -
2023-10-15 01:30:31,715	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:30:31] "POST / HTTP/1.1" 200 -
2023-10-15 01:31:31,990	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:31:31] "POST / HTTP/1.1" 200 -
2023-10-15 01:31:32,030	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:31:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:32:32,297	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:32:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:33:32,820	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:33:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:33:32,862	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:33:32] "POST / HTTP/1.1" 200 -
2023-10-15 01:34:33,128	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:34:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:35:33,455	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:35:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:35:33,496	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:35:33] "POST / HTTP/1.1" 200 -
2023-10-15 01:36:34,437	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:36:34] "POST / HTTP/1.1" 200 -
2023-10-15 01:37:57,551	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:37:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:37:57,581	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:37:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:38:57,858	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:38:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:39:58,154	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:39:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:39:58,184	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:39:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:40:58,529	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:40:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:41:58,853	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:41:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:41:58,894	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:41:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:42:59,260	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:42:59] "POST / HTTP/1.1" 200 -
2023-10-15 01:43:59,614	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:43:59] "POST / HTTP/1.1" 200 -
2023-10-15 01:43:59,645	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:43:59] "POST / HTTP/1.1" 200 -
2023-10-15 01:45:28,646	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:45:28] "POST / HTTP/1.1" 200 -
2023-10-15 01:46:29,736	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:46:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:46:29,764	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:46:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:47:30,112	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:47:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:48:30,464	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:48:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:48:30,504	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:48:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:49:30,840	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:49:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:50:57,403	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:50:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:50:57,442	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:50:57] "POST / HTTP/1.1" 200 -
2023-10-15 01:51:58,636	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:51:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:52:58,981	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:52:58] "POST / HTTP/1.1" 200 -
2023-10-15 01:52:59,022	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:52:59] "POST / HTTP/1.1" 200 -
2023-10-15 01:56:28,192	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:56:28] "POST / HTTP/1.1" 200 -
2023-10-15 01:57:29,640	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:57:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:57:29,680	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:57:29] "POST / HTTP/1.1" 200 -
2023-10-15 01:58:30,062	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:58:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:59:30,421	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 01:59:30] "POST / HTTP/1.1" 200 -
2023-10-15 01:59:30,455	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 01:59:30] "POST / HTTP/1.1" 200 -
2023-10-15 02:00:30,960	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:00:30] "POST / HTTP/1.1" 200 -
2023-10-15 02:02:27,911	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:02:27] "POST / HTTP/1.1" 200 -
2023-10-15 02:02:27,943	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:02:27] "POST / HTTP/1.1" 200 -
2023-10-15 02:03:28,306	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:03:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:06:57,367	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:06:57] "POST / HTTP/1.1" 200 -
2023-10-15 02:06:57,410	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:06:57] "POST / HTTP/1.1" 200 -
2023-10-15 02:07:57,730	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:07:57] "POST / HTTP/1.1" 200 -
2023-10-15 02:11:28,482	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:11:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:11:28,526	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:11:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:15:58,123	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:15:58] "POST / HTTP/1.1" 200 -
2023-10-15 02:19:28,529	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:19:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:19:28,587	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:19:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:20:28,951	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:20:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:23:28,520	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:23:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:23:28,554	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:23:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:26:28,216	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:26:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:29:28,665	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:29:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:29:28,703	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:29:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:30:29,051	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:30:29] "POST / HTTP/1.1" 200 -
2023-10-15 02:33:28,695	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:33:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:33:28,735	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:33:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:36:27,825	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:36:27] "POST / HTTP/1.1" 200 -
2023-10-15 02:37:28,351	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:37:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:37:28,383	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:37:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:38:28,657	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:38:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:41:28,238	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:41:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:41:28,274	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:41:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:42:28,552	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:42:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:43:28,834	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:43:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:43:28,870	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:43:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:44:29,196	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:44:29] "POST / HTTP/1.1" 200 -
2023-10-15 02:45:29,739	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:45:29] "POST / HTTP/1.1" 200 -
2023-10-15 02:45:29,772	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:45:29] "POST / HTTP/1.1" 200 -
2023-10-15 02:48:27,955	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:48:27] "POST / HTTP/1.1" 200 -
2023-10-15 02:51:28,392	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:51:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:51:28,425	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:51:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:53:27,659	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:53:27] "POST / HTTP/1.1" 200 -
2023-10-15 02:54:27,938	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:54:27] "POST / HTTP/1.1" 200 -
2023-10-15 02:54:27,972	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:54:27] "POST / HTTP/1.1" 200 -
2023-10-15 02:55:28,433	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:55:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:57:28,616	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:57:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:57:28,655	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:57:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:58:28,927	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:58:28] "POST / HTTP/1.1" 200 -
2023-10-15 02:59:29,203	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 02:59:29] "POST / HTTP/1.1" 200 -
2023-10-15 02:59:29,240	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 02:59:29] "POST / HTTP/1.1" 200 -
2023-10-15 03:03:27,824	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:03:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:04:28,095	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:04:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:04:28,127	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:04:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:05:28,350	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:05:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:06:28,564	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:06:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:06:28,596	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:06:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:07:28,737	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:07:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:08:28,984	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:08:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:08:29,022	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:08:29] "POST / HTTP/1.1" 200 -
2023-10-15 03:09:29,269	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:09:29] "POST / HTTP/1.1" 200 -
2023-10-15 03:10:29,518	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:10:29] "POST / HTTP/1.1" 200 -
2023-10-15 03:10:29,548	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:10:29] "POST / HTTP/1.1" 200 -
2023-10-15 03:11:29,784	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:11:29] "POST / HTTP/1.1" 200 -
2023-10-15 03:12:30,049	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:12:30] "POST / HTTP/1.1" 200 -
2023-10-15 03:12:30,082	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:12:30] "POST / HTTP/1.1" 200 -
2023-10-15 03:13:30,316	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:13:30] "POST / HTTP/1.1" 200 -
2023-10-15 03:14:30,571	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:14:30] "POST / HTTP/1.1" 200 -
2023-10-15 03:14:30,603	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:14:30] "POST / HTTP/1.1" 200 -
2023-10-15 03:15:30,872	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:15:30] "POST / HTTP/1.1" 200 -
2023-10-15 03:16:31,125	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:16:31] "POST / HTTP/1.1" 200 -
2023-10-15 03:16:31,158	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:16:31] "POST / HTTP/1.1" 200 -
2023-10-15 03:19:27,607	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:19:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:20:27,836	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:20:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:20:27,875	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:20:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:21:28,082	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:21:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:23:27,579	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:23:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:23:27,628	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:23:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:25:27,579	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:25:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:26:27,830	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:26:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:26:27,875	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:26:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:27:28,000	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:27:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:28:28,229	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:28:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:28:28,274	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:28:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:29:28,473	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:29:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:31:27,609	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:31:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:31:27,658	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:31:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:34:28,142	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:34:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:37:27,824	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:37:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:37:27,856	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:37:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:38:27,999	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:38:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:39:28,183	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:39:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:39:28,216	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:39:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:40:28,426	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:40:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:41:28,658	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:41:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:41:28,689	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:41:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:43:27,944	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:43:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:47:27,628	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:47:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:47:27,664	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:47:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:51:27,886	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:51:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:52:28,150	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:52:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:52:28,182	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:52:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:54:28,462	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:54:28] "POST / HTTP/1.1" 200 -
2023-10-15 03:57:27,691	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 03:57:27] "POST / HTTP/1.1" 200 -
2023-10-15 03:57:27,723	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 03:57:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:00:28,079	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:00:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:03:28,198	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:03:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:03:28,234	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:03:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:06:27,808	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:06:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:09:27,583	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:09:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:09:27,616	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:09:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:10:27,870	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:10:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:11:28,143	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:11:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:11:28,181	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:11:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:15:27,629	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:15:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:16:27,899	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:16:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:16:27,934	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:16:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:17:28,215	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:17:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:18:28,508	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:18:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:18:28,542	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:18:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:19:28,848	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:19:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:20:29,220	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:20:29] "POST / HTTP/1.1" 200 -
2023-10-15 04:20:29,260	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:20:29] "POST / HTTP/1.1" 200 -
2023-10-15 04:21:29,595	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:21:29] "POST / HTTP/1.1" 200 -
2023-10-15 04:22:57,849	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:22:57] "POST / HTTP/1.1" 200 -
2023-10-15 04:22:57,884	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:22:57] "POST / HTTP/1.1" 200 -
2023-10-15 04:23:58,169	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:23:58] "POST / HTTP/1.1" 200 -
2023-10-15 04:24:58,471	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:24:58] "POST / HTTP/1.1" 200 -
2023-10-15 04:24:58,503	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:24:58] "POST / HTTP/1.1" 200 -
2023-10-15 04:25:58,786	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:25:58] "POST / HTTP/1.1" 200 -
2023-10-15 04:26:59,136	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:26:59] "POST / HTTP/1.1" 200 -
2023-10-15 04:26:59,177	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:26:59] "POST / HTTP/1.1" 200 -
2023-10-15 04:27:59,484	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:27:59] "POST / HTTP/1.1" 200 -
2023-10-15 04:28:59,826	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:28:59] "POST / HTTP/1.1" 200 -
2023-10-15 04:28:59,857	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:28:59] "POST / HTTP/1.1" 200 -
2023-10-15 04:30:00,245	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:30:00] "POST / HTTP/1.1" 200 -
2023-10-15 04:31:00,565	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:31:00] "POST / HTTP/1.1" 200 -
2023-10-15 04:31:00,604	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:31:00] "POST / HTTP/1.1" 200 -
2023-10-15 04:32:27,680	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:32:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:33:27,988	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:33:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:33:28,020	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:33:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:34:28,328	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:34:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:35:28,640	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:35:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:35:28,669	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:35:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:36:28,981	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:36:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:37:29,360	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:37:29] "POST / HTTP/1.1" 200 -
2023-10-15 04:37:29,390	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:37:29] "POST / HTTP/1.1" 200 -
2023-10-15 04:38:52,332	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:38:52] "POST / HTTP/1.1" 200 -
2023-10-15 04:39:57,220	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:39:57] "POST / HTTP/1.1" 200 -
2023-10-15 04:39:57,252	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:39:57] "POST / HTTP/1.1" 200 -
2023-10-15 04:40:57,596	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:40:57] "POST / HTTP/1.1" 200 -
2023-10-15 04:43:28,337	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:43:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:43:28,371	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:43:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:44:29,151	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:44:29] "POST / HTTP/1.1" 200 -
2023-10-15 04:47:27,851	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:47:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:47:27,884	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:47:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:50:28,642	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:50:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:53:27,821	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:53:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:53:27,854	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:53:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:55:27,685	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:55:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:56:27,992	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:56:27] "POST / HTTP/1.1" 200 -
2023-10-15 04:56:28,043	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 04:56:28] "POST / HTTP/1.1" 200 -
2023-10-15 04:58:28,606	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 04:58:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:01:27,645	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:01:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:01:27,685	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:01:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:02:28,060	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:02:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:05:28,048	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:05:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:05:28,084	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:05:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:08:28,672	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:08:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:09:29,438	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:09:29] "POST / HTTP/1.1" 200 -
2023-10-15 05:09:29,484	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:09:29] "POST / HTTP/1.1" 200 -
2023-10-15 05:12:27,803	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:12:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:15:28,645	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:15:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:15:28,683	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:15:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:16:28,997	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:16:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:17:29,315	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:17:29] "POST / HTTP/1.1" 200 -
2023-10-15 05:17:29,347	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:17:29] "POST / HTTP/1.1" 200 -
2023-10-15 05:18:30,059	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:18:30] "POST / HTTP/1.1" 200 -
2023-10-15 05:19:30,356	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:19:30] "POST / HTTP/1.1" 200 -
2023-10-15 05:19:30,393	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:19:30] "POST / HTTP/1.1" 200 -
2023-10-15 05:20:30,936	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:20:30] "POST / HTTP/1.1" 200 -
2023-10-15 05:21:31,223	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:21:31] "POST / HTTP/1.1" 200 -
2023-10-15 05:21:31,255	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:21:31] "POST / HTTP/1.1" 200 -
2023-10-15 05:22:31,564	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:22:31] "POST / HTTP/1.1" 200 -
2023-10-15 05:23:31,840	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:23:31] "POST / HTTP/1.1" 200 -
2023-10-15 05:23:31,871	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:23:31] "POST / HTTP/1.1" 200 -
2023-10-15 05:24:32,136	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:24:32] "POST / HTTP/1.1" 200 -
2023-10-15 05:25:32,416	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:25:32] "POST / HTTP/1.1" 200 -
2023-10-15 05:25:32,447	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:25:32] "POST / HTTP/1.1" 200 -
2023-10-15 05:28:27,612	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:28:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:32:28,350	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:32:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:32:28,385	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:32:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:33:29,056	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:33:29] "POST / HTTP/1.1" 200 -
2023-10-15 05:35:28,584	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:35:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:35:28,618	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:35:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:36:28,881	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:36:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:40:27,921	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:40:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:40:27,951	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:40:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:42:27,914	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:42:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:43:28,201	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:43:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:43:28,238	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:43:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:44:28,513	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:44:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:45:28,793	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:45:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:45:28,825	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:45:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:46:29,128	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:46:29] "POST / HTTP/1.1" 200 -
2023-10-15 05:48:27,622	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:48:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:48:27,655	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:48:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:49:27,902	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:49:27] "POST / HTTP/1.1" 200 -
2023-10-15 05:50:28,210	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:50:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:50:28,279	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:50:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:54:28,229	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:54:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:58:28,517	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:58:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:58:28,547	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 05:58:28] "POST / HTTP/1.1" 200 -
2023-10-15 05:59:28,788	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 05:59:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:00:29,536	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:00:29] "POST / HTTP/1.1" 200 -
2023-10-15 06:00:29,578	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:00:29] "POST / HTTP/1.1" 200 -
2023-10-15 06:01:29,857	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:01:29] "POST / HTTP/1.1" 200 -
2023-10-15 06:02:30,153	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:02:30] "POST / HTTP/1.1" 200 -
2023-10-15 06:02:30,188	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:02:30] "POST / HTTP/1.1" 200 -
2023-10-15 06:03:30,938	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:03:30] "POST / HTTP/1.1" 200 -
2023-10-15 06:04:31,204	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:04:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:04:31,241	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:04:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:05:31,502	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:05:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:06:57,982	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:06:57] "POST / HTTP/1.1" 200 -
2023-10-15 06:06:58,021	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:06:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:07:58,286	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:07:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:08:58,556	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:08:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:08:58,592	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:08:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:09:58,859	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:09:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:12:28,241	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:12:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:12:28,273	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:12:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:15:27,999	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:15:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:17:27,781	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:17:27] "POST / HTTP/1.1" 200 -
2023-10-15 06:17:27,811	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:17:27] "POST / HTTP/1.1" 200 -
2023-10-15 06:18:28,081	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:18:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:21:28,383	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:21:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:21:28,417	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:21:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:22:28,673	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:22:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:23:28,966	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:23:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:23:28,998	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:23:28] "POST / HTTP/1.1" 200 -
2023-10-15 06:24:29,274	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:24:29] "POST / HTTP/1.1" 200 -
2023-10-15 06:25:29,589	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:25:29] "POST / HTTP/1.1" 200 -
2023-10-15 06:25:29,624	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:25:29] "POST / HTTP/1.1" 200 -
2023-10-15 06:26:29,878	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:26:29] "POST / HTTP/1.1" 200 -
2023-10-15 06:27:30,178	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:27:30] "POST / HTTP/1.1" 200 -
2023-10-15 06:27:30,210	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:27:30] "POST / HTTP/1.1" 200 -
2023-10-15 06:28:30,491	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:28:30] "POST / HTTP/1.1" 200 -
2023-10-15 06:29:31,253	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:29:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:29:31,288	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:29:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:30:31,523	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:30:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:31:31,805	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:31:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:31:31,839	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:31:31] "POST / HTTP/1.1" 200 -
2023-10-15 06:32:32,107	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:32:32] "POST / HTTP/1.1" 200 -
2023-10-15 06:33:32,387	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:33:32] "POST / HTTP/1.1" 200 -
2023-10-15 06:33:32,425	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:33:32] "POST / HTTP/1.1" 200 -
2023-10-15 06:34:57,596	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:34:57] "POST / HTTP/1.1" 200 -
2023-10-15 06:35:57,914	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:35:57] "POST / HTTP/1.1" 200 -
2023-10-15 06:35:57,945	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:35:57] "POST / HTTP/1.1" 200 -
2023-10-15 06:36:58,278	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:36:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:37:58,620	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:37:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:37:58,664	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:37:58] "POST / HTTP/1.1" 200 -
2023-10-15 06:38:59,020	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:38:59] "POST / HTTP/1.1" 200 -
2023-10-15 06:39:59,393	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:39:59] "POST / HTTP/1.1" 200 -
2023-10-15 06:39:59,428	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:39:59] "POST / HTTP/1.1" 200 -
2023-10-15 06:40:59,809	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:40:59] "POST / HTTP/1.1" 200 -
2023-10-15 06:42:00,248	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:42:00] "POST / HTTP/1.1" 200 -
2023-10-15 06:42:00,298	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:42:00] "POST / HTTP/1.1" 200 -
2023-10-15 06:43:00,725	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:43:00] "POST / HTTP/1.1" 200 -
2023-10-15 06:44:01,067	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:44:01] "POST / HTTP/1.1" 200 -
2023-10-15 06:44:01,106	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:44:01] "POST / HTTP/1.1" 200 -
2023-10-15 06:45:01,516	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:45:01] "POST / HTTP/1.1" 200 -
2023-10-15 06:46:01,910	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:46:01] "POST / HTTP/1.1" 200 -
2023-10-15 06:46:01,956	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:46:01] "POST / HTTP/1.1" 200 -
2023-10-15 06:47:03,037	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:47:03] "POST / HTTP/1.1" 200 -
2023-10-15 06:48:03,356	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:48:03] "POST / HTTP/1.1" 200 -
2023-10-15 06:48:03,397	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:48:03] "POST / HTTP/1.1" 200 -
2023-10-15 06:49:03,695	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:49:03] "POST / HTTP/1.1" 200 -
2023-10-15 06:50:04,101	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:50:04] "POST / HTTP/1.1" 200 -
2023-10-15 06:50:04,133	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:50:04] "POST / HTTP/1.1" 200 -
2023-10-15 06:51:04,428	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:51:04] "POST / HTTP/1.1" 200 -
2023-10-15 06:52:04,750	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:52:04] "POST / HTTP/1.1" 200 -
2023-10-15 06:52:04,779	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:52:04] "POST / HTTP/1.1" 200 -
2023-10-15 06:53:05,070	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:53:05] "POST / HTTP/1.1" 200 -
2023-10-15 06:54:05,459	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:54:05] "POST / HTTP/1.1" 200 -
2023-10-15 06:54:05,494	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:54:05] "POST / HTTP/1.1" 200 -
2023-10-15 06:55:05,856	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:55:05] "POST / HTTP/1.1" 200 -
2023-10-15 06:56:06,202	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:56:06] "POST / HTTP/1.1" 200 -
2023-10-15 06:56:06,244	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:56:06] "POST / HTTP/1.1" 200 -
2023-10-15 06:57:06,539	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:57:06] "POST / HTTP/1.1" 200 -
2023-10-15 06:58:07,066	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:58:07] "POST / HTTP/1.1" 200 -
2023-10-15 06:58:07,099	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 06:58:07] "POST / HTTP/1.1" 200 -
2023-10-15 06:59:07,453	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 06:59:07] "POST / HTTP/1.1" 200 -
2023-10-15 07:00:07,752	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:00:07] "POST / HTTP/1.1" 200 -
2023-10-15 07:00:07,793	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:00:07] "POST / HTTP/1.1" 200 -
2023-10-15 07:01:08,128	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:01:08] "POST / HTTP/1.1" 200 -
2023-10-15 07:02:08,505	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:02:08] "POST / HTTP/1.1" 200 -
2023-10-15 07:02:08,536	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:02:08] "POST / HTTP/1.1" 200 -
2023-10-15 07:03:08,894	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:03:08] "POST / HTTP/1.1" 200 -
2023-10-15 07:04:09,314	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:04:09] "POST / HTTP/1.1" 200 -
2023-10-15 07:04:09,346	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:04:09] "POST / HTTP/1.1" 200 -
2023-10-15 07:05:09,619	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:05:09] "POST / HTTP/1.1" 200 -
2023-10-15 07:06:10,046	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:06:10] "POST / HTTP/1.1" 200 -
2023-10-15 07:06:10,084	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:06:10] "POST / HTTP/1.1" 200 -
2023-10-15 07:07:10,440	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:07:10] "POST / HTTP/1.1" 200 -
2023-10-15 07:08:10,820	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:08:10] "POST / HTTP/1.1" 200 -
2023-10-15 07:08:10,857	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:08:10] "POST / HTTP/1.1" 200 -
2023-10-15 07:09:11,145	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:09:11] "POST / HTTP/1.1" 200 -
2023-10-15 07:10:11,497	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:10:11] "POST / HTTP/1.1" 200 -
2023-10-15 07:10:11,528	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:10:11] "POST / HTTP/1.1" 200 -
2023-10-15 07:11:11,765	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:11:11] "POST / HTTP/1.1" 200 -
2023-10-15 07:12:12,133	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:12:12] "POST / HTTP/1.1" 200 -
2023-10-15 07:12:12,167	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:12:12] "POST / HTTP/1.1" 200 -
2023-10-15 07:13:12,451	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:13:12] "POST / HTTP/1.1" 200 -
2023-10-15 07:14:12,774	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:14:12] "POST / HTTP/1.1" 200 -
2023-10-15 07:14:12,804	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:14:12] "POST / HTTP/1.1" 200 -
2023-10-15 07:15:13,098	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:15:13] "POST / HTTP/1.1" 200 -
2023-10-15 07:16:13,437	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:16:13] "POST / HTTP/1.1" 200 -
2023-10-15 07:16:13,468	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:16:13] "POST / HTTP/1.1" 200 -
2023-10-15 07:17:13,762	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:17:13] "POST / HTTP/1.1" 200 -
2023-10-15 07:18:14,083	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:18:14] "POST / HTTP/1.1" 200 -
2023-10-15 07:18:14,116	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:18:14] "POST / HTTP/1.1" 200 -
2023-10-15 07:19:14,442	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:19:14] "POST / HTTP/1.1" 200 -
2023-10-15 07:20:14,759	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:20:14] "POST / HTTP/1.1" 200 -
2023-10-15 07:20:14,791	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:20:14] "POST / HTTP/1.1" 200 -
2023-10-15 07:21:15,042	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:21:15] "POST / HTTP/1.1" 200 -
2023-10-15 07:22:15,308	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:22:15] "POST / HTTP/1.1" 200 -
2023-10-15 07:22:15,355	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:22:15] "POST / HTTP/1.1" 200 -
2023-10-15 07:23:15,648	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:23:15] "POST / HTTP/1.1" 200 -
2023-10-15 07:24:15,929	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:24:15] "POST / HTTP/1.1" 200 -
2023-10-15 07:24:15,965	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:24:15] "POST / HTTP/1.1" 200 -
2023-10-15 07:25:16,233	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:25:16] "POST / HTTP/1.1" 200 -
2023-10-15 07:26:16,530	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:26:16] "POST / HTTP/1.1" 200 -
2023-10-15 07:26:16,564	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:26:16] "POST / HTTP/1.1" 200 -
2023-10-15 07:27:16,826	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:27:16] "POST / HTTP/1.1" 200 -
2023-10-15 07:28:17,179	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:28:17] "POST / HTTP/1.1" 200 -
2023-10-15 07:28:17,216	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:28:17] "POST / HTTP/1.1" 200 -
2023-10-15 07:29:17,565	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:29:17] "POST / HTTP/1.1" 200 -
2023-10-15 07:30:17,880	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:30:17] "POST / HTTP/1.1" 200 -
2023-10-15 07:30:17,911	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:30:17] "POST / HTTP/1.1" 200 -
2023-10-15 07:31:18,208	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:31:18] "POST / HTTP/1.1" 200 -
2023-10-15 07:32:18,505	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:32:18] "POST / HTTP/1.1" 200 -
2023-10-15 07:32:18,541	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:32:18] "POST / HTTP/1.1" 200 -
2023-10-15 07:33:18,810	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:33:18] "POST / HTTP/1.1" 200 -
2023-10-15 07:34:19,149	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:34:19] "POST / HTTP/1.1" 200 -
2023-10-15 07:34:19,181	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:34:19] "POST / HTTP/1.1" 200 -
2023-10-15 07:35:19,428	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:35:19] "POST / HTTP/1.1" 200 -
2023-10-15 07:36:19,702	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:36:19] "POST / HTTP/1.1" 200 -
2023-10-15 07:36:19,733	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:36:19] "POST / HTTP/1.1" 200 -
2023-10-15 07:37:19,992	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:37:19] "POST / HTTP/1.1" 200 -
2023-10-15 07:38:20,226	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:38:20] "POST / HTTP/1.1" 200 -
2023-10-15 07:38:20,259	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:38:20] "POST / HTTP/1.1" 200 -
2023-10-15 07:39:20,529	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:39:20] "POST / HTTP/1.1" 200 -
2023-10-15 07:40:20,829	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:40:20] "POST / HTTP/1.1" 200 -
2023-10-15 07:40:20,859	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:40:20] "POST / HTTP/1.1" 200 -
2023-10-15 07:41:21,125	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:41:21] "POST / HTTP/1.1" 200 -
2023-10-15 07:42:21,412	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:42:21] "POST / HTTP/1.1" 200 -
2023-10-15 07:42:21,443	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:42:21] "POST / HTTP/1.1" 200 -
2023-10-15 07:43:21,734	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:43:21] "POST / HTTP/1.1" 200 -
2023-10-15 07:44:22,236	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:44:22] "POST / HTTP/1.1" 200 -
2023-10-15 07:44:22,272	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:44:22] "POST / HTTP/1.1" 200 -
2023-10-15 07:45:22,582	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:45:22] "POST / HTTP/1.1" 200 -
2023-10-15 07:46:22,876	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:46:22] "POST / HTTP/1.1" 200 -
2023-10-15 07:46:22,910	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:46:22] "POST / HTTP/1.1" 200 -
2023-10-15 07:47:23,171	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:47:23] "POST / HTTP/1.1" 200 -
2023-10-15 07:48:23,449	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:48:23] "POST / HTTP/1.1" 200 -
2023-10-15 07:48:23,489	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:48:23] "POST / HTTP/1.1" 200 -
2023-10-15 07:49:23,803	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:49:23] "POST / HTTP/1.1" 200 -
2023-10-15 07:50:24,179	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:50:24] "POST / HTTP/1.1" 200 -
2023-10-15 07:50:24,211	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:50:24] "POST / HTTP/1.1" 200 -
2023-10-15 07:51:24,545	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:51:24] "POST / HTTP/1.1" 200 -
2023-10-15 07:52:24,880	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:52:24] "POST / HTTP/1.1" 200 -
2023-10-15 07:52:24,921	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:52:24] "POST / HTTP/1.1" 200 -
2023-10-15 07:53:25,272	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:53:25] "POST / HTTP/1.1" 200 -
2023-10-15 07:54:25,620	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:54:25] "POST / HTTP/1.1" 200 -
2023-10-15 07:54:25,651	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:54:25] "POST / HTTP/1.1" 200 -
2023-10-15 07:55:26,007	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:55:26] "POST / HTTP/1.1" 200 -
2023-10-15 07:56:26,334	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:56:26] "POST / HTTP/1.1" 200 -
2023-10-15 07:56:26,379	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:56:26] "POST / HTTP/1.1" 200 -
2023-10-15 07:57:26,709	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:57:26] "POST / HTTP/1.1" 200 -
2023-10-15 07:58:27,563	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:58:27] "POST / HTTP/1.1" 200 -
2023-10-15 07:58:27,601	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 07:58:27] "POST / HTTP/1.1" 200 -
2023-10-15 07:59:27,937	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 07:59:27] "POST / HTTP/1.1" 200 -
2023-10-15 08:00:39,035	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:00:39] "POST / HTTP/1.1" 200 -
2023-10-15 08:00:39,074	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:00:39] "POST / HTTP/1.1" 200 -
2023-10-15 08:01:39,436	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:01:39] "POST / HTTP/1.1" 200 -
2023-10-15 08:02:57,078	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:02:57] "POST / HTTP/1.1" 200 -
2023-10-15 08:02:57,110	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:02:57] "POST / HTTP/1.1" 200 -
2023-10-15 08:03:57,653	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:03:57] "POST / HTTP/1.1" 200 -
2023-10-15 08:04:58,023	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:04:58] "POST / HTTP/1.1" 200 -
2023-10-15 08:04:58,072	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:04:58] "POST / HTTP/1.1" 200 -
2023-10-15 08:05:58,408	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:05:58] "POST / HTTP/1.1" 200 -
2023-10-15 08:06:58,774	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:06:58] "POST / HTTP/1.1" 200 -
2023-10-15 08:06:58,813	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:06:58] "POST / HTTP/1.1" 200 -
2023-10-15 08:07:59,235	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:07:59] "POST / HTTP/1.1" 200 -
2023-10-15 08:08:59,595	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:08:59] "POST / HTTP/1.1" 200 -
2023-10-15 08:08:59,626	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:08:59] "POST / HTTP/1.1" 200 -
2023-10-15 08:09:59,983	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:09:59] "POST / HTTP/1.1" 200 -
2023-10-15 08:11:00,347	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:11:00] "POST / HTTP/1.1" 200 -
2023-10-15 08:11:00,382	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:11:00] "POST / HTTP/1.1" 200 -
2023-10-15 08:12:00,742	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:12:00] "POST / HTTP/1.1" 200 -
2023-10-15 08:13:01,115	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:13:01] "POST / HTTP/1.1" 200 -
2023-10-15 08:13:01,145	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:13:01] "POST / HTTP/1.1" 200 -
2023-10-15 08:14:01,474	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:14:01] "POST / HTTP/1.1" 200 -
2023-10-15 08:15:01,838	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:15:01] "POST / HTTP/1.1" 200 -
2023-10-15 08:15:01,874	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:15:01] "POST / HTTP/1.1" 200 -
2023-10-15 08:16:02,204	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:16:02] "POST / HTTP/1.1" 200 -
2023-10-15 08:17:02,540	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:17:02] "POST / HTTP/1.1" 200 -
2023-10-15 08:17:02,584	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:17:02] "POST / HTTP/1.1" 200 -
2023-10-15 08:18:02,865	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:18:02] "POST / HTTP/1.1" 200 -
2023-10-15 08:19:03,156	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:19:03] "POST / HTTP/1.1" 200 -
2023-10-15 08:19:03,194	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:19:03] "POST / HTTP/1.1" 200 -
2023-10-15 08:20:03,473	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:20:03] "POST / HTTP/1.1" 200 -
2023-10-15 08:21:03,750	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:21:03] "POST / HTTP/1.1" 200 -
2023-10-15 08:21:03,780	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:21:03] "POST / HTTP/1.1" 200 -
2023-10-15 08:22:04,132	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:22:04] "POST / HTTP/1.1" 200 -
2023-10-15 08:23:04,431	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:23:04] "POST / HTTP/1.1" 200 -
2023-10-15 08:23:04,463	INFO policy_server_input.py:288 -- Got sample batch of size 2 from client.
127.0.0.1 - - [15/Oct/2023 08:23:04] "POST / HTTP/1.1" 200 -
2023-10-15 08:24:04,731	INFO policy_server_input.py:284 -- Sending worker weights to client.
127.0.0.1 - - [15/Oct/2023 08:24:04] "POST / HTTP/1.1" 200 -
e_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 7562.96768116951
time_this_iter_s: 120.59070777893066
time_total_s: 7562.96768116951
timers:
  learn_throughput: 2.864
  learn_time_ms: 698.306
  load_throughput: 5529.006
  load_time_ms: 0.362
  sample_time_ms: 125453.682
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 126153.006
timestamp: 1697320893
timesteps_total: 74
training_iteration: 37
trial_id: default

Last checkpoint 36 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000037
server side epoch loop 37
algo.train executed
agent_timesteps_total: 76
connector_metrics: {}
counters:
  num_agent_steps_sampled: 76
  num_agent_steps_trained: 76
  num_env_steps_sampled: 76
  num_env_steps_trained: 76
custom_metrics: {}
date: 2023-10-15_01-03-33
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20272865295410156
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8137389520804088
        entropy_coeff: 0.0
        grad_gnorm: 10.542729018380244
        kl: 0.017431742888099202
        policy_loss: -0.2137055238087972
        total_loss: 0.03724601864814758
        vf_explained_var: .nan
        vf_loss: 0.24741763107742068
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2250.5
  num_agent_steps_sampled: 76
  num_agent_steps_trained: 76
  num_env_steps_sampled: 76
  num_env_steps_trained: 76
iterations_since_restore: 38
node_ip: 10.27.41.23
num_agent_steps_sampled: 76
num_agent_steps_trained: 76
num_env_steps_sampled: 76
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016610265326251518
num_env_steps_trained: 76
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016610265326251518
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.95406976744186
  ram_util_percent: 46.72209302325581
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 7683.375397443771
time_this_iter_s: 120.40771627426147
time_total_s: 7683.375397443771
timers:
  learn_throughput: 2.893
  learn_time_ms: 691.386
  load_throughput: 5579.016
  load_time_ms: 0.358
  sample_time_ms: 125444.075
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 126136.466
timestamp: 1697321013
timesteps_total: 76
training_iteration: 38
trial_id: default

Last checkpoint 37 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000038
server side epoch loop 38
algo.train executed
agent_timesteps_total: 78
connector_metrics: {}
counters:
  num_agent_steps_sampled: 78
  num_agent_steps_trained: 78
  num_env_steps_sampled: 78
  num_env_steps_trained: 78
custom_metrics: {}
date: 2023-10-15_01-05-34
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20272865295410156
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8635228594144186
        entropy_coeff: 0.0
        grad_gnorm: 22.481845966974895
        kl: 0.017826654806109826
        policy_loss: -0.20118221441904705
        total_loss: 0.7886491315749784
        vf_explained_var: .nan
        vf_loss: 0.9862173681089189
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2310.5
  num_agent_steps_sampled: 78
  num_agent_steps_trained: 78
  num_env_steps_sampled: 78
  num_env_steps_trained: 78
iterations_since_restore: 39
node_ip: 10.27.41.23
num_agent_steps_sampled: 78
num_agent_steps_trained: 78
num_env_steps_sampled: 78
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658607767596843
num_env_steps_trained: 78
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658607767596843
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.763372093023253
  ram_util_percent: 47.32674418604652
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 7803.958653926849
time_this_iter_s: 120.583256483078
time_total_s: 7803.958653926849
timers:
  learn_throughput: 2.932
  learn_time_ms: 682.228
  load_throughput: 5684.494
  load_time_ms: 0.352
  sample_time_ms: 119883.677
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120566.899
timestamp: 1697321134
timesteps_total: 78
training_iteration: 39
trial_id: default

Last checkpoint 38 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000039
server side epoch loop 39
algo.train executed
agent_timesteps_total: 80
connector_metrics: {}
counters:
  num_agent_steps_sampled: 80
  num_agent_steps_trained: 80
  num_env_steps_sampled: 80
  num_env_steps_trained: 80
custom_metrics: {}
date: 2023-10-15_01-07-34
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20272865295410156
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9144035120805105
        entropy_coeff: 0.0
        grad_gnorm: 12.65290587147077
        kl: 0.022404785646358505
        policy_loss: -0.25463221172491707
        total_loss: 0.020712887371579804
        vf_explained_var: .nan
        vf_loss: 0.27080300703116034
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2370.5
  num_agent_steps_sampled: 80
  num_agent_steps_trained: 80
  num_env_steps_sampled: 80
  num_env_steps_trained: 80
iterations_since_restore: 40
node_ip: 10.27.41.23
num_agent_steps_sampled: 80
num_agent_steps_trained: 80
num_env_steps_sampled: 80
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01659194847817833
num_env_steps_trained: 80
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01659194847817833
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.59186046511628
  ram_util_percent: 48.17558139534883
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 7924.499253988266
time_this_iter_s: 120.54060006141663
time_total_s: 7924.499253988266
timers:
  learn_throughput: 3.05
  learn_time_ms: 655.735
  load_throughput: 5610.734
  load_time_ms: 0.356
  sample_time_ms: 119887.503
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120544.243
timestamp: 1697321254
timesteps_total: 80
training_iteration: 40
trial_id: default

Last checkpoint 39 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000040
server side epoch loop 40
algo.train executed
agent_timesteps_total: 82
connector_metrics: {}
counters:
  num_agent_steps_sampled: 82
  num_agent_steps_trained: 82
  num_env_steps_sampled: 82
  num_env_steps_trained: 82
custom_metrics: {}
date: 2023-10-15_01-09-58
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.3040929794311525
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8164695660273233
        entropy_coeff: 0.0
        grad_gnorm: 18.89787564277649
        kl: 0.00961876761672708
        policy_loss: -0.20076643427213034
        total_loss: 0.26637054681777955
        vf_explained_var: .nan
        vf_loss: 0.46421197877886394
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2430.5
  num_agent_steps_sampled: 82
  num_agent_steps_trained: 82
  num_env_steps_sampled: 82
  num_env_steps_trained: 82
iterations_since_restore: 41
node_ip: 10.27.41.23
num_agent_steps_sampled: 82
num_agent_steps_trained: 82
num_env_steps_sampled: 82
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013977561318310485
num_env_steps_trained: 82
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013977561318310485
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.347058823529412
  ram_util_percent: 47.382843137254895
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 8067.585985183716
time_this_iter_s: 143.08673119544983
time_total_s: 8067.585985183716
timers:
  learn_throughput: 3.084
  learn_time_ms: 648.591
  load_throughput: 5403.986
  load_time_ms: 0.37
  sample_time_ms: 122140.924
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 122790.543
timestamp: 1697321398
timesteps_total: 82
training_iteration: 41
trial_id: default

Last checkpoint 40 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000041
server side epoch loop 41
algo.train executed
agent_timesteps_total: 84
connector_metrics: {}
counters:
  num_agent_steps_sampled: 84
  num_agent_steps_trained: 84
  num_env_steps_sampled: 84
  num_env_steps_trained: 84
custom_metrics: {}
date: 2023-10-15_01-11-58
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.3040929794311525
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8418112178643544
        entropy_coeff: 0.0
        grad_gnorm: 13.459836276372274
        kl: 0.014710193596077186
        policy_loss: -0.21821486949920654
        total_loss: 0.00274759034315745
        vf_explained_var: .nan
        vf_loss: 0.21648919098079206
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2490.5
  num_agent_steps_sampled: 84
  num_agent_steps_trained: 84
  num_env_steps_sampled: 84
  num_env_steps_trained: 84
iterations_since_restore: 42
node_ip: 10.27.41.23
num_agent_steps_sampled: 84
num_agent_steps_trained: 84
num_env_steps_sampled: 84
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016573530787068396
num_env_steps_trained: 84
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016573530787068396
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.853757225433524
  ram_util_percent: 46.839884393063585
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 8188.260554075241
time_this_iter_s: 120.67456889152527
time_total_s: 8188.260554075241
timers:
  learn_throughput: 3.192
  learn_time_ms: 626.476
  load_throughput: 5546.921
  load_time_ms: 0.361
  sample_time_ms: 122161.628
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 122789.123
timestamp: 1697321518
timesteps_total: 84
training_iteration: 42
trial_id: default

Last checkpoint 41 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000042
server side epoch loop 42
algo.train executed
agent_timesteps_total: 86
connector_metrics: {}
counters:
  num_agent_steps_sampled: 86
  num_agent_steps_trained: 86
  num_env_steps_sampled: 86
  num_env_steps_trained: 86
custom_metrics: {}
date: 2023-10-15_01-13-59
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.3040929794311525
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9077185928821563
        entropy_coeff: 0.0
        grad_gnorm: 26.653166181842487
        kl: 0.02344759894622257
        policy_loss: -0.22135382990042368
        total_loss: 0.9288709849119187
        vf_explained_var: .nan
        vf_loss: 1.1430945618851789
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2550.5
  num_agent_steps_sampled: 86
  num_agent_steps_trained: 86
  num_env_steps_sampled: 86
  num_env_steps_trained: 86
iterations_since_restore: 43
node_ip: 10.27.41.23
num_agent_steps_sampled: 86
num_agent_steps_trained: 86
num_env_steps_sampled: 86
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016587643650090914
num_env_steps_trained: 86
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016587643650090914
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.42732558139535
  ram_util_percent: 47.14418604651163
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 8308.832424879074
time_this_iter_s: 120.57187080383301
time_total_s: 8308.832424879074
timers:
  learn_throughput: 4.108
  learn_time_ms: 486.895
  load_throughput: 5603.613
  load_time_ms: 0.357
  sample_time_ms: 122181.828
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 122669.728
timestamp: 1697321639
timesteps_total: 86
training_iteration: 43
trial_id: default

Last checkpoint 42 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000043
server side epoch loop 43
algo.train executed
agent_timesteps_total: 88
connector_metrics: {}
counters:
  num_agent_steps_sampled: 88
  num_agent_steps_trained: 88
  num_env_steps_sampled: 88
  num_env_steps_trained: 88
custom_metrics: {}
date: 2023-10-15_01-16-58
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.45613946914672837
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8822478036085764
        entropy_coeff: 0.0
        grad_gnorm: 16.92424920698007
        kl: 0.0404553447306777
        policy_loss: -0.2320041666428248
        total_loss: 0.1976047081251939
        vf_explained_var: .nan
        vf_loss: 0.41115560180824107
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2610.5
  num_agent_steps_sampled: 88
  num_agent_steps_trained: 88
  num_env_steps_sampled: 88
  num_env_steps_trained: 88
iterations_since_restore: 44
node_ip: 10.27.41.23
num_agent_steps_sampled: 88
num_agent_steps_trained: 88
num_env_steps_sampled: 88
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011195084664545352
num_env_steps_trained: 88
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011195084664545352
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.529527559055119
  ram_util_percent: 47.594488188976364
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 8487.482455968857
time_this_iter_s: 178.65003108978271
time_total_s: 8487.482455968857
timers:
  learn_throughput: 4.16
  learn_time_ms: 480.714
  load_throughput: 5653.844
  load_time_ms: 0.354
  sample_time_ms: 128133.966
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 128615.671
timestamp: 1697321818
timesteps_total: 88
training_iteration: 44
trial_id: default

Last checkpoint 43 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000044
server side epoch loop 44
algo.train executed
agent_timesteps_total: 90
connector_metrics: {}
counters:
  num_agent_steps_sampled: 90
  num_agent_steps_trained: 90
  num_env_steps_sampled: 90
  num_env_steps_trained: 90
custom_metrics: {}
date: 2023-10-15_01-19-28
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.684209203720093
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8136438310146332
        entropy_coeff: 0.0
        grad_gnorm: 18.14096352259318
        kl: 0.01595812527060237
        policy_loss: -0.2536821335554123
        total_loss: 0.1109190339843432
        vf_explained_var: .nan
        vf_loss: 0.35368246684471766
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2670.5
  num_agent_steps_sampled: 90
  num_agent_steps_trained: 90
  num_env_steps_sampled: 90
  num_env_steps_trained: 90
iterations_since_restore: 45
node_ip: 10.27.41.23
num_agent_steps_sampled: 90
num_agent_steps_trained: 90
num_env_steps_sampled: 90
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013283469670766036
num_env_steps_trained: 90
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013283469670766036
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.855813953488372
  ram_util_percent: 46.48139534883721
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 8638.04572558403
time_this_iter_s: 150.56326961517334
time_total_s: 8638.04572558403
timers:
  learn_throughput: 4.101
  learn_time_ms: 487.69
  load_throughput: 5651.178
  load_time_ms: 0.354
  sample_time_ms: 131128.096
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 131616.692
timestamp: 1697321968
timesteps_total: 90
training_iteration: 45
trial_id: default

Last checkpoint 44 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000045
server side epoch loop 45
algo.train executed
agent_timesteps_total: 92
connector_metrics: {}
counters:
  num_agent_steps_sampled: 92
  num_agent_steps_trained: 92
  num_env_steps_sampled: 92
  num_env_steps_trained: 92
custom_metrics: {}
date: 2023-10-15_01-21-29
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.684209203720093
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8979490617911021
        entropy_coeff: 0.0
        grad_gnorm: 10.651173043747743
        kl: 0.013705531520827208
        policy_loss: -0.2512201984723409
        total_loss: -0.09033971230189006
        vf_explained_var: .nan
        vf_loss: 0.15150303139441046
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2730.5
  num_agent_steps_sampled: 92
  num_agent_steps_trained: 92
  num_env_steps_sampled: 92
  num_env_steps_trained: 92
iterations_since_restore: 46
node_ip: 10.27.41.23
num_agent_steps_sampled: 92
num_agent_steps_trained: 92
num_env_steps_sampled: 92
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658892063620228
num_env_steps_trained: 92
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658892063620228
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.777906976744186
  ram_util_percent: 48.66802325581394
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 8758.608383893967
time_this_iter_s: 120.56265830993652
time_total_s: 8758.608383893967
timers:
  learn_throughput: 4.14
  learn_time_ms: 483.056
  load_throughput: 5511.569
  load_time_ms: 0.363
  sample_time_ms: 131138.917
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 131622.888
timestamp: 1697322089
timesteps_total: 92
training_iteration: 46
trial_id: default

Last checkpoint 45 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000046
server side epoch loop 46
algo.train executed
agent_timesteps_total: 94
connector_metrics: {}
counters:
  num_agent_steps_sampled: 94
  num_agent_steps_trained: 94
  num_env_steps_sampled: 94
  num_env_steps_trained: 94
custom_metrics: {}
date: 2023-10-15_01-23-29
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.684209203720093
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9040882587432861
        entropy_coeff: 0.0
        grad_gnorm: 21.725295615196227
        kl: 0.007683150607772404
        policy_loss: -0.09683248897393544
        total_loss: 0.5895664279659589
        vf_explained_var: .nan
        vf_loss: 0.6811420339285784
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2790.5
  num_agent_steps_sampled: 94
  num_agent_steps_trained: 94
  num_env_steps_sampled: 94
  num_env_steps_trained: 94
iterations_since_restore: 47
node_ip: 10.27.41.23
num_agent_steps_sampled: 94
num_agent_steps_trained: 94
num_env_steps_sampled: 94
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016573341787179743
num_env_steps_trained: 94
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016573341787179743
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.39479768786127
  ram_util_percent: 47.408092485549126
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 8879.28433585167
time_this_iter_s: 120.67595195770264
time_total_s: 8879.28433585167
timers:
  learn_throughput: 4.148
  learn_time_ms: 482.144
  load_throughput: 5587.563
  load_time_ms: 0.358
  sample_time_ms: 131148.35
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 131631.411
timestamp: 1697322209
timesteps_total: 94
training_iteration: 47
trial_id: default

Last checkpoint 46 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000047
server side epoch loop 47
algo.train executed
agent_timesteps_total: 96
connector_metrics: {}
counters:
  num_agent_steps_sampled: 96
  num_agent_steps_trained: 96
  num_env_steps_sampled: 96
  num_env_steps_trained: 96
custom_metrics: {}
date: 2023-10-15_01-25-31
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.684209203720093
        cur_lr: 5.0000000000000016e-05
        entropy: 1.769836316506068
        entropy_coeff: 0.0
        grad_gnorm: 16.45342124799887
        kl: 0.012857262664086495
        policy_loss: -0.21909851531187693
        total_loss: 0.16958306948654353
        vf_explained_var: .nan
        vf_loss: 0.3798845279043235
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2850.5
  num_agent_steps_sampled: 96
  num_agent_steps_trained: 96
  num_env_steps_sampled: 96
  num_env_steps_trained: 96
iterations_since_restore: 48
node_ip: 10.27.41.23
num_agent_steps_sampled: 96
num_agent_steps_trained: 96
num_env_steps_sampled: 96
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016509880681616247
num_env_steps_trained: 96
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016509880681616247
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.759302325581395
  ram_util_percent: 46.80232558139535
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9000.424118041992
time_this_iter_s: 121.13978219032288
time_total_s: 9000.424118041992
timers:
  learn_throughput: 3.752
  learn_time_ms: 533.057
  load_throughput: 5583.1
  load_time_ms: 0.358
  sample_time_ms: 131170.65
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 131704.622
timestamp: 1697322331
timesteps_total: 96
training_iteration: 48
trial_id: default

Last checkpoint 47 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000048
server side epoch loop 48
algo.train executed
agent_timesteps_total: 98
connector_metrics: {}
counters:
  num_agent_steps_sampled: 98
  num_agent_steps_trained: 98
  num_env_steps_sampled: 98
  num_env_steps_trained: 98
custom_metrics: {}
date: 2023-10-15_01-27-31
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.684209203720093
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8002543210983277
        entropy_coeff: 0.0
        grad_gnorm: 13.399096163113912
        kl: 0.009741880993412147
        policy_loss: -0.10785487294197083
        total_loss: 0.12505795396864414
        vf_explained_var: .nan
        vf_loss: 0.22624734534620075
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2910.5
  num_agent_steps_sampled: 98
  num_agent_steps_trained: 98
  num_env_steps_sampled: 98
  num_env_steps_trained: 98
iterations_since_restore: 49
node_ip: 10.27.41.23
num_agent_steps_sampled: 98
num_agent_steps_trained: 98
num_env_steps_sampled: 98
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0166339985697662
num_env_steps_trained: 98
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0166339985697662
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.42267441860465
  ram_util_percent: 47.10058139534885
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9120.66000533104
time_this_iter_s: 120.23588728904724
time_total_s: 9120.66000533104
timers:
  learn_throughput: 3.668
  learn_time_ms: 545.319
  load_throughput: 4999.17
  load_time_ms: 0.4
  sample_time_ms: 131123.598
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 131669.882
timestamp: 1697322451
timesteps_total: 98
training_iteration: 49
trial_id: default

Last checkpoint 48 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000049
server side epoch loop 49
algo.train executed
agent_timesteps_total: 100
connector_metrics: {}
counters:
  num_agent_steps_sampled: 100
  num_agent_steps_trained: 100
  num_env_steps_sampled: 100
  num_env_steps_trained: 100
custom_metrics: {}
date: 2023-10-15_01-29-32
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.684209203720093
        cur_lr: 5.0000000000000016e-05
        entropy: 1.905834323167801
        entropy_coeff: 0.0
        grad_gnorm: 14.233744009335835
        kl: 0.007844968687762351
        policy_loss: -0.20745352109273274
        total_loss: 0.018680736422538757
        vf_explained_var: .nan
        vf_loss: 0.2207666597639521
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 2970.5
  num_agent_steps_sampled: 100
  num_agent_steps_trained: 100
  num_env_steps_sampled: 100
  num_env_steps_trained: 100
iterations_since_restore: 50
node_ip: 10.27.41.23
num_agent_steps_sampled: 100
num_agent_steps_trained: 100
num_env_steps_sampled: 100
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01657253095715742
num_env_steps_trained: 100
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01657253095715742
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.29186046511628
  ram_util_percent: 46.45755813953488
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9241.341890096664
time_this_iter_s: 120.681884765625
time_total_s: 9241.341890096664
timers:
  learn_throughput: 3.573
  learn_time_ms: 559.751
  load_throughput: 5008.423
  load_time_ms: 0.399
  sample_time_ms: 131123.278
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 131684.004
timestamp: 1697322572
timesteps_total: 100
training_iteration: 50
trial_id: default

Last checkpoint 49 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000050
server side epoch loop 50
algo.train executed
agent_timesteps_total: 102
connector_metrics: {}
counters:
  num_agent_steps_sampled: 102
  num_agent_steps_trained: 102
  num_env_steps_sampled: 102
  num_env_steps_trained: 102
custom_metrics: {}
date: 2023-10-15_01-31-32
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.684209203720093
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7944920261700947
        entropy_coeff: 0.0
        grad_gnorm: 13.959630584716797
        kl: 0.020277980554965325
        policy_loss: -0.24230118095874786
        total_loss: 0.032268673678239185
        vf_explained_var: .nan
        vf_loss: 0.26069547585987796
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3030.5
  num_agent_steps_sampled: 102
  num_agent_steps_trained: 102
  num_env_steps_sampled: 102
  num_env_steps_trained: 102
iterations_since_restore: 51
node_ip: 10.27.41.23
num_agent_steps_sampled: 102
num_agent_steps_trained: 102
num_env_steps_sampled: 102
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016606490943859756
num_env_steps_trained: 102
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016606490943859756
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.312790697674417
  ram_util_percent: 47.58779069767442
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9361.776947021484
time_this_iter_s: 120.43505692481995
time_total_s: 9361.776947021484
timers:
  learn_throughput: 3.585
  learn_time_ms: 557.819
  load_throughput: 5032.46
  load_time_ms: 0.397
  sample_time_ms: 128860.052
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 129418.84
timestamp: 1697322692
timesteps_total: 102
training_iteration: 51
trial_id: default

Last checkpoint 50 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000051
server side epoch loop 51
algo.train executed
agent_timesteps_total: 104
connector_metrics: {}
counters:
  num_agent_steps_sampled: 104
  num_agent_steps_trained: 104
  num_env_steps_sampled: 104
  num_env_steps_trained: 104
custom_metrics: {}
date: 2023-10-15_01-33-33
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0263138055801393
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8058566868305206
        entropy_coeff: 0.0
        grad_gnorm: 22.311065822839737
        kl: 0.013867491555841601
        policy_loss: -0.17321030398209888
        total_loss: 0.5991647614476582
        vf_explained_var: .nan
        vf_loss: 0.758142677453058
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3090.5
  num_agent_steps_sampled: 104
  num_agent_steps_trained: 104
  num_env_steps_sampled: 104
  num_env_steps_trained: 104
iterations_since_restore: 52
node_ip: 10.27.41.23
num_agent_steps_sampled: 104
num_agent_steps_trained: 104
num_env_steps_sampled: 104
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01650051344487348
num_env_steps_trained: 104
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01650051344487348
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.439306358381502
  ram_util_percent: 47.88208092485548
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9482.985512971878
time_this_iter_s: 121.20856595039368
time_total_s: 9482.985512971878
timers:
  learn_throughput: 3.379
  learn_time_ms: 591.974
  load_throughput: 4996.193
  load_time_ms: 0.4
  sample_time_ms: 128879.302
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 129472.24
timestamp: 1697322813
timesteps_total: 104
training_iteration: 52
trial_id: default

Last checkpoint 51 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000052
server side epoch loop 52
algo.train executed
agent_timesteps_total: 106
connector_metrics: {}
counters:
  num_agent_steps_sampled: 106
  num_agent_steps_trained: 106
  num_env_steps_sampled: 106
  num_env_steps_trained: 106
custom_metrics: {}
date: 2023-10-15_01-35-34
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0263138055801393
        cur_lr: 5.0000000000000016e-05
        entropy: 1.86997482975324
        entropy_coeff: 0.0
        grad_gnorm: 12.82510146299998
        kl: 0.014142528757171627
        policy_loss: -0.20587220191955566
        total_loss: 0.01555489053328832
        vf_explained_var: .nan
        vf_loss: 0.206912413865939
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3150.5
  num_agent_steps_sampled: 106
  num_agent_steps_trained: 106
  num_env_steps_sampled: 106
  num_env_steps_trained: 106
iterations_since_restore: 53
node_ip: 10.27.41.23
num_agent_steps_sampled: 106
num_agent_steps_trained: 106
num_env_steps_sampled: 106
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016635224048903698
num_env_steps_trained: 106
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016635224048903698
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.834502923976608
  ram_util_percent: 47.04035087719298
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9603.212510108948
time_this_iter_s: 120.2269971370697
time_total_s: 9603.212510108948
timers:
  learn_throughput: 3.388
  learn_time_ms: 590.26
  load_throughput: 4989.061
  load_time_ms: 0.401
  sample_time_ms: 128846.532
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 129437.754
timestamp: 1697322934
timesteps_total: 106
training_iteration: 53
trial_id: default

Last checkpoint 52 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000053
server side epoch loop 53
algo.train executed
agent_timesteps_total: 108
connector_metrics: {}
counters:
  num_agent_steps_sampled: 108
  num_agent_steps_trained: 108
  num_env_steps_sampled: 108
  num_env_steps_trained: 108
custom_metrics: {}
date: 2023-10-15_01-37-58
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0263138055801393
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9132379372914632
        entropy_coeff: 0.0
        grad_gnorm: 19.37851889928182
        kl: 0.0033672549388332603
        policy_loss: 0.025778865814208983
        total_loss: 0.5471694809074203
        vf_explained_var: .nan
        vf_loss: 0.5179347566561774
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3210.5
  num_agent_steps_sampled: 108
  num_agent_steps_trained: 108
  num_env_steps_sampled: 108
  num_env_steps_trained: 108
iterations_since_restore: 54
node_ip: 10.27.41.23
num_agent_steps_sampled: 108
num_agent_steps_trained: 108
num_env_steps_sampled: 108
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013879559008084064
num_env_steps_trained: 108
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013879559008084064
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.802427184466021
  ram_util_percent: 47.62135922330096
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9747.309535980225
time_this_iter_s: 144.09702587127686
time_total_s: 9747.309535980225
timers:
  learn_throughput: 3.377
  learn_time_ms: 592.205
  load_throughput: 5014.711
  load_time_ms: 0.399
  sample_time_ms: 125389.275
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 125982.45
timestamp: 1697323078
timesteps_total: 108
training_iteration: 54
trial_id: default

Last checkpoint 53 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000054
server side epoch loop 54
algo.train executed
agent_timesteps_total: 110
connector_metrics: {}
counters:
  num_agent_steps_sampled: 110
  num_agent_steps_trained: 110
  num_env_steps_sampled: 110
  num_env_steps_trained: 110
custom_metrics: {}
date: 2023-10-15_01-39-58
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.5131569027900696
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8828443944454194
        entropy_coeff: 0.0
        grad_gnorm: 8.943863201141358
        kl: 0.0024404001157715054
        policy_loss: -0.0884662002325058
        total_loss: -0.016230698426564535
        vf_explained_var: .nan
        vf_loss: 0.07098319549841108
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3270.5
  num_agent_steps_sampled: 110
  num_agent_steps_trained: 110
  num_env_steps_sampled: 110
  num_env_steps_trained: 110
iterations_since_restore: 55
node_ip: 10.27.41.23
num_agent_steps_sampled: 110
num_agent_steps_trained: 110
num_env_steps_sampled: 110
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016599491486067475
num_env_steps_trained: 110
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016599491486067475
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.168604651162792
  ram_util_percent: 47.16104651162791
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9867.795367002487
time_this_iter_s: 120.48583102226257
time_total_s: 9867.795367002487
timers:
  learn_throughput: 3.455
  learn_time_ms: 578.829
  load_throughput: 5082.465
  load_time_ms: 0.394
  sample_time_ms: 122394.912
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 122974.705
timestamp: 1697323198
timesteps_total: 110
training_iteration: 55
trial_id: default

Last checkpoint 54 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000055
server side epoch loop 55
algo.train executed
agent_timesteps_total: 112
connector_metrics: {}
counters:
  num_agent_steps_sampled: 112
  num_agent_steps_trained: 112
  num_env_steps_sampled: 112
  num_env_steps_trained: 112
custom_metrics: {}
date: 2023-10-15_01-41-59
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2565784513950348
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8057740350564322
        entropy_coeff: 0.0
        grad_gnorm: 10.433375688890616
        kl: 0.017278204783785136
        policy_loss: -0.218196298678716
        total_loss: -0.0816300223271052
        vf_explained_var: .nan
        vf_loss: 0.1321330575553778
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3330.5
  num_agent_steps_sampled: 112
  num_agent_steps_trained: 112
  num_env_steps_sampled: 112
  num_env_steps_trained: 112
iterations_since_restore: 56
node_ip: 10.27.41.23
num_agent_steps_sampled: 112
num_agent_steps_trained: 112
num_env_steps_sampled: 112
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016562001403995805
num_env_steps_trained: 112
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016562001403995805
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.948255813953488
  ram_util_percent: 47.31686046511628
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 9988.55400967598
time_this_iter_s: 120.75864267349243
time_total_s: 9988.55400967598
timers:
  learn_throughput: 3.452
  learn_time_ms: 579.386
  load_throughput: 5184.233
  load_time_ms: 0.386
  sample_time_ms: 122413.958
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 122994.299
timestamp: 1697323319
timesteps_total: 112
training_iteration: 56
trial_id: default

Last checkpoint 55 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000056
server side epoch loop 56
algo.train executed
agent_timesteps_total: 114
connector_metrics: {}
counters:
  num_agent_steps_sampled: 114
  num_agent_steps_trained: 114
  num_env_steps_sampled: 114
  num_env_steps_trained: 114
custom_metrics: {}
date: 2023-10-15_01-44-00
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2565784513950348
        cur_lr: 5.0000000000000016e-05
        entropy: 1.810510621468226
        entropy_coeff: 0.0
        grad_gnorm: 7.4675990025202434
        kl: 0.01726600996165265
        policy_loss: -0.1512288123369217
        total_loss: -0.12779747148354847
        vf_explained_var: .nan
        vf_loss: 0.019001254876396464
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3390.5
  num_agent_steps_sampled: 114
  num_agent_steps_trained: 114
  num_env_steps_sampled: 114
  num_env_steps_trained: 114
iterations_since_restore: 57
node_ip: 10.27.41.23
num_agent_steps_sampled: 114
num_agent_steps_trained: 114
num_env_steps_sampled: 114
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016568215483643664
num_env_steps_trained: 114
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016568215483643664
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.604651162790697
  ram_util_percent: 47.5843023255814
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 10109.267285823822
time_this_iter_s: 120.71327614784241
time_total_s: 10109.267285823822
timers:
  learn_throughput: 3.432
  learn_time_ms: 582.834
  load_throughput: 5213.554
  load_time_ms: 0.384
  sample_time_ms: 122414.254
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 122998.033
timestamp: 1697323440
timesteps_total: 114
training_iteration: 57
trial_id: default

Last checkpoint 56 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000057
server side epoch loop 57
algo.train executed
agent_timesteps_total: 116
connector_metrics: {}
counters:
  num_agent_steps_sampled: 116
  num_agent_steps_trained: 116
  num_env_steps_sampled: 116
  num_env_steps_trained: 116
custom_metrics: {}
date: 2023-10-15_01-46-32
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2565784513950348
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8910429358482361
        entropy_coeff: 0.0
        grad_gnorm: 11.221869150797525
        kl: 0.010507414462820937
        policy_loss: -0.2003336896498998
        total_loss: -0.0354596401254336
        vf_explained_var: .nan
        vf_loss: 0.16217808040504073
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3450.5
  num_agent_steps_sampled: 116
  num_agent_steps_trained: 116
  num_env_steps_sampled: 116
  num_env_steps_trained: 116
iterations_since_restore: 58
node_ip: 10.27.41.23
num_agent_steps_sampled: 116
num_agent_steps_trained: 116
num_env_steps_sampled: 116
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013116283005487108
num_env_steps_trained: 116
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013116283005487108
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.758715596330276
  ram_util_percent: 47.849999999999994
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 10261.749689102173
time_this_iter_s: 152.48240327835083
time_total_s: 10261.749689102173
timers:
  learn_throughput: 2.581
  learn_time_ms: 774.917
  load_throughput: 5253.058
  load_time_ms: 0.381
  sample_time_ms: 125356.436
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 126132.298
timestamp: 1697323592
timesteps_total: 116
training_iteration: 58
trial_id: default

Last checkpoint 57 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000058
server side epoch loop 58
algo.train executed
agent_timesteps_total: 118
connector_metrics: {}
counters:
  num_agent_steps_sampled: 118
  num_agent_steps_trained: 118
  num_env_steps_sampled: 118
  num_env_steps_trained: 118
custom_metrics: {}
date: 2023-10-15_01-48-32
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2565784513950348
        cur_lr: 5.0000000000000016e-05
        entropy: 1.91153391400973
        entropy_coeff: 0.0
        grad_gnorm: 14.268821191787719
        kl: 0.010201771991705754
        policy_loss: -0.11922122438748678
        total_loss: 0.12096956980725129
        vf_explained_var: .nan
        vf_loss: 0.23757324367379623
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3510.5
  num_agent_steps_sampled: 118
  num_agent_steps_trained: 118
  num_env_steps_sampled: 118
  num_env_steps_trained: 118
iterations_since_restore: 59
node_ip: 10.27.41.23
num_agent_steps_sampled: 118
num_agent_steps_trained: 118
num_env_steps_sampled: 118
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016649613924965934
num_env_steps_trained: 118
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016649613924965934
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.464912280701755
  ram_util_percent: 47.57251461988305
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 10381.872794628143
time_this_iter_s: 120.12310552597046
time_total_s: 10381.872794628143
timers:
  learn_throughput: 2.119
  learn_time_ms: 943.734
  load_throughput: 5777.278
  load_time_ms: 0.346
  sample_time_ms: 125176.385
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 126121.022
timestamp: 1697323712
timesteps_total: 118
training_iteration: 59
trial_id: default

Last checkpoint 58 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000059
server side epoch loop 59
algo.train executed
agent_timesteps_total: 120
connector_metrics: {}
counters:
  num_agent_steps_sampled: 120
  num_agent_steps_trained: 120
  num_env_steps_sampled: 120
  num_env_steps_trained: 120
custom_metrics: {}
date: 2023-10-15_01-50-57
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2565784513950348
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9101615687211355
        entropy_coeff: 0.0
        grad_gnorm: 9.04597413962086
        kl: 0.004586863449306596
        policy_loss: -0.19491397539774577
        total_loss: -0.08217592934767405
        vf_explained_var: .nan
        vf_loss: 0.11156115350950131
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3570.5
  num_agent_steps_sampled: 120
  num_agent_steps_trained: 120
  num_env_steps_sampled: 120
  num_env_steps_trained: 120
iterations_since_restore: 60
node_ip: 10.27.41.23
num_agent_steps_sampled: 120
num_agent_steps_trained: 120
num_env_steps_sampled: 120
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013784142784324778
num_env_steps_trained: 120
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013784142784324778
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.323671497584542
  ram_util_percent: 48.86135265700484
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 10526.96727180481
time_this_iter_s: 145.09447717666626
time_total_s: 10526.96727180481
timers:
  learn_throughput: 2.147
  learn_time_ms: 931.415
  load_throughput: 5764.178
  load_time_ms: 0.347
  sample_time_ms: 127629.981
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 128562.286
timestamp: 1697323857
timesteps_total: 120
training_iteration: 60
trial_id: default

Last checkpoint 59 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000060
server side epoch loop 60
algo.train executed
agent_timesteps_total: 122
connector_metrics: {}
counters:
  num_agent_steps_sampled: 122
  num_agent_steps_trained: 122
  num_env_steps_sampled: 122
  num_env_steps_trained: 122
custom_metrics: {}
date: 2023-10-15_01-52-59
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1282892256975174
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8884731531143188
        entropy_coeff: 0.0
        grad_gnorm: 17.42619781891505
        kl: 0.0157582439892091
        policy_loss: -0.22761776546637216
        total_loss: 0.23030164218507707
        vf_explained_var: .nan
        vf_loss: 0.4558977940226517
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3630.5
  num_agent_steps_sampled: 122
  num_agent_steps_trained: 122
  num_env_steps_sampled: 122
  num_env_steps_trained: 122
iterations_since_restore: 61
node_ip: 10.27.41.23
num_agent_steps_sampled: 122
num_agent_steps_trained: 122
num_env_steps_sampled: 122
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01645087099126548
num_env_steps_trained: 122
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01645087099126548
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 12.573563218390806
  ram_util_percent: 47.52183908045978
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 10648.541621208191
time_this_iter_s: 121.57434940338135
time_total_s: 10648.541621208191
timers:
  learn_throughput: 2.14
  learn_time_ms: 934.527
  load_throughput: 5838.803
  load_time_ms: 0.343
  sample_time_ms: 127740.81
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 128676.214
timestamp: 1697323979
timesteps_total: 122
training_iteration: 61
trial_id: default

Last checkpoint 60 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000061
server side epoch loop 61
algo.train executed
agent_timesteps_total: 124
connector_metrics: {}
counters:
  num_agent_steps_sampled: 124
  num_agent_steps_trained: 124
  num_env_steps_sampled: 124
  num_env_steps_trained: 124
custom_metrics: {}
date: 2023-10-15_01-57-30
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1282892256975174
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8799914399782816
        entropy_coeff: 0.0
        grad_gnorm: 15.755039342741172
        kl: 0.024786843823555196
        policy_loss: -0.22953637341658276
        total_loss: 0.13057630912711224
        vf_explained_var: .nan
        vf_loss: 0.3569327921995258
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3690.5
  num_agent_steps_sampled: 124
  num_agent_steps_trained: 124
  num_env_steps_sampled: 124
  num_env_steps_trained: 124
iterations_since_restore: 62
node_ip: 10.27.41.23
num_agent_steps_sampled: 124
num_agent_steps_trained: 124
num_env_steps_sampled: 124
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.007387692105417375
num_env_steps_trained: 124
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.007387692105417375
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.926683937823833
  ram_util_percent: 47.72227979274612
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 10919.26240825653
time_this_iter_s: 270.72078704833984
time_total_s: 10919.26240825653
timers:
  learn_throughput: 2.2
  learn_time_ms: 909.259
  load_throughput: 5905.391
  load_time_ms: 0.339
  sample_time_ms: 142717.295
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 143627.432
timestamp: 1697324250
timesteps_total: 124
training_iteration: 62
trial_id: default

Last checkpoint 61 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000062
server side epoch loop 62
algo.train executed
agent_timesteps_total: 126
connector_metrics: {}
counters:
  num_agent_steps_sampled: 126
  num_agent_steps_trained: 126
  num_env_steps_sampled: 126
  num_env_steps_trained: 126
custom_metrics: {}
date: 2023-10-15_01-59-32
done: false
episode_len_mean: .nan
episode_media: {}
episode_reward_max: .nan
episode_reward_mean: .nan
episode_reward_min: .nan
episodes_this_iter: 0
episodes_total: 0
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.19243383854627602
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8178459405899048
        entropy_coeff: 0.0
        grad_gnorm: 10.826182222366333
        kl: 0.028717299105574058
        policy_loss: -0.2434310535589854
        total_loss: -0.08881897976001103
        vf_explained_var: .nan
        vf_loss: 0.1490858912092032
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3750.5
  num_agent_steps_sampled: 126
  num_agent_steps_trained: 126
  num_env_steps_sampled: 126
  num_env_steps_trained: 126
iterations_since_restore: 63
node_ip: 10.27.41.23
num_agent_steps_sampled: 126
num_agent_steps_trained: 126
num_env_steps_sampled: 126
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016392515944049568
num_env_steps_trained: 126
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016392515944049568
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.794252873563217
  ram_util_percent: 47.5787356321839
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf: {}
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  hist_stats:
    episode_lengths: []
    episode_reward: []
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
time_since_restore: 11041.26951432228
time_this_iter_s: 122.00710606575012
time_total_s: 11041.26951432228
timers:
  learn_throughput: 1.915
  learn_time_ms: 1044.189
  load_throughput: 5843.277
  load_time_ms: 0.342
  sample_time_ms: 142760.367
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 143805.44
timestamp: 1697324372
timesteps_total: 126
training_iteration: 63
trial_id: default

Last checkpoint 62 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000063
server side epoch loop 63
algo.train executed
agent_timesteps_total: 128
connector_metrics: {}
counters:
  num_agent_steps_sampled: 128
  num_agent_steps_trained: 128
  num_env_steps_sampled: 128
  num_env_steps_trained: 128
custom_metrics: {}
date: 2023-10-15_02-02-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 1
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.28865075781941424
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9090065717697144
        entropy_coeff: 0.0
        grad_gnorm: 4.127322277799249
        kl: 0.014639689218347485
        policy_loss: -0.24892343779404957
        total_loss: 4.806931741535664
        vf_explained_var: .nan
        vf_loss: 5.051629496521006
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3810.5
  num_agent_steps_sampled: 128
  num_agent_steps_trained: 128
  num_env_steps_sampled: 128
  num_env_steps_trained: 128
iterations_since_restore: 64
node_ip: 10.27.41.23
num_agent_steps_sampled: 128
num_agent_steps_trained: 128
num_env_steps_sampled: 128
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011352671724897876
num_env_steps_trained: 128
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011352671724897876
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.320318725099604
  ram_util_percent: 47.45179282868526
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 11217.43978381157
time_this_iter_s: 176.17026948928833
time_total_s: 11217.43978381157
timers:
  learn_throughput: 1.911
  learn_time_ms: 1046.305
  load_throughput: 5779.667
  load_time_ms: 0.346
  sample_time_ms: 145965.573
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 147012.759
timestamp: 1697324548
timesteps_total: 128
training_iteration: 64
trial_id: default

Last checkpoint 63 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000064
server side epoch loop 64
algo.train executed
agent_timesteps_total: 130
connector_metrics: {}
counters:
  num_agent_steps_sampled: 130
  num_agent_steps_trained: 130
  num_env_steps_sampled: 130
  num_env_steps_trained: 130
custom_metrics: {}
date: 2023-10-15_02-06-57
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.28865075781941424
        cur_lr: 5.0000000000000016e-05
        entropy: 1.87819415529569
        entropy_coeff: 0.0
        grad_gnorm: 20.608438857396443
        kl: 0.022942139386820295
        policy_loss: -0.22032179335753124
        total_loss: 0.260453932483991
        vf_explained_var: .nan
        vf_loss: 0.4741534593204657
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3870.5
  num_agent_steps_sampled: 130
  num_agent_steps_trained: 130
  num_env_steps_sampled: 130
  num_env_steps_trained: 130
iterations_since_restore: 65
node_ip: 10.27.41.23
num_agent_steps_sampled: 130
num_agent_steps_trained: 130
num_env_steps_sampled: 130
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.007424122916042848
num_env_steps_trained: 130
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.007424122916042848
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.834805194805194
  ram_util_percent: 47.21662337662338
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 11486.83214521408
time_this_iter_s: 269.3923614025116
time_total_s: 11486.83214521408
timers:
  learn_throughput: 1.902
  learn_time_ms: 1051.72
  load_throughput: 5790.039
  load_time_ms: 0.345
  sample_time_ms: 160850.811
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 161903.407
timestamp: 1697324817
timesteps_total: 130
training_iteration: 65
trial_id: default

Last checkpoint 64 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000065
server side epoch loop 65
algo.train executed
agent_timesteps_total: 132
connector_metrics: {}
counters:
  num_agent_steps_sampled: 132
  num_agent_steps_trained: 132
  num_env_steps_sampled: 132
  num_env_steps_trained: 132
custom_metrics: {}
date: 2023-10-15_02-11-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4329761367291213
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8423705399036407
        entropy_coeff: 0.0
        grad_gnorm: 30.076391214629016
        kl: 0.017497054635896347
        policy_loss: -0.26396631201108295
        total_loss: 2.8945288583636284
        vf_explained_var: .nan
        vf_loss: 3.1509194067174877
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3930.5
  num_agent_steps_sampled: 132
  num_agent_steps_trained: 132
  num_env_steps_sampled: 132
  num_env_steps_trained: 132
iterations_since_restore: 66
node_ip: 10.27.41.23
num_agent_steps_sampled: 132
num_agent_steps_trained: 132
num_env_steps_sampled: 132
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.007376942779402204
num_env_steps_trained: 132
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.007376942779402204
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.682170542635658
  ram_util_percent: 47.24573643410852
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 11757.94762969017
time_this_iter_s: 271.1154844760895
time_total_s: 11757.94762969017
timers:
  learn_throughput: 1.895
  learn_time_ms: 1055.315
  load_throughput: 5982.889
  load_time_ms: 0.334
  sample_time_ms: 175882.904
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 176939.075
timestamp: 1697325089
timesteps_total: 132
training_iteration: 66
trial_id: default

Last checkpoint 65 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000066
server side epoch loop 66
algo.train executed
agent_timesteps_total: 134
connector_metrics: {}
counters:
  num_agent_steps_sampled: 134
  num_agent_steps_trained: 134
  num_env_steps_sampled: 134
  num_env_steps_trained: 134
custom_metrics: {}
date: 2023-10-15_02-19-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4329761367291213
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7438907325267792
        entropy_coeff: 0.0
        grad_gnorm: 20.811705224215984
        kl: 0.009038470769761867
        policy_loss: -0.2258483350276947
        total_loss: 0.3399098773797353
        vf_explained_var: .nan
        vf_loss: 0.5618447648894895
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 3990.5
  num_agent_steps_sampled: 134
  num_agent_steps_trained: 134
  num_env_steps_sampled: 134
  num_env_steps_trained: 134
iterations_since_restore: 67
node_ip: 10.27.41.23
num_agent_steps_sampled: 134
num_agent_steps_trained: 134
num_env_steps_sampled: 134
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0041646176144651
num_env_steps_trained: 134
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0041646176144651
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.592992700729926
  ram_util_percent: 47.10087591240876
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 12238.18414235115
time_this_iter_s: 480.2365126609802
time_total_s: 12238.18414235115
timers:
  learn_throughput: 1.87
  learn_time_ms: 1069.762
  load_throughput: 5777.278
  load_time_ms: 0.346
  sample_time_ms: 211820.753
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 212891.386
timestamp: 1697325569
timesteps_total: 134
training_iteration: 67
trial_id: default

Last checkpoint 66 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000067
server side epoch loop 67
algo.train executed
agent_timesteps_total: 136
connector_metrics: {}
counters:
  num_agent_steps_sampled: 136
  num_agent_steps_trained: 136
  num_env_steps_sampled: 136
  num_env_steps_trained: 136
custom_metrics: {}
date: 2023-10-15_02-23-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4329761367291213
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7046099285284677
        entropy_coeff: 0.0
        grad_gnorm: 22.284001382191978
        kl: 0.03236956603041108
        policy_loss: -0.21377160052458447
        total_loss: 0.48518946130449575
        vf_explained_var: .nan
        vf_loss: 0.6849458129836421
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4050.5
  num_agent_steps_sampled: 136
  num_agent_steps_trained: 136
  num_env_steps_sampled: 136
  num_env_steps_trained: 136
iterations_since_restore: 68
node_ip: 10.27.41.23
num_agent_steps_sampled: 136
num_agent_steps_trained: 136
num_env_steps_sampled: 136
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.008340146594077813
num_env_steps_trained: 136
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.008340146594077813
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.482456140350877
  ram_util_percent: 47.71637426900584
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 12477.988478899002
time_this_iter_s: 239.80433654785156
time_total_s: 12477.988478899002
timers:
  learn_throughput: 2.384
  learn_time_ms: 838.993
  load_throughput: 5657.657
  load_time_ms: 0.354
  sample_time_ms: 220783.67
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 221623.557
timestamp: 1697325809
timesteps_total: 136
training_iteration: 68
trial_id: default

Last checkpoint 67 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000068
server side epoch loop 68
algo.train executed
agent_timesteps_total: 138
connector_metrics: {}
counters:
  num_agent_steps_sampled: 138
  num_agent_steps_trained: 138
  num_env_steps_sampled: 138
  num_env_steps_trained: 138
custom_metrics: {}
date: 2023-10-15_02-29-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.6494642050936815
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8015482723712921
        entropy_coeff: 0.0
        grad_gnorm: 13.848056337734063
        kl: 0.021795761413522997
        policy_loss: -0.20655198395252228
        total_loss: 0.06352430023252964
        vf_explained_var: .nan
        vf_loss: 0.25592071826031315
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4110.5
  num_agent_steps_sampled: 138
  num_agent_steps_trained: 138
  num_env_steps_sampled: 138
  num_env_steps_trained: 138
iterations_since_restore: 69
node_ip: 10.27.41.23
num_agent_steps_sampled: 138
num_agent_steps_trained: 138
num_env_steps_sampled: 138
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005555499678602736
num_env_steps_trained: 138
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005555499678602736
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.215789473684211
  ram_util_percent: 47.273684210526326
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 12837.992403030396
time_this_iter_s: 360.00392413139343
time_total_s: 12837.992403030396
timers:
  learn_throughput: 3.025
  learn_time_ms: 661.223
  load_throughput: 5380.072
  load_time_ms: 0.372
  sample_time_ms: 244949.366
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 245611.627
timestamp: 1697326169
timesteps_total: 138
training_iteration: 69
trial_id: default

Last checkpoint 68 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000069
server side epoch loop 69
algo.train executed
agent_timesteps_total: 140
connector_metrics: {}
counters:
  num_agent_steps_sampled: 140
  num_agent_steps_trained: 140
  num_env_steps_sampled: 140
  num_env_steps_trained: 140
custom_metrics: {}
date: 2023-10-15_02-33-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.974196307640523
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7935824890931447
        entropy_coeff: 0.0
        grad_gnorm: 18.510201634963355
        kl: 0.025404325503556417
        policy_loss: -0.21681049366792043
        total_loss: 0.28164968925217787
        vf_explained_var: .nan
        vf_loss: 0.4737113861978287
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4170.5
  num_agent_steps_sampled: 140
  num_agent_steps_trained: 140
  num_env_steps_sampled: 140
  num_env_steps_trained: 140
iterations_since_restore: 70
node_ip: 10.27.41.23
num_agent_steps_sampled: 140
num_agent_steps_trained: 140
num_env_steps_sampled: 140
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.008330299444267391
num_env_steps_trained: 140
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.008330299444267391
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.301749271137027
  ram_util_percent: 47.253061224489784
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 13078.08026599884
time_this_iter_s: 240.08786296844482
time_total_s: 13078.08026599884
timers:
  learn_throughput: 3.001
  learn_time_ms: 666.532
  load_throughput: 5409.562
  load_time_ms: 0.37
  sample_time_ms: 254443.371
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 255110.942
timestamp: 1697326409
timesteps_total: 140
training_iteration: 70
trial_id: default

Last checkpoint 69 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000070
server side epoch loop 70
algo.train executed
agent_timesteps_total: 142
connector_metrics: {}
counters:
  num_agent_steps_sampled: 142
  num_agent_steps_trained: 142
  num_env_steps_sampled: 142
  num_env_steps_trained: 142
custom_metrics: {}
date: 2023-10-15_02-37-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4612944614607841
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8937496880690257
        entropy_coeff: 0.0
        grad_gnorm: 20.35202798048655
        kl: 0.02106696978007676
        policy_loss: -0.23135773539543153
        total_loss: 0.3972509499018391
        vf_explained_var: .nan
        vf_loss: 0.5978236414623704
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4230.5
  num_agent_steps_sampled: 142
  num_agent_steps_trained: 142
  num_env_steps_sampled: 142
  num_env_steps_trained: 142
iterations_since_restore: 71
node_ip: 10.27.41.23
num_agent_steps_sampled: 142
num_agent_steps_trained: 142
num_env_steps_sampled: 142
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.00834094641422821
num_env_steps_trained: 142
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.00834094641422821
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.570175438596493
  ram_util_percent: 47.235380116959064
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 13317.861485481262
time_this_iter_s: 239.78121948242188
time_total_s: 13317.861485481262
timers:
  learn_throughput: 2.881
  learn_time_ms: 694.169
  load_throughput: 5444.673
  load_time_ms: 0.367
  sample_time_ms: 266236.426
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 266931.626
timestamp: 1697326649
timesteps_total: 142
training_iteration: 71
trial_id: default

Last checkpoint 70 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000071
server side epoch loop 71
algo.train executed
agent_timesteps_total: 144
connector_metrics: {}
counters:
  num_agent_steps_sampled: 144
  num_agent_steps_trained: 144
  num_env_steps_sampled: 144
  num_env_steps_trained: 144
custom_metrics: {}
date: 2023-10-15_02-41-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.191941692191176
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8612248778343201
        entropy_coeff: 0.0
        grad_gnorm: 9.81536617676417
        kl: 0.010580628489454588
        policy_loss: -0.13531861106554668
        total_loss: -0.010042131940523783
        vf_explained_var: .nan
        vf_loss: 0.1020843529101209
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4290.5
  num_agent_steps_sampled: 144
  num_agent_steps_trained: 144
  num_env_steps_sampled: 144
  num_env_steps_trained: 144
iterations_since_restore: 72
node_ip: 10.27.41.23
num_agent_steps_sampled: 144
num_agent_steps_trained: 144
num_env_steps_sampled: 144
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.008343228778200387
num_env_steps_trained: 144
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.008343228778200387
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.93187134502924
  ram_util_percent: 47.176900584795334
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 13557.577153921127
time_this_iter_s: 239.7156684398651
time_total_s: 13557.577153921127
timers:
  learn_throughput: 2.891
  learn_time_ms: 691.859
  load_throughput: 5376.624
  load_time_ms: 0.372
  sample_time_ms: 263138.206
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 263831.107
timestamp: 1697326888
timesteps_total: 144
training_iteration: 72
trial_id: default

Last checkpoint 71 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000072
server side epoch loop 72
algo.train executed
agent_timesteps_total: 146
connector_metrics: {}
counters:
  num_agent_steps_sampled: 146
  num_agent_steps_trained: 146
  num_env_steps_sampled: 146
  num_env_steps_trained: 146
custom_metrics: {}
date: 2023-10-15_02-43-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.191941692191176
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8828124463558198
        entropy_coeff: 0.0
        grad_gnorm: 19.624646838506063
        kl: 0.004879218084776464
        policy_loss: -0.07841816842556
        total_loss: 0.47767504341900346
        vf_explained_var: .nan
        vf_loss: 0.545398254909378
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4350.5
  num_agent_steps_sampled: 146
  num_agent_steps_trained: 146
  num_env_steps_sampled: 146
  num_env_steps_trained: 146
iterations_since_restore: 73
node_ip: 10.27.41.23
num_agent_steps_sampled: 146
num_agent_steps_trained: 146
num_env_steps_sampled: 146
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016593101730835738
num_env_steps_trained: 146
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016593101730835738
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.745930232558138
  ram_util_percent: 46.9046511627907
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 13678.109463214874
time_this_iter_s: 120.53230929374695
time_total_s: 13678.109463214874
timers:
  learn_throughput: 3.541
  learn_time_ms: 564.788
  load_throughput: 5151.126
  load_time_ms: 0.388
  sample_time_ms: 263117.764
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 263683.62
timestamp: 1697327009
timesteps_total: 146
training_iteration: 73
trial_id: default

Last checkpoint 72 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000073
server side epoch loop 73
algo.train executed
agent_timesteps_total: 148
connector_metrics: {}
counters:
  num_agent_steps_sampled: 148
  num_agent_steps_trained: 148
  num_env_steps_sampled: 148
  num_env_steps_trained: 148
custom_metrics: {}
date: 2023-10-15_02-45-30
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.095970846095588
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8775374233722686
        entropy_coeff: 0.0
        grad_gnorm: 18.614668030540148
        kl: 0.01734230480954769
        policy_loss: -0.19745748341083527
        total_loss: 0.26112918198729557
        vf_explained_var: .nan
        vf_loss: 0.43958000538744574
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4410.5
  num_agent_steps_sampled: 148
  num_agent_steps_trained: 148
  num_env_steps_sampled: 148
  num_env_steps_trained: 148
iterations_since_restore: 74
node_ip: 10.27.41.23
num_agent_steps_sampled: 148
num_agent_steps_trained: 148
num_env_steps_sampled: 148
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016547509888166313
num_env_steps_trained: 148
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016547509888166313
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.974418604651163
  ram_util_percent: 46.67209302325581
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 13798.973833799362
time_this_iter_s: 120.86437058448792
time_total_s: 13798.973833799362
timers:
  learn_throughput: 3.535
  learn_time_ms: 565.708
  load_throughput: 5181.031
  load_time_ms: 0.386
  sample_time_ms: 257586.264
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 258153.031
timestamp: 1697327130
timesteps_total: 148
training_iteration: 74
trial_id: default

Last checkpoint 73 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000074
server side epoch loop 74
algo.train executed
agent_timesteps_total: 150
connector_metrics: {}
counters:
  num_agent_steps_sampled: 150
  num_agent_steps_trained: 150
  num_env_steps_sampled: 150
  num_env_steps_trained: 150
custom_metrics: {}
date: 2023-10-15_02-51-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.095970846095588
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8416961828867595
        entropy_coeff: 0.0
        grad_gnorm: 12.213518524666627
        kl: 0.014272828830386667
        policy_loss: -0.26763642330964404
        total_loss: -0.04321085711320241
        vf_explained_var: .nan
        vf_loss: 0.20878296321219145
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4470.5
  num_agent_steps_sampled: 150
  num_agent_steps_trained: 150
  num_env_steps_sampled: 150
  num_env_steps_trained: 150
iterations_since_restore: 75
node_ip: 10.27.41.23
num_agent_steps_sampled: 150
num_agent_steps_trained: 150
num_env_steps_sampled: 150
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005577544620124913
num_env_steps_trained: 150
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005577544620124913
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.2080078125
  ram_util_percent: 47.360546875
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 14157.554877996445
time_this_iter_s: 358.5810441970825
time_total_s: 14157.554877996445
timers:
  learn_throughput: 3.531
  learn_time_ms: 566.411
  load_throughput: 5057.949
  load_time_ms: 0.395
  sample_time_ms: 266504.41
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 267071.894
timestamp: 1697327488
timesteps_total: 150
training_iteration: 75
trial_id: default

Last checkpoint 74 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000075
server side epoch loop 75
algo.train executed
agent_timesteps_total: 152
connector_metrics: {}
counters:
  num_agent_steps_sampled: 152
  num_agent_steps_trained: 152
  num_env_steps_sampled: 152
  num_env_steps_trained: 152
custom_metrics: {}
date: 2023-10-15_02-54-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.095970846095588
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8904754181702932
        entropy_coeff: 0.0
        grad_gnorm: 18.2126274228096
        kl: 0.013620765486545375
        policy_loss: -0.2136434813340505
        total_loss: 0.2813888626173139
        vf_explained_var: .nan
        vf_loss: 0.48010438182973303
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4530.5
  num_agent_steps_sampled: 152
  num_agent_steps_trained: 152
  num_env_steps_sampled: 152
  num_env_steps_trained: 152
iterations_since_restore: 76
node_ip: 10.27.41.23
num_agent_steps_sampled: 152
num_agent_steps_trained: 152
num_env_steps_sampled: 152
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011142932850709815
num_env_steps_trained: 152
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011142932850709815
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.137500000000001
  ram_util_percent: 47.639062499999994
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 14337.041127204895
time_this_iter_s: 179.48624920845032
time_total_s: 14337.041127204895
timers:
  learn_throughput: 3.592
  learn_time_ms: 556.852
  load_throughput: 5088.94
  load_time_ms: 0.393
  sample_time_ms: 257351.069
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 257908.988
timestamp: 1697327668
timesteps_total: 152
training_iteration: 76
trial_id: default

Last checkpoint 75 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000076
server side epoch loop 76
algo.train executed
agent_timesteps_total: 154
connector_metrics: {}
counters:
  num_agent_steps_sampled: 154
  num_agent_steps_trained: 154
  num_env_steps_sampled: 154
  num_env_steps_trained: 154
custom_metrics: {}
date: 2023-10-15_02-57-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.095970846095588
        cur_lr: 5.0000000000000016e-05
        entropy: 1.902775812149048
        entropy_coeff: 0.0
        grad_gnorm: 22.22360036770503
        kl: 0.020099259226117282
        policy_loss: -0.2018980662027995
        total_loss: 0.4797952470680078
        vf_explained_var: .nan
        vf_loss: 0.6596651143516283
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4590.5
  num_agent_steps_sampled: 154
  num_agent_steps_trained: 154
  num_env_steps_sampled: 154
  num_env_steps_trained: 154
iterations_since_restore: 77
node_ip: 10.27.41.23
num_agent_steps_sampled: 154
num_agent_steps_trained: 154
num_env_steps_sampled: 154
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0110604682484837
num_env_steps_trained: 154
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0110604682484837
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.53139534883721
  ram_util_percent: 47.8031007751938
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 14517.865587472916
time_this_iter_s: 180.82446026802063
time_total_s: 14517.865587472916
timers:
  learn_throughput: 3.602
  learn_time_ms: 555.289
  load_throughput: 5221.667
  load_time_ms: 0.383
  sample_time_ms: 227411.446
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 227967.788
timestamp: 1697327849
timesteps_total: 154
training_iteration: 77
trial_id: default

Last checkpoint 76 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000077
server side epoch loop 77
algo.train executed
agent_timesteps_total: 156
connector_metrics: {}
counters:
  num_agent_steps_sampled: 156
  num_agent_steps_trained: 156
  num_env_steps_sampled: 156
  num_env_steps_trained: 156
custom_metrics: {}
date: 2023-10-15_02-59-31
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6439562691433827
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8744268616040547
        entropy_coeff: 0.0
        grad_gnorm: 12.240601523717244
        kl: 0.005804716220639724
        policy_loss: -0.13259273370107014
        total_loss: 0.04786812315384547
        vf_explained_var: .nan
        vf_loss: 0.17091815929209891
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4650.5
  num_agent_steps_sampled: 156
  num_agent_steps_trained: 156
  num_env_steps_sampled: 156
  num_env_steps_trained: 156
iterations_since_restore: 78
node_ip: 10.27.41.23
num_agent_steps_sampled: 156
num_agent_steps_trained: 156
num_env_steps_sampled: 156
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016442470388126174
num_env_steps_trained: 156
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016442470388126174
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.486206896551723
  ram_util_percent: 46.47758620689656
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 14639.502143859863
time_this_iter_s: 121.63655638694763
time_total_s: 14639.502143859863
timers:
  learn_throughput: 2.957
  learn_time_ms: 676.429
  load_throughput: 5337.962
  load_time_ms: 0.375
  sample_time_ms: 215473.555
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 216151.018
timestamp: 1697327971
timesteps_total: 156
training_iteration: 78
trial_id: default

Last checkpoint 77 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000078
server side epoch loop 78
algo.train executed
agent_timesteps_total: 158
connector_metrics: {}
counters:
  num_agent_steps_sampled: 158
  num_agent_steps_trained: 158
  num_env_steps_sampled: 158
  num_env_steps_trained: 158
custom_metrics: {}
date: 2023-10-15_03-04-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6439562691433827
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9129350582758586
        entropy_coeff: 0.0
        grad_gnorm: 22.04090735514959
        kl: 0.012864075376516364
        policy_loss: -0.18137423594792684
        total_loss: 0.5397095044453939
        vf_explained_var: .nan
        vf_loss: 0.6999357644153861
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4710.5
  num_agent_steps_sampled: 158
  num_agent_steps_trained: 158
  num_env_steps_sampled: 158
  num_env_steps_trained: 158
iterations_since_restore: 79
node_ip: 10.27.41.23
num_agent_steps_sampled: 158
num_agent_steps_trained: 158
num_env_steps_sampled: 158
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0067208707050749
num_env_steps_trained: 158
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0067208707050749
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.701179245283019
  ram_util_percent: 46.695754716981135
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 14937.082949638367
time_this_iter_s: 297.5808057785034
time_total_s: 14937.082949638367
timers:
  learn_throughput: 2.965
  learn_time_ms: 674.497
  load_throughput: 5579.016
  load_time_ms: 0.358
  sample_time_ms: 209233.302
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 209908.706
timestamp: 1697328268
timesteps_total: 158
training_iteration: 79
trial_id: default

Last checkpoint 78 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000079
server side epoch loop 79
algo.train executed
agent_timesteps_total: 160
connector_metrics: {}
counters:
  num_agent_steps_sampled: 160
  num_agent_steps_trained: 160
  num_env_steps_sampled: 160
  num_env_steps_trained: 160
custom_metrics: {}
date: 2023-10-15_03-06-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6439562691433827
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8455318629741668
        entropy_coeff: 0.0
        grad_gnorm: 11.162620878219604
        kl: 0.007925128340624117
        policy_loss: -0.12193788488705953
        total_loss: 0.028321649630864462
        vf_explained_var: .nan
        vf_loss: 0.1372309733997099
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4770.5
  num_agent_steps_sampled: 160
  num_agent_steps_trained: 160
  num_env_steps_sampled: 160
  num_env_steps_trained: 160
iterations_since_restore: 80
node_ip: 10.27.41.23
num_agent_steps_sampled: 160
num_agent_steps_trained: 160
num_env_steps_sampled: 160
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01660089037333876
num_env_steps_trained: 160
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01660089037333876
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.61279069767442
  ram_util_percent: 48.22790697674419
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 15057.558736085892
time_this_iter_s: 120.47578644752502
time_total_s: 15057.558736085892
timers:
  learn_throughput: 2.986
  learn_time_ms: 669.731
  load_throughput: 5609.983
  load_time_ms: 0.357
  sample_time_ms: 197276.879
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 197947.512
timestamp: 1697328389
timesteps_total: 160
training_iteration: 80
trial_id: default

Last checkpoint 79 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000080
server side epoch loop 80
algo.train executed
agent_timesteps_total: 162
connector_metrics: {}
counters:
  num_agent_steps_sampled: 162
  num_agent_steps_trained: 162
  num_env_steps_sampled: 162
  num_env_steps_trained: 162
custom_metrics: {}
date: 2023-10-15_03-08-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6439562691433827
        cur_lr: 5.0000000000000016e-05
        entropy: 1.903373505671819
        entropy_coeff: 0.0
        grad_gnorm: 21.32006117502848
        kl: 0.020122688943714214
        policy_loss: -0.22388357321421307
        total_loss: 0.2540771280725797
        vf_explained_var: .nan
        vf_loss: 0.4448798711101214
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4830.5
  num_agent_steps_sampled: 162
  num_agent_steps_trained: 162
  num_env_steps_sampled: 162
  num_env_steps_trained: 162
iterations_since_restore: 81
node_ip: 10.27.41.23
num_agent_steps_sampled: 162
num_agent_steps_trained: 162
num_env_steps_sampled: 162
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016606453630811358
num_env_steps_trained: 162
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016606453630811358
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.596511627906976
  ram_util_percent: 47.16976744186047
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 15177.994163751602
time_this_iter_s: 120.43542766571045
time_total_s: 15177.994163751602
timers:
  learn_throughput: 3.081
  learn_time_ms: 649.239
  load_throughput: 5726.794
  load_time_ms: 0.349
  sample_time_ms: 185362.669
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 186012.928
timestamp: 1697328509
timesteps_total: 162
training_iteration: 81
trial_id: default

Last checkpoint 80 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000081
server side epoch loop 81
algo.train executed
agent_timesteps_total: 164
connector_metrics: {}
counters:
  num_agent_steps_sampled: 164
  num_agent_steps_trained: 164
  num_env_steps_sampled: 164
  num_env_steps_trained: 164
custom_metrics: {}
date: 2023-10-15_03-10-30
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8966072221597037
        entropy_coeff: 0.0
        grad_gnorm: 16.795811907450357
        kl: 0.015335948222794589
        policy_loss: -0.23014303147792817
        total_loss: 0.19000129569321872
        vf_explained_var: .nan
        vf_loss: 0.3823268809688064
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4890.5
  num_agent_steps_sampled: 164
  num_agent_steps_trained: 164
  num_env_steps_sampled: 164
  num_env_steps_trained: 164
iterations_since_restore: 82
node_ip: 10.27.41.23
num_agent_steps_sampled: 164
num_agent_steps_trained: 164
num_env_steps_sampled: 164
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01659541075270172
num_env_steps_trained: 164
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01659541075270172
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.116279069767442
  ram_util_percent: 47.11104651162791
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 15298.509701251984
time_this_iter_s: 120.51553750038147
time_total_s: 15298.509701251984
timers:
  learn_throughput: 3.091
  learn_time_ms: 647.022
  load_throughput: 5638.263
  load_time_ms: 0.355
  sample_time_ms: 173444.876
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 174092.918
timestamp: 1697328630
timesteps_total: 164
training_iteration: 82
trial_id: default

Last checkpoint 81 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000082
server side epoch loop 82
algo.train executed
agent_timesteps_total: 166
connector_metrics: {}
counters:
  num_agent_steps_sampled: 166
  num_agent_steps_trained: 166
  num_env_steps_sampled: 166
  num_env_steps_trained: 166
custom_metrics: {}
date: 2023-10-15_03-12-30
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8686479647954306
        entropy_coeff: 0.0
        grad_gnorm: 15.076076996326446
        kl: 0.012700828454399016
        policy_loss: -0.23050056596597035
        total_loss: 0.09770128855016083
        vf_explained_var: .nan
        vf_loss: 0.29688244092709887
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 4950.5
  num_agent_steps_sampled: 166
  num_agent_steps_trained: 166
  num_env_steps_sampled: 166
  num_env_steps_trained: 166
iterations_since_restore: 83
node_ip: 10.27.41.23
num_agent_steps_sampled: 166
num_agent_steps_trained: 166
num_env_steps_sampled: 166
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016597089086714832
num_env_steps_trained: 166
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016597089086714832
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.59127906976744
  ram_util_percent: 48.17732558139535
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 15419.013261556625
time_this_iter_s: 120.50356030464172
time_total_s: 15419.013261556625
timers:
  learn_throughput: 3.105
  learn_time_ms: 644.087
  load_throughput: 5886.329
  load_time_ms: 0.34
  sample_time_ms: 173444.921
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 174090.02
timestamp: 1697328750
timesteps_total: 166
training_iteration: 83
trial_id: default

Last checkpoint 82 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000083
server side epoch loop 83
algo.train executed
agent_timesteps_total: 168
connector_metrics: {}
counters:
  num_agent_steps_sampled: 168
  num_agent_steps_trained: 168
  num_env_steps_sampled: 168
  num_env_steps_trained: 168
custom_metrics: {}
date: 2023-10-15_03-14-32
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9268904268741607
        entropy_coeff: 0.0
        grad_gnorm: 19.67028707166513
        kl: 0.011481693393579916
        policy_loss: -0.2531647026538849
        total_loss: 0.32242544622470937
        vf_explained_var: .nan
        vf_loss: 0.5472770467305963
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5010.5
  num_agent_steps_sampled: 168
  num_agent_steps_trained: 168
  num_env_steps_sampled: 168
  num_env_steps_trained: 168
iterations_since_restore: 84
node_ip: 10.27.41.23
num_agent_steps_sampled: 168
num_agent_steps_trained: 168
num_env_steps_sampled: 168
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016454447951335993
num_env_steps_trained: 168
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016454447951335993
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.453179190751444
  ram_util_percent: 46.89826589595375
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 15540.561224937439
time_this_iter_s: 121.5479633808136
time_total_s: 15540.561224937439
timers:
  learn_throughput: 2.634
  learn_time_ms: 759.198
  load_throughput: 5803.257
  load_time_ms: 0.345
  sample_time_ms: 173398.159
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 174158.376
timestamp: 1697328872
timesteps_total: 168
training_iteration: 84
trial_id: default

Last checkpoint 83 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000084
server side epoch loop 84
algo.train executed
agent_timesteps_total: 170
connector_metrics: {}
counters:
  num_agent_steps_sampled: 170
  num_agent_steps_trained: 170
  num_env_steps_sampled: 170
  num_env_steps_trained: 170
custom_metrics: {}
date: 2023-10-15_03-16-31
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8634830176830293
        entropy_coeff: 0.0
        grad_gnorm: 18.108609584967294
        kl: 0.016065048904723274
        policy_loss: -0.22311344345410664
        total_loss: 0.23372303534609576
        vf_explained_var: .nan
        vf_loss: 0.4172211172789199
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5070.5
  num_agent_steps_sampled: 170
  num_agent_steps_trained: 170
  num_env_steps_sampled: 170
  num_env_steps_trained: 170
iterations_since_restore: 85
node_ip: 10.27.41.23
num_agent_steps_sampled: 170
num_agent_steps_trained: 170
num_env_steps_sampled: 170
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016747521706153772
num_env_steps_trained: 170
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016747521706153772
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.299999999999999
  ram_util_percent: 47.98011695906432
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 15659.982192516327
time_this_iter_s: 119.42096757888794
time_total_s: 15659.982192516327
timers:
  learn_throughput: 2.6
  learn_time_ms: 769.193
  load_throughput: 6018.516
  load_time_ms: 0.332
  sample_time_ms: 149472.174
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 150242.369
timestamp: 1697328991
timesteps_total: 170
training_iteration: 85
trial_id: default

Last checkpoint 84 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000085
server side epoch loop 85
algo.train executed
agent_timesteps_total: 172
connector_metrics: {}
counters:
  num_agent_steps_sampled: 172
  num_agent_steps_trained: 172
  num_env_steps_sampled: 172
  num_env_steps_trained: 172
custom_metrics: {}
date: 2023-10-15_03-20-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8872283200422924
        entropy_coeff: 0.0
        grad_gnorm: 11.710394336779911
        kl: 0.017007293384813237
        policy_loss: -0.22944721281528474
        total_loss: -0.006360483169555664
        vf_explained_var: .nan
        vf_loss: 0.18114785481423798
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5130.5
  num_agent_steps_sampled: 172
  num_agent_steps_trained: 172
  num_env_steps_sampled: 172
  num_env_steps_trained: 172
iterations_since_restore: 86
node_ip: 10.27.41.23
num_agent_steps_sampled: 172
num_agent_steps_trained: 172
num_env_steps_sampled: 172
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.00845305971473763
num_env_steps_trained: 172
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.00845305971473763
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.074777448071217
  ram_util_percent: 47.0593471810089
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 15896.583232164383
time_this_iter_s: 236.60103964805603
time_total_s: 15896.583232164383
timers:
  learn_throughput: 2.588
  learn_time_ms: 772.882
  load_throughput: 5897.088
  load_time_ms: 0.339
  sample_time_ms: 155179.943
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 155953.844
timestamp: 1697329228
timesteps_total: 172
training_iteration: 86
trial_id: default

Last checkpoint 85 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000086
server side epoch loop 86
algo.train executed
agent_timesteps_total: 174
connector_metrics: {}
counters:
  num_agent_steps_sampled: 174
  num_agent_steps_trained: 174
  num_env_steps_sampled: 174
  num_env_steps_trained: 174
custom_metrics: {}
date: 2023-10-15_03-23-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9147413671016693
        entropy_coeff: 0.0
        grad_gnorm: 16.469953727722167
        kl: 0.01241711414962386
        policy_loss: -0.23876204391320546
        total_loss: 0.1663435496389866
        vf_explained_var: .nan
        vf_loss: 0.3744858061761382
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5190.5
  num_agent_steps_sampled: 174
  num_agent_steps_trained: 174
  num_env_steps_sampled: 174
  num_env_steps_trained: 174
iterations_since_restore: 87
node_ip: 10.27.41.23
num_agent_steps_sampled: 174
num_agent_steps_trained: 174
num_env_steps_sampled: 174
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011094574380838404
num_env_steps_trained: 174
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011094574380838404
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.894941634241246
  ram_util_percent: 47.790661478599226
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 16076.85185790062
time_this_iter_s: 180.26862573623657
time_total_s: 16076.85185790062
timers:
  learn_throughput: 2.476
  learn_time_ms: 807.721
  load_throughput: 5768.141
  load_time_ms: 0.347
  sample_time_ms: 155089.508
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 155898.256
timestamp: 1697329408
timesteps_total: 174
training_iteration: 87
trial_id: default

Last checkpoint 86 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000087
server side epoch loop 87
algo.train executed
agent_timesteps_total: 176
connector_metrics: {}
counters:
  num_agent_steps_sampled: 176
  num_agent_steps_trained: 176
  num_env_steps_sampled: 176
  num_env_steps_trained: 176
custom_metrics: {}
date: 2023-10-15_03-26-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8746123135089874
        entropy_coeff: 0.0
        grad_gnorm: 22.20659753183524
        kl: 0.009792369006512066
        policy_loss: -0.18422654370466868
        total_loss: 0.46080107192198433
        vf_explained_var: .nan
        vf_loss: 0.6208802729001036
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5250.5
  num_agent_steps_sampled: 176
  num_agent_steps_trained: 176
  num_env_steps_sampled: 176
  num_env_steps_trained: 176
iterations_since_restore: 88
node_ip: 10.27.41.23
num_agent_steps_sampled: 176
num_agent_steps_trained: 176
num_env_steps_sampled: 176
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01112594352201058
num_env_steps_trained: 176
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01112594352201058
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.238910505836577
  ram_util_percent: 47.914396887159526
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 16256.61224937439
time_this_iter_s: 179.76039147377014
time_total_s: 16256.61224937439
timers:
  learn_throughput: 2.938
  learn_time_ms: 680.818
  load_throughput: 5698.395
  load_time_ms: 0.351
  sample_time_ms: 161028.793
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 161710.637
timestamp: 1697329588
timesteps_total: 176
training_iteration: 88
trial_id: default

Last checkpoint 87 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000088
server side epoch loop 88
algo.train executed
agent_timesteps_total: 178
connector_metrics: {}
counters:
  num_agent_steps_sampled: 178
  num_agent_steps_trained: 178
  num_env_steps_sampled: 178
  num_env_steps_trained: 178
custom_metrics: {}
date: 2023-10-15_03-28-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8552276929219564
        entropy_coeff: 0.0
        grad_gnorm: 21.851476033528645
        kl: 0.014685541708604432
        policy_loss: -0.2578405559062958
        total_loss: 0.23774333894252778
        vf_explained_var: .nan
        vf_loss: 0.4593703031539917
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5310.5
  num_agent_steps_sampled: 178
  num_agent_steps_trained: 178
  num_env_steps_sampled: 178
  num_env_steps_trained: 178
iterations_since_restore: 89
node_ip: 10.27.41.23
num_agent_steps_sampled: 178
num_agent_steps_trained: 178
num_env_steps_sampled: 178
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01662215321846962
num_env_steps_trained: 178
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01662215321846962
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.7894736842105265
  ram_util_percent: 47.20116959064327
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 16376.933891773224
time_this_iter_s: 120.32164239883423
time_total_s: 16376.933891773224
timers:
  learn_throughput: 2.94
  learn_time_ms: 680.165
  load_throughput: 5760.22
  load_time_ms: 0.347
  sample_time_ms: 143303.555
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 143984.723
timestamp: 1697329708
timesteps_total: 178
training_iteration: 89
trial_id: default

Last checkpoint 88 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000089
server side epoch loop 89
algo.train executed
agent_timesteps_total: 180
connector_metrics: {}
counters:
  num_agent_steps_sampled: 180
  num_agent_steps_trained: 180
  num_env_steps_sampled: 180
  num_env_steps_trained: 180
custom_metrics: {}
date: 2023-10-15_03-31-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.824265823761622
        entropy_coeff: 0.0
        grad_gnorm: 21.037626737356184
        kl: 0.014925475650428174
        policy_loss: -0.2561676303545634
        total_loss: 0.3486225779478749
        vf_explained_var: .nan
        vf_loss: 0.5679849635790257
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5370.5
  num_agent_steps_sampled: 180
  num_agent_steps_trained: 180
  num_env_steps_sampled: 180
  num_env_steps_trained: 180
iterations_since_restore: 90
node_ip: 10.27.41.23
num_agent_steps_sampled: 180
num_agent_steps_trained: 180
num_env_steps_sampled: 180
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011149546701964944
num_env_steps_trained: 180
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011149546701964944
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.871484375
  ram_util_percent: 46.903125
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 16556.313756227493
time_this_iter_s: 179.3798644542694
time_total_s: 16556.313756227493
timers:
  learn_throughput: 2.928
  learn_time_ms: 683.022
  load_throughput: 5482.392
  load_time_ms: 0.365
  sample_time_ms: 149191.073
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 149875.126
timestamp: 1697329888
timesteps_total: 180
training_iteration: 90
trial_id: default

Last checkpoint 89 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000090
server side epoch loop 90
algo.train executed
agent_timesteps_total: 182
connector_metrics: {}
counters:
  num_agent_steps_sampled: 182
  num_agent_steps_trained: 182
  num_env_steps_sampled: 182
  num_env_steps_trained: 182
custom_metrics: {}
date: 2023-10-15_03-37-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.85910764336586
        entropy_coeff: 0.0
        grad_gnorm: 16.53495672146479
        kl: 0.013984007640101482
        policy_loss: -0.24208591679732006
        total_loss: 0.15973114967346191
        vf_explained_var: .nan
        vf_loss: 0.3673334163751861
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5430.5
  num_agent_steps_sampled: 182
  num_agent_steps_trained: 182
  num_env_steps_sampled: 182
  num_env_steps_trained: 182
iterations_since_restore: 91
node_ip: 10.27.41.23
num_agent_steps_sampled: 182
num_agent_steps_trained: 182
num_env_steps_sampled: 182
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0055531242244216215
num_env_steps_trained: 182
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0055531242244216215
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.436575875486382
  ram_util_percent: 47.58657587548639
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 16916.47171640396
time_this_iter_s: 360.1579601764679
time_total_s: 16916.47171640396
timers:
  learn_throughput: 2.959
  learn_time_ms: 675.965
  load_throughput: 5488.131
  load_time_ms: 0.364
  sample_time_ms: 173170.509
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 173847.377
timestamp: 1697330248
timesteps_total: 182
training_iteration: 91
trial_id: default

Last checkpoint 90 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000091
server side epoch loop 91
algo.train executed
agent_timesteps_total: 184
connector_metrics: {}
counters:
  num_agent_steps_sampled: 184
  num_agent_steps_trained: 184
  num_env_steps_sampled: 184
  num_env_steps_trained: 184
custom_metrics: {}
date: 2023-10-15_03-39-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4659344037150728
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9070778409639995
        entropy_coeff: 0.0
        grad_gnorm: 43.15880296230316
        kl: 0.0011750993583215556
        policy_loss: -0.1030449777841568
        total_loss: 2.3233471020435292
        vf_explained_var: .nan
        vf_loss: 2.4234943736722925
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5490.5
  num_agent_steps_sampled: 184
  num_agent_steps_trained: 184
  num_env_steps_sampled: 184
  num_env_steps_trained: 184
iterations_since_restore: 92
node_ip: 10.27.41.23
num_agent_steps_sampled: 184
num_agent_steps_trained: 184
num_env_steps_sampled: 184
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016619188128689445
num_env_steps_trained: 184
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016619188128689445
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.919186046511627
  ram_util_percent: 46.060465116279076
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 17036.81486058235
time_this_iter_s: 120.3431441783905
time_total_s: 17036.81486058235
timers:
  learn_throughput: 2.991
  learn_time_ms: 668.65
  load_throughput: 5666.447
  load_time_ms: 0.353
  sample_time_ms: 173160.599
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 173830.135
timestamp: 1697330368
timesteps_total: 184
training_iteration: 92
trial_id: default

Last checkpoint 91 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000092
server side epoch loop 92
algo.train executed
agent_timesteps_total: 186
connector_metrics: {}
counters:
  num_agent_steps_sampled: 186
  num_agent_steps_trained: 186
  num_env_steps_sampled: 186
  num_env_steps_trained: 186
custom_metrics: {}
date: 2023-10-15_03-41-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2329672018575364
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9154508670171102
        entropy_coeff: 0.0
        grad_gnorm: 21.266286245981853
        kl: 0.001724829978532701
        policy_loss: 0.014499001701672872
        total_loss: 0.559259295463562
        vf_explained_var: .nan
        vf_loss: 0.5426336350734345
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5550.5
  num_agent_steps_sampled: 186
  num_agent_steps_trained: 186
  num_env_steps_sampled: 186
  num_env_steps_trained: 186
iterations_since_restore: 93
node_ip: 10.27.41.23
num_agent_steps_sampled: 186
num_agent_steps_trained: 186
num_env_steps_sampled: 186
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01659495382131989
num_env_steps_trained: 186
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01659495382131989
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.138953488372094
  ram_util_percent: 46.388372093023264
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 17157.333729743958
time_this_iter_s: 120.51886916160583
time_total_s: 17157.333729743958
timers:
  learn_throughput: 2.942
  learn_time_ms: 679.85
  load_throughput: 5801.652
  load_time_ms: 0.345
  sample_time_ms: 173150.979
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 173831.688
timestamp: 1697330489
timesteps_total: 186
training_iteration: 93
trial_id: default

Last checkpoint 92 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000093
server side epoch loop 93
algo.train executed
agent_timesteps_total: 188
connector_metrics: {}
counters:
  num_agent_steps_sampled: 188
  num_agent_steps_trained: 188
  num_env_steps_sampled: 188
  num_env_steps_trained: 188
custom_metrics: {}
date: 2023-10-15_03-47-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.6164836009287682
        cur_lr: 5.0000000000000016e-05
        entropy: 1.891268539428711
        entropy_coeff: 0.0
        grad_gnorm: 19.608112814525764
        kl: 0.012437452161490607
        policy_loss: -0.21563472549120585
        total_loss: 0.30608496073012553
        vf_explained_var: .nan
        vf_loss: 0.5140521958155053
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5610.5
  num_agent_steps_sampled: 188
  num_agent_steps_trained: 188
  num_env_steps_sampled: 188
  num_env_steps_trained: 188
iterations_since_restore: 94
node_ip: 10.27.41.23
num_agent_steps_sampled: 188
num_agent_steps_trained: 188
num_env_steps_sampled: 188
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005574003037289871
num_env_steps_trained: 188
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005574003037289871
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.3388671875
  ram_util_percent: 47.3482421875
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 17516.1426281929
time_this_iter_s: 358.8088984489441
time_total_s: 17516.1426281929
timers:
  learn_throughput: 3.582
  learn_time_ms: 558.296
  load_throughput: 5599.872
  load_time_ms: 0.357
  sample_time_ms: 196998.593
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 197557.776
timestamp: 1697330848
timesteps_total: 188
training_iteration: 94
trial_id: default

Last checkpoint 93 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000094
server side epoch loop 94
algo.train executed
agent_timesteps_total: 190
connector_metrics: {}
counters:
  num_agent_steps_sampled: 190
  num_agent_steps_trained: 190
  num_env_steps_sampled: 190
  num_env_steps_trained: 190
custom_metrics: {}
date: 2023-10-15_03-52-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.6164836009287682
        cur_lr: 5.0000000000000016e-05
        entropy: 1.9011540353298186
        entropy_coeff: 0.0
        grad_gnorm: 18.379885237912337
        kl: 0.018729377064058403
        policy_loss: -0.2287886987129847
        total_loss: 0.23970074703296027
        vf_explained_var: .nan
        vf_loss: 0.4569430902217088
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5670.5
  num_agent_steps_sampled: 190
  num_agent_steps_trained: 190
  num_env_steps_sampled: 190
  num_env_steps_trained: 190
iterations_since_restore: 95
node_ip: 10.27.41.23
num_agent_steps_sampled: 190
num_agent_steps_trained: 190
num_env_steps_sampled: 190
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.006654842621566977
num_env_steps_trained: 190
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.006654842621566977
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.408624708624709
  ram_util_percent: 47.24825174825175
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 17816.67599749565
time_this_iter_s: 300.53336930274963
time_total_s: 17816.67599749565
timers:
  learn_throughput: 3.628
  learn_time_ms: 551.306
  load_throughput: 5529.006
  load_time_ms: 0.362
  sample_time_ms: 215116.815
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 215669.014
timestamp: 1697331148
timesteps_total: 190
training_iteration: 95
trial_id: default

Last checkpoint 94 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000095
server side epoch loop 95
algo.train executed
agent_timesteps_total: 192
connector_metrics: {}
counters:
  num_agent_steps_sampled: 192
  num_agent_steps_trained: 192
  num_env_steps_sampled: 192
  num_env_steps_trained: 192
custom_metrics: {}
date: 2023-10-15_03-57-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.6164836009287682
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8826854646205902
        entropy_coeff: 0.0
        grad_gnorm: 17.63202263712883
        kl: 0.02446940861955227
        policy_loss: -0.24555517137050628
        total_loss: 0.18943755174987018
        vf_explained_var: .nan
        vf_loss: 0.419907727498988
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5730.5
  num_agent_steps_sampled: 192
  num_agent_steps_trained: 192
  num_env_steps_sampled: 192
  num_env_steps_trained: 192
iterations_since_restore: 96
node_ip: 10.27.41.23
num_agent_steps_sampled: 192
num_agent_steps_trained: 192
num_env_steps_sampled: 192
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.00667820972215449
num_env_steps_trained: 192
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.00667820972215449
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.017096018735362
  ram_util_percent: 47.61475409836066
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 18116.157778024673
time_this_iter_s: 299.4817805290222
time_total_s: 18116.157778024673
timers:
  learn_throughput: 3.629
  learn_time_ms: 551.119
  load_throughput: 5615.617
  load_time_ms: 0.356
  sample_time_ms: 221405.09
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 221957.087
timestamp: 1697331448
timesteps_total: 192
training_iteration: 96
trial_id: default

Last checkpoint 95 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000096
server side epoch loop 96
algo.train executed
agent_timesteps_total: 194
connector_metrics: {}
counters:
  num_agent_steps_sampled: 194
  num_agent_steps_trained: 194
  num_env_steps_sampled: 194
  num_env_steps_trained: 194
custom_metrics: {}
date: 2023-10-15_04-03-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.9247254013931524
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8411932031313578
        entropy_coeff: 0.0
        grad_gnorm: 13.531942570706208
        kl: 0.013128166844398947
        policy_loss: -0.15100355048974354
        total_loss: 0.09251214439670245
        vf_explained_var: .nan
        vf_loss: 0.23137574062978578
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5790.5
  num_agent_steps_sampled: 194
  num_agent_steps_trained: 194
  num_env_steps_sampled: 194
  num_env_steps_trained: 194
iterations_since_restore: 97
node_ip: 10.27.41.23
num_agent_steps_sampled: 194
num_agent_steps_trained: 194
num_env_steps_sampled: 194
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005546800493723623
num_env_steps_trained: 194
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005546800493723623
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.982101167315173
  ram_util_percent: 47.26556420233463
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 18476.726353645325
time_this_iter_s: 360.56857562065125
time_total_s: 18476.726353645325
timers:
  learn_throughput: 3.962
  learn_time_ms: 504.763
  load_throughput: 5457.78
  load_time_ms: 0.366
  sample_time_ms: 239481.325
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 239987.079
timestamp: 1697331808
timesteps_total: 194
training_iteration: 97
trial_id: default

Last checkpoint 96 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000097
server side epoch loop 97
algo.train executed
agent_timesteps_total: 196
connector_metrics: {}
counters:
  num_agent_steps_sampled: 196
  num_agent_steps_trained: 196
  num_env_steps_sampled: 196
  num_env_steps_trained: 196
custom_metrics: {}
date: 2023-10-15_04-09-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.9247254013931524
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8831898232301076
        entropy_coeff: 0.0
        grad_gnorm: 14.867920517921448
        kl: 0.004404584108124254
        policy_loss: 0.04100401798884074
        total_loss: 0.3049175022480389
        vf_explained_var: .nan
        vf_loss: 0.2598404469650025
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5850.5
  num_agent_steps_sampled: 196
  num_agent_steps_trained: 196
  num_env_steps_sampled: 196
  num_env_steps_trained: 196
iterations_since_restore: 98
node_ip: 10.27.41.23
num_agent_steps_sampled: 196
num_agent_steps_trained: 196
num_env_steps_sampled: 196
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005566912692105596
num_env_steps_trained: 196
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005566912692105596
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.048538011695905
  ram_util_percent: 47.36023391812865
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 18835.992223739624
time_this_iter_s: 359.2658700942993
time_total_s: 18835.992223739624
timers:
  learn_throughput: 3.958
  learn_time_ms: 505.326
  load_throughput: 5453.522
  load_time_ms: 0.367
  sample_time_ms: 257431.302
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 257937.632
timestamp: 1697332168
timesteps_total: 196
training_iteration: 98
trial_id: default

Last checkpoint 97 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000098
server side epoch loop 98
algo.train executed
agent_timesteps_total: 198
connector_metrics: {}
counters:
  num_agent_steps_sampled: 198
  num_agent_steps_trained: 198
  num_env_steps_sampled: 198
  num_env_steps_trained: 198
custom_metrics: {}
date: 2023-10-15_04-11-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4623627006965762
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8395701388518015
        entropy_coeff: 0.0
        grad_gnorm: 12.415293645858764
        kl: 0.01665078090870035
        policy_loss: -0.21407304306825
        total_loss: -0.0029040813446044923
        vf_explained_var: .nan
        vf_loss: 0.2034702543753762
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5910.5
  num_agent_steps_sampled: 198
  num_agent_steps_trained: 198
  num_env_steps_sampled: 198
  num_env_steps_trained: 198
iterations_since_restore: 99
node_ip: 10.27.41.23
num_agent_steps_sampled: 198
num_agent_steps_trained: 198
num_env_steps_sampled: 198
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016585283896733784
num_env_steps_trained: 198
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016585283896733784
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.884302325581395
  ram_util_percent: 48.13255813953489
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 18956.581403017044
time_this_iter_s: 120.58917927742004
time_total_s: 18956.581403017044
timers:
  learn_throughput: 3.905
  learn_time_ms: 512.121
  load_throughput: 5304.545
  load_time_ms: 0.377
  sample_time_ms: 257451.23
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 257964.38
timestamp: 1697332288
timesteps_total: 198
training_iteration: 99
trial_id: default

Last checkpoint 98 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000099
server side epoch loop 99
algo.train executed
agent_timesteps_total: 200
connector_metrics: {}
counters:
  num_agent_steps_sampled: 200
  num_agent_steps_trained: 200
  num_env_steps_sampled: 200
  num_env_steps_trained: 200
custom_metrics: {}
date: 2023-10-15_04-16-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4623627006965762
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8537965655326842
        entropy_coeff: 0.0
        grad_gnorm: 17.896548198660216
        kl: 0.019774867671397564
        policy_loss: -0.21026016573111217
        total_loss: 0.2425116144120693
        vf_explained_var: .nan
        vf_loss: 0.44362861866441866
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 5970.5
  num_agent_steps_sampled: 200
  num_agent_steps_trained: 200
  num_env_steps_sampled: 200
  num_env_steps_trained: 200
iterations_since_restore: 100
node_ip: 10.27.41.23
num_agent_steps_sampled: 200
num_agent_steps_trained: 200
num_env_steps_sampled: 200
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.006673774023168887
num_env_steps_trained: 200
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.006673774023168887
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.362295081967213
  ram_util_percent: 47.628337236533966
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 19256.262204647064
time_this_iter_s: 299.68080163002014
time_total_s: 19256.262204647064
timers:
  learn_throughput: 3.917
  learn_time_ms: 510.62
  load_throughput: 5599.498
  load_time_ms: 0.357
  sample_time_ms: 269482.863
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 269994.482
timestamp: 1697332588
timesteps_total: 200
training_iteration: 100
trial_id: default

Last checkpoint 99 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000100
server side epoch loop 100
algo.train executed
agent_timesteps_total: 202
connector_metrics: {}
counters:
  num_agent_steps_sampled: 202
  num_agent_steps_trained: 202
  num_env_steps_sampled: 202
  num_env_steps_trained: 202
custom_metrics: {}
date: 2023-10-15_04-18-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4623627006965762
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7802973488966625
        entropy_coeff: 0.0
        grad_gnorm: 12.051800210773944
        kl: 0.025763866543517602
        policy_loss: -0.23588687976201375
        total_loss: -0.030899514257907868
        vf_explained_var: .nan
        vf_loss: 0.19307511464794516
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6030.5
  num_agent_steps_sampled: 202
  num_agent_steps_trained: 202
  num_env_steps_sampled: 202
  num_env_steps_trained: 202
iterations_since_restore: 101
node_ip: 10.27.41.23
num_agent_steps_sampled: 202
num_agent_steps_trained: 202
num_env_steps_sampled: 202
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0165835947371542
num_env_steps_trained: 202
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0165835947371542
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.341279069767443
  ram_util_percent: 47.84244186046512
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 19376.863627910614
time_this_iter_s: 120.6014232635498
time_total_s: 19376.863627910614
timers:
  learn_throughput: 3.892
  learn_time_ms: 513.875
  load_throughput: 5617.873
  load_time_ms: 0.356
  sample_time_ms: 245523.955
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 246038.831
timestamp: 1697332709
timesteps_total: 202
training_iteration: 101
trial_id: default

Last checkpoint 100 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000101
server side epoch loop 101
algo.train executed
agent_timesteps_total: 204
connector_metrics: {}
counters:
  num_agent_steps_sampled: 204
  num_agent_steps_trained: 204
  num_env_steps_sampled: 204
  num_env_steps_trained: 204
custom_metrics: {}
date: 2023-10-15_04-20-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.6935440510448646
        cur_lr: 5.0000000000000016e-05
        entropy: 1.812176118294398
        entropy_coeff: 0.0
        grad_gnorm: 15.05525674968958
        kl: 0.026069095319204885
        policy_loss: -0.2599534859259923
        total_loss: 0.06088035404682159
        vf_explained_var: .nan
        vf_loss: 0.30275377500559747
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6090.5
  num_agent_steps_sampled: 204
  num_agent_steps_trained: 204
  num_env_steps_sampled: 204
  num_env_steps_trained: 204
iterations_since_restore: 102
node_ip: 10.27.41.23
num_agent_steps_sampled: 204
num_agent_steps_trained: 204
num_env_steps_sampled: 204
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01656352787303255
num_env_steps_trained: 204
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01656352787303255
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.005202312138728
  ram_util_percent: 47.32023121387283
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 19497.611181735992
time_this_iter_s: 120.74755382537842
time_total_s: 19497.611181735992
timers:
  learn_throughput: 3.814
  learn_time_ms: 524.397
  load_throughput: 5618.249
  load_time_ms: 0.356
  sample_time_ms: 245553.868
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 246079.27
timestamp: 1697332829
timesteps_total: 204
training_iteration: 102
trial_id: default

Last checkpoint 101 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000102
server side epoch loop 102
algo.train executed
agent_timesteps_total: 206
connector_metrics: {}
counters:
  num_agent_steps_sampled: 206
  num_agent_steps_trained: 206
  num_env_steps_sampled: 206
  num_env_steps_trained: 206
custom_metrics: {}
date: 2023-10-15_04-22-58
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0403160765672963
        cur_lr: 5.0000000000000016e-05
        entropy: 1.828851612408956
        entropy_coeff: 0.0
        grad_gnorm: 15.435996814568837
        kl: 0.016749771712238726
        policy_loss: -0.15626926521460216
        total_loss: 0.16256020509948332
        vf_explained_var: .nan
        vf_loss: 0.30140441508726024
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6150.5
  num_agent_steps_sampled: 206
  num_agent_steps_trained: 206
  num_env_steps_sampled: 206
  num_env_steps_trained: 206
iterations_since_restore: 103
node_ip: 10.27.41.23
num_agent_steps_sampled: 206
num_agent_steps_trained: 206
num_env_steps_sampled: 206
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013465583560876319
num_env_steps_trained: 206
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013465583560876319
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.916037735849056
  ram_util_percent: 47.908490566037734
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 19646.138349056244
time_this_iter_s: 148.52716732025146
time_total_s: 19646.138349056244
timers:
  learn_throughput: 3.91
  learn_time_ms: 511.528
  load_throughput: 5588.307
  load_time_ms: 0.358
  sample_time_ms: 248367.552
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 248880.093
timestamp: 1697332978
timesteps_total: 206
training_iteration: 103
trial_id: default

Last checkpoint 102 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000103
server side epoch loop 103
algo.train executed
agent_timesteps_total: 208
connector_metrics: {}
counters:
  num_agent_steps_sampled: 208
  num_agent_steps_trained: 208
  num_env_steps_sampled: 208
  num_env_steps_trained: 208
custom_metrics: {}
date: 2023-10-15_04-24-59
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0403160765672963
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8738667666912079
        entropy_coeff: 0.0
        grad_gnorm: 15.123938096563021
        kl: 0.011794014664095206
        policy_loss: -0.12486310799916585
        total_loss: 0.18378168353810906
        vf_explained_var: .nan
        vf_loss: 0.2963752856228287
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6210.5
  num_agent_steps_sampled: 208
  num_agent_steps_trained: 208
  num_env_steps_sampled: 208
  num_env_steps_trained: 208
iterations_since_restore: 104
node_ip: 10.27.41.23
num_agent_steps_sampled: 208
num_agent_steps_trained: 208
num_env_steps_sampled: 208
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658788867289768
num_env_steps_trained: 208
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658788867289768
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.967441860465117
  ram_util_percent: 46.704651162790704
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 19766.708563566208
time_this_iter_s: 120.57021450996399
time_total_s: 19766.708563566208
timers:
  learn_throughput: 3.881
  learn_time_ms: 515.282
  load_throughput: 5650.036
  load_time_ms: 0.354
  sample_time_ms: 224539.927
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 225056.228
timestamp: 1697333099
timesteps_total: 208
training_iteration: 104
trial_id: default

Last checkpoint 103 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000104
server side epoch loop 104
algo.train executed
agent_timesteps_total: 210
connector_metrics: {}
counters:
  num_agent_steps_sampled: 210
  num_agent_steps_trained: 210
  num_env_steps_sampled: 210
  num_env_steps_trained: 210
custom_metrics: {}
date: 2023-10-15_04-26-59
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0403160765672963
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7166211207707722
        entropy_coeff: 0.0
        grad_gnorm: 14.909201645851136
        kl: 0.009192878538548636
        policy_loss: -0.21388781666755677
        total_loss: 0.07675982142488162
        vf_explained_var: .nan
        vf_loss: 0.28108413291192846
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6270.5
  num_agent_steps_sampled: 210
  num_agent_steps_trained: 210
  num_env_steps_sampled: 210
  num_env_steps_trained: 210
iterations_since_restore: 105
node_ip: 10.27.41.23
num_agent_steps_sampled: 210
num_agent_steps_trained: 210
num_env_steps_sampled: 210
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016559177141663082
num_env_steps_trained: 210
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016559177141663082
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.669767441860465
  ram_util_percent: 47.28720930232558
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 19887.487889051437
time_this_iter_s: 120.77932548522949
time_total_s: 19887.487889051437
timers:
  learn_throughput: 3.775
  learn_time_ms: 529.811
  load_throughput: 5388.366
  load_time_ms: 0.371
  sample_time_ms: 206549.955
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 207080.819
timestamp: 1697333219
timesteps_total: 210
training_iteration: 105
trial_id: default

Last checkpoint 104 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000105
server side epoch loop 105
algo.train executed
agent_timesteps_total: 212
connector_metrics: {}
counters:
  num_agent_steps_sampled: 212
  num_agent_steps_trained: 212
  num_env_steps_sampled: 212
  num_env_steps_trained: 212
custom_metrics: {}
date: 2023-10-15_04-29-00
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0403160765672963
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8238702813784282
        entropy_coeff: 0.0
        grad_gnorm: 16.40286795794964
        kl: 0.017956940822962984
        policy_loss: -0.23742855588595072
        total_loss: 0.14023520424962044
        vf_explained_var: .nan
        vf_loss: 0.35898287107750854
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6330.5
  num_agent_steps_sampled: 212
  num_agent_steps_trained: 212
  num_env_steps_sampled: 212
  num_env_steps_trained: 212
iterations_since_restore: 106
node_ip: 10.27.41.23
num_agent_steps_sampled: 212
num_agent_steps_trained: 212
num_env_steps_sampled: 212
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658997001857133
num_env_steps_trained: 212
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658997001857133
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.897093023255813
  ram_util_percent: 46.80174418604651
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 20008.042962789536
time_this_iter_s: 120.55507373809814
time_total_s: 20008.042962789536
timers:
  learn_throughput: 3.695
  learn_time_ms: 541.222
  load_throughput: 5470.237
  load_time_ms: 0.366
  sample_time_ms: 188645.88
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 189188.151
timestamp: 1697333340
timesteps_total: 212
training_iteration: 106
trial_id: default

Last checkpoint 105 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000106
server side epoch loop 106
algo.train executed
agent_timesteps_total: 214
connector_metrics: {}
counters:
  num_agent_steps_sampled: 214
  num_agent_steps_trained: 214
  num_env_steps_sampled: 214
  num_env_steps_trained: 214
custom_metrics: {}
date: 2023-10-15_04-31-01
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0403160765672963
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7747499426205953
        entropy_coeff: 0.0
        grad_gnorm: 9.734124120076498
        kl: 0.01654955199761995
        policy_loss: -0.2001261939605077
        total_loss: -0.10253853599230449
        vf_explained_var: .nan
        vf_loss: 0.08037089314311743
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6390.5
  num_agent_steps_sampled: 214
  num_agent_steps_trained: 214
  num_env_steps_sampled: 214
  num_env_steps_trained: 214
iterations_since_restore: 107
node_ip: 10.27.41.23
num_agent_steps_sampled: 214
num_agent_steps_trained: 214
num_env_steps_sampled: 214
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01657820325740415
num_env_steps_trained: 214
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01657820325740415
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.97732558139535
  ram_util_percent: 47.31744186046512
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 20128.683593273163
time_this_iter_s: 120.64063048362732
time_total_s: 20128.683593273163
timers:
  learn_throughput: 3.755
  learn_time_ms: 532.605
  load_throughput: 5651.178
  load_time_ms: 0.354
  sample_time_ms: 164661.824
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 165195.363
timestamp: 1697333461
timesteps_total: 214
training_iteration: 107
trial_id: default

Last checkpoint 106 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000107
server side epoch loop 107
algo.train executed
agent_timesteps_total: 216
connector_metrics: {}
counters:
  num_agent_steps_sampled: 216
  num_agent_steps_trained: 216
  num_env_steps_sampled: 216
  num_env_steps_trained: 216
custom_metrics: {}
date: 2023-10-15_04-33-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0403160765672963
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8474219957987468
        entropy_coeff: 0.0
        grad_gnorm: 6.140330325563749
        kl: 0.019084004042451853
        policy_loss: -0.22437294920285542
        total_loss: -0.1695834110180537
        vf_explained_var: .nan
        vf_loss: 0.03493614394780404
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6450.5
  num_agent_steps_sampled: 216
  num_agent_steps_trained: 216
  num_env_steps_sampled: 216
  num_env_steps_trained: 216
iterations_since_restore: 108
node_ip: 10.27.41.23
num_agent_steps_sampled: 216
num_agent_steps_trained: 216
num_env_steps_sampled: 216
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013567254491621262
num_env_steps_trained: 216
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013567254491621262
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.704285714285714
  ram_util_percent: 47.313809523809525
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 20276.097667455673
time_this_iter_s: 147.41407418251038
time_total_s: 20276.097667455673
timers:
  learn_throughput: 3.756
  learn_time_ms: 532.491
  load_throughput: 5705.76
  load_time_ms: 0.351
  sample_time_ms: 143476.767
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 144010.182
timestamp: 1697333608
timesteps_total: 216
training_iteration: 108
trial_id: default

Last checkpoint 107 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000108
server side epoch loop 108
algo.train executed
agent_timesteps_total: 218
connector_metrics: {}
counters:
  num_agent_steps_sampled: 218
  num_agent_steps_trained: 218
  num_env_steps_sampled: 218
  num_env_steps_trained: 218
custom_metrics: {}
date: 2023-10-15_04-35-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0403160765672963
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6288997530937195
        entropy_coeff: 0.0
        grad_gnorm: 7.758200917641322
        kl: 0.020149539048422108
        policy_loss: -0.2313076655069987
        total_loss: -0.14415842692057293
        vf_explained_var: .nan
        vf_loss: 0.06618734964188965
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6510.5
  num_agent_steps_sampled: 218
  num_agent_steps_trained: 218
  num_env_steps_sampled: 218
  num_env_steps_trained: 218
iterations_since_restore: 109
node_ip: 10.27.41.23
num_agent_steps_sampled: 218
num_agent_steps_trained: 218
num_env_steps_sampled: 218
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01657599626827623
num_env_steps_trained: 218
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01657599626827623
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.435260115606935
  ram_util_percent: 47.80867052023121
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 20396.754372119904
time_this_iter_s: 120.65670466423035
time_total_s: 20396.754372119904
timers:
  learn_throughput: 3.779
  learn_time_ms: 529.251
  load_throughput: 5955.703
  load_time_ms: 0.336
  sample_time_ms: 143486.788
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 144016.939
timestamp: 1697333729
timesteps_total: 218
training_iteration: 109
trial_id: default

Last checkpoint 108 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000109
server side epoch loop 109
algo.train executed
agent_timesteps_total: 220
connector_metrics: {}
counters:
  num_agent_steps_sampled: 220
  num_agent_steps_trained: 220
  num_env_steps_sampled: 220
  num_env_steps_trained: 220
custom_metrics: {}
date: 2023-10-15_04-37-30
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5604741148509451
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7053170144557952
        entropy_coeff: 0.0
        grad_gnorm: 7.331112976868948
        kl: 0.018716488844772054
        policy_loss: -0.23001836935679118
        total_loss: -0.14617162942886353
        vf_explained_var: .nan
        vf_loss: 0.05464013914461248
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6570.5
  num_agent_steps_sampled: 220
  num_agent_steps_trained: 220
  num_env_steps_sampled: 220
  num_env_steps_trained: 220
iterations_since_restore: 110
node_ip: 10.27.41.23
num_agent_steps_sampled: 220
num_agent_steps_trained: 220
num_env_steps_sampled: 220
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01656145076954228
num_env_steps_trained: 220
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01656145076954228
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.05
  ram_util_percent: 47.01220930232558
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 20517.51706624031
time_this_iter_s: 120.7626941204071
time_total_s: 20517.51706624031
timers:
  learn_throughput: 3.703
  learn_time_ms: 540.098
  load_throughput: 5883.852
  load_time_ms: 0.34
  sample_time_ms: 125584.12
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 126125.124
timestamp: 1697333850
timesteps_total: 220
training_iteration: 110
trial_id: default

Last checkpoint 109 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000110
server side epoch loop 110
algo.train executed
agent_timesteps_total: 222
connector_metrics: {}
counters:
  num_agent_steps_sampled: 222
  num_agent_steps_trained: 222
  num_env_steps_sampled: 222
  num_env_steps_trained: 222
custom_metrics: {}
date: 2023-10-15_04-39-57
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5604741148509451
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8402799427509309
        entropy_coeff: 0.0
        grad_gnorm: 12.025092410047849
        kl: 0.01283854060069037
        policy_loss: -0.2501655489206314
        total_loss: -0.03726965934038162
        vf_explained_var: .nan
        vf_loss: 0.19286167920848432
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6630.5
  num_agent_steps_sampled: 222
  num_agent_steps_trained: 222
  num_env_steps_sampled: 222
  num_env_steps_trained: 222
iterations_since_restore: 111
node_ip: 10.27.41.23
num_agent_steps_sampled: 222
num_agent_steps_trained: 222
num_env_steps_sampled: 222
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013537001227272276
num_env_steps_trained: 222
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013537001227272276
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.985781990521327
  ram_util_percent: 47.19099526066349
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 20665.26056933403
time_this_iter_s: 147.74350309371948
time_total_s: 20665.26056933403
timers:
  learn_throughput: 3.7
  learn_time_ms: 540.542
  load_throughput: 5858.376
  load_time_ms: 0.341
  sample_time_ms: 128297.882
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 128839.333
timestamp: 1697333997
timesteps_total: 222
training_iteration: 111
trial_id: default

Last checkpoint 110 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000111
server side epoch loop 111
algo.train executed
agent_timesteps_total: 224
connector_metrics: {}
counters:
  num_agent_steps_sampled: 224
  num_agent_steps_trained: 224
  num_env_steps_sampled: 224
  num_env_steps_trained: 224
custom_metrics: {}
date: 2023-10-15_04-43-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5604741148509451
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8593205392360688
        entropy_coeff: 0.0
        grad_gnorm: 10.072605416178703
        kl: 0.012481652459852437
        policy_loss: -0.25034518241882325
        total_loss: -0.1024235357840856
        vf_explained_var: .nan
        vf_loss: 0.1284443537639163
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6690.5
  num_agent_steps_sampled: 224
  num_agent_steps_trained: 224
  num_env_steps_sampled: 224
  num_env_steps_trained: 224
iterations_since_restore: 112
node_ip: 10.27.41.23
num_agent_steps_sampled: 224
num_agent_steps_trained: 224
num_env_steps_sampled: 224
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.00947583734889246
num_env_steps_trained: 224
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.00947583734889246
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.879734219269103
  ram_util_percent: 47.78604651162791
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 20876.324097156525
time_this_iter_s: 211.0635278224945
time_total_s: 20876.324097156525
timers:
  learn_throughput: 3.781
  learn_time_ms: 528.905
  load_throughput: 5468.81
  load_time_ms: 0.366
  sample_time_ms: 137341.08
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 137870.926
timestamp: 1697334208
timesteps_total: 224
training_iteration: 112
trial_id: default

Last checkpoint 111 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000112
server side epoch loop 112
algo.train executed
agent_timesteps_total: 226
connector_metrics: {}
counters:
  num_agent_steps_sampled: 226
  num_agent_steps_trained: 226
  num_env_steps_sampled: 226
  num_env_steps_trained: 226
custom_metrics: {}
date: 2023-10-15_04-47-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5604741148509451
        cur_lr: 5.0000000000000016e-05
        entropy: 1.809684157371521
        entropy_coeff: 0.0
        grad_gnorm: 13.513022540012996
        kl: 0.017946269100987896
        policy_loss: -0.2462418258190155
        total_loss: 0.026700537900129953
        vf_explained_var: .nan
        vf_loss: 0.24493767054057874
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6750.5
  num_agent_steps_sampled: 226
  num_agent_steps_trained: 226
  num_env_steps_sampled: 226
  num_env_steps_trained: 226
iterations_since_restore: 113
node_ip: 10.27.41.23
num_agent_steps_sampled: 226
num_agent_steps_trained: 226
num_env_steps_sampled: 226
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.008350860249430738
num_env_steps_trained: 226
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.008350860249430738
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.403812316715541
  ram_util_percent: 47.08504398826979
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 21115.82079935074
time_this_iter_s: 239.49670219421387
time_total_s: 21115.82079935074
timers:
  learn_throughput: 3.765
  learn_time_ms: 531.217
  load_throughput: 5443.26
  load_time_ms: 0.367
  sample_time_ms: 146435.692
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 146967.874
timestamp: 1697334448
timesteps_total: 226
training_iteration: 113
trial_id: default

Last checkpoint 112 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000113
server side epoch loop 113
algo.train executed
agent_timesteps_total: 228
connector_metrics: {}
counters:
  num_agent_steps_sampled: 228
  num_agent_steps_trained: 228
  num_env_steps_sampled: 228
  num_env_steps_trained: 228
custom_metrics: {}
date: 2023-10-15_04-53-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5604741148509451
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8424562096595765
        entropy_coeff: 0.0
        grad_gnorm: 12.894356778264045
        kl: 0.017954652731956837
        policy_loss: -0.240696319937706
        total_loss: 0.00806154931584994
        vf_explained_var: .nan
        vf_loss: 0.22074009651890567
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6810.5
  num_agent_steps_sampled: 228
  num_agent_steps_trained: 228
  num_env_steps_sampled: 228
  num_env_steps_trained: 228
iterations_since_restore: 114
node_ip: 10.27.41.23
num_agent_steps_sampled: 228
num_agent_steps_trained: 228
num_env_steps_sampled: 228
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005556589104696232
num_env_steps_trained: 228
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005556589104696232
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.87237354085603
  ram_util_percent: 47.452140077821014
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 21475.754199504852
time_this_iter_s: 359.93340015411377
time_total_s: 21475.754199504852
timers:
  learn_throughput: 3.757
  learn_time_ms: 532.329
  load_throughput: 5614.865
  load_time_ms: 0.356
  sample_time_ms: 170370.922
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 170904.189
timestamp: 1697334808
timesteps_total: 228
training_iteration: 114
trial_id: default

Last checkpoint 113 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000114
server side epoch loop 114
algo.train executed
agent_timesteps_total: 230
connector_metrics: {}
counters:
  num_agent_steps_sampled: 230
  num_agent_steps_trained: 230
  num_env_steps_sampled: 230
  num_env_steps_trained: 230
custom_metrics: {}
date: 2023-10-15_04-56-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5604741148509451
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8513207336266835
        entropy_coeff: 0.0
        grad_gnorm: 14.630244915684065
        kl: 0.020623707005021665
        policy_loss: -0.218869016567866
        total_loss: 0.09932809760794044
        vf_explained_var: .nan
        vf_loss: 0.2860143523243702
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6870.5
  num_agent_steps_sampled: 230
  num_agent_steps_trained: 230
  num_env_steps_sampled: 230
  num_env_steps_trained: 230
iterations_since_restore: 115
node_ip: 10.27.41.23
num_agent_steps_sampled: 230
num_agent_steps_trained: 230
num_env_steps_sampled: 230
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01110227299413726
num_env_steps_trained: 230
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01110227299413726
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.362256809338524
  ram_util_percent: 46.71867704280156
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 21655.89778137207
time_this_iter_s: 180.14358186721802
time_total_s: 21655.89778137207
timers:
  learn_throughput: 3.881
  learn_time_ms: 515.288
  load_throughput: 5947.258
  load_time_ms: 0.336
  sample_time_ms: 176324.435
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 176840.623
timestamp: 1697334988
timesteps_total: 230
training_iteration: 115
trial_id: default

Last checkpoint 114 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000115
server side epoch loop 115
algo.train executed
agent_timesteps_total: 232
connector_metrics: {}
counters:
  num_agent_steps_sampled: 232
  num_agent_steps_trained: 232
  num_env_steps_sampled: 232
  num_env_steps_trained: 232
custom_metrics: {}
date: 2023-10-15_05-01-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.840305781364441
        entropy_coeff: 0.0
        grad_gnorm: 14.303222480416299
        kl: 0.011019398442779977
        policy_loss: -0.26583972573280334
        total_loss: 0.04008315329750379
        vf_explained_var: .nan
        vf_loss: 0.2801296509591339
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6930.5
  num_agent_steps_sampled: 232
  num_agent_steps_trained: 232
  num_env_steps_sampled: 232
  num_env_steps_trained: 232
iterations_since_restore: 116
node_ip: 10.27.41.23
num_agent_steps_sampled: 232
num_agent_steps_trained: 232
num_env_steps_sampled: 232
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.006675535716104511
num_env_steps_trained: 232
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.006675535716104511
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.388758782201405
  ram_util_percent: 47.32974238875879
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 21955.49949836731
time_this_iter_s: 299.60171699523926
time_total_s: 21955.49949836731
timers:
  learn_throughput: 3.966
  learn_time_ms: 504.241
  load_throughput: 5797.642
  load_time_ms: 0.345
  sample_time_ms: 194240.136
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 194745.288
timestamp: 1697335288
timesteps_total: 232
training_iteration: 116
trial_id: default

Last checkpoint 115 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000116
server side epoch loop 116
algo.train executed
agent_timesteps_total: 234
connector_metrics: {}
counters:
  num_agent_steps_sampled: 234
  num_agent_steps_trained: 234
  num_env_steps_sampled: 234
  num_env_steps_trained: 234
custom_metrics: {}
date: 2023-10-15_05-05-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8165753444035848
        entropy_coeff: 0.0
        grad_gnorm: 13.124017369747161
        kl: 0.012605789518662884
        policy_loss: -0.23183490733305614
        total_loss: 0.014707691222429275
        vf_explained_var: .nan
        vf_loss: 0.2170360889014167
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 6990.5
  num_agent_steps_sampled: 234
  num_agent_steps_trained: 234
  num_env_steps_sampled: 234
  num_env_steps_trained: 234
iterations_since_restore: 117
node_ip: 10.27.41.23
num_agent_steps_sampled: 234
num_agent_steps_trained: 234
num_env_steps_sampled: 234
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.008320184483485085
num_env_steps_trained: 234
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.008320184483485085
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.614868804664722
  ram_util_percent: 47.04723032069972
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 22195.87911772728
time_this_iter_s: 240.3796193599701
time_total_s: 22195.87911772728
timers:
  learn_throughput: 3.919
  learn_time_ms: 510.303
  load_throughput: 5747.984
  load_time_ms: 0.348
  sample_time_ms: 206207.954
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 206719.184
timestamp: 1697335528
timesteps_total: 234
training_iteration: 117
trial_id: default

Last checkpoint 116 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000117
server side epoch loop 117
algo.train executed
agent_timesteps_total: 236
connector_metrics: {}
counters:
  num_agent_steps_sampled: 236
  num_agent_steps_trained: 236
  num_env_steps_sampled: 236
  num_env_steps_trained: 236
custom_metrics: {}
date: 2023-10-15_05-09-30
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8328547716140746
        entropy_coeff: 0.0
        grad_gnorm: 13.417241488893827
        kl: 0.012572173564512923
        policy_loss: -0.22815751234690348
        total_loss: 0.034176783512036006
        vf_explained_var: .nan
        vf_loss: 0.2329064686326698
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7050.5
  num_agent_steps_sampled: 236
  num_agent_steps_trained: 236
  num_env_steps_sampled: 236
  num_env_steps_trained: 236
iterations_since_restore: 118
node_ip: 10.27.41.23
num_agent_steps_sampled: 236
num_agent_steps_trained: 236
num_env_steps_sampled: 236
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.008278992802378666
num_env_steps_trained: 236
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.008278992802378666
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.687536231884058
  ram_util_percent: 46.78057971014493
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 22437.454681158066
time_this_iter_s: 241.57556343078613
time_total_s: 22437.454681158066
timers:
  learn_throughput: 3.736
  learn_time_ms: 535.392
  load_throughput: 5651.178
  load_time_ms: 0.354
  sample_time_ms: 215599.013
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 216135.336
timestamp: 1697335770
timesteps_total: 236
training_iteration: 118
trial_id: default

Last checkpoint 117 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000118
server side epoch loop 118
algo.train executed
agent_timesteps_total: 238
connector_metrics: {}
counters:
  num_agent_steps_sampled: 238
  num_agent_steps_trained: 238
  num_env_steps_sampled: 238
  num_env_steps_trained: 238
custom_metrics: {}
date: 2023-10-15_05-15-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8647042910257976
        entropy_coeff: 0.0
        grad_gnorm: 11.770388752222061
        kl: 0.017467501952205568
        policy_loss: -0.24357585807641347
        total_loss: -0.024568879107634226
        vf_explained_var: .nan
        vf_loss: 0.17812060824192788
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7110.5
  num_agent_steps_sampled: 238
  num_agent_steps_trained: 238
  num_env_steps_sampled: 238
  num_env_steps_trained: 238
iterations_since_restore: 119
node_ip: 10.27.41.23
num_agent_steps_sampled: 238
num_agent_steps_trained: 238
num_env_steps_sampled: 238
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.005571750260796533
num_env_steps_trained: 238
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.005571750260796533
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.659765625
  ram_util_percent: 47.114453125
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 22796.4086933136
time_this_iter_s: 358.95401215553284
time_total_s: 22796.4086933136
timers:
  learn_throughput: 3.698
  learn_time_ms: 540.771
  load_throughput: 5576.42
  load_time_ms: 0.359
  sample_time_ms: 239423.349
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 239965.059
timestamp: 1697336129
timesteps_total: 238
training_iteration: 119
trial_id: default

Last checkpoint 118 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000119
server side epoch loop 119
algo.train executed
agent_timesteps_total: 240
connector_metrics: {}
counters:
  num_agent_steps_sampled: 240
  num_agent_steps_trained: 240
  num_env_steps_sampled: 240
  num_env_steps_trained: 240
custom_metrics: {}
date: 2023-10-15_05-17-30
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8778189559777578
        entropy_coeff: 0.0
        grad_gnorm: 9.734949334462483
        kl: 0.013581223605130314
        policy_loss: -0.23208704690138499
        total_loss: -0.08188427686691284
        vf_explained_var: .nan
        vf_loss: 0.11841305385629917
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7170.5
  num_agent_steps_sampled: 240
  num_agent_steps_trained: 240
  num_env_steps_sampled: 240
  num_env_steps_trained: 240
iterations_since_restore: 120
node_ip: 10.27.41.23
num_agent_steps_sampled: 240
num_agent_steps_trained: 240
num_env_steps_sampled: 240
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0165586669639536
num_env_steps_trained: 240
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0165586669639536
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.81686046511628
  ram_util_percent: 47.35174418604651
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 22917.191714048386
time_this_iter_s: 120.78302073478699
time_total_s: 22917.191714048386
timers:
  learn_throughput: 3.673
  learn_time_ms: 544.557
  load_throughput: 5612.235
  load_time_ms: 0.356
  sample_time_ms: 239421.578
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 239967.089
timestamp: 1697336250
timesteps_total: 240
training_iteration: 120
trial_id: default

Last checkpoint 119 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000120
server side epoch loop 120
algo.train executed
agent_timesteps_total: 242
connector_metrics: {}
counters:
  num_agent_steps_sampled: 242
  num_agent_steps_trained: 242
  num_env_steps_sampled: 242
  num_env_steps_trained: 242
custom_metrics: {}
date: 2023-10-15_05-19-30
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7310462474822998
        entropy_coeff: 0.0
        grad_gnorm: 11.856303482254345
        kl: 0.011402497874466158
        policy_loss: -0.23245974977811176
        total_loss: -0.020612275103727977
        vf_explained_var: .nan
        vf_loss: 0.18515751899588698
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7230.5
  num_agent_steps_sampled: 242
  num_agent_steps_trained: 242
  num_env_steps_sampled: 242
  num_env_steps_trained: 242
iterations_since_restore: 121
node_ip: 10.27.41.23
num_agent_steps_sampled: 242
num_agent_steps_trained: 242
num_env_steps_sampled: 242
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016541340748591007
num_env_steps_trained: 242
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016541340748591007
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.186127167630056
  ram_util_percent: 47.85549132947975
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 23038.101244211197
time_this_iter_s: 120.90953016281128
time_total_s: 23038.101244211197
timers:
  learn_throughput: 3.669
  learn_time_ms: 545.044
  load_throughput: 5444.32
  load_time_ms: 0.367
  sample_time_ms: 236737.657
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 237283.687
timestamp: 1697336370
timesteps_total: 242
training_iteration: 121
trial_id: default

Last checkpoint 120 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000121
server side epoch loop 121
algo.train executed
agent_timesteps_total: 244
connector_metrics: {}
counters:
  num_agent_steps_sampled: 244
  num_agent_steps_trained: 244
  num_env_steps_sampled: 244
  num_env_steps_trained: 244
custom_metrics: {}
date: 2023-10-15_05-21-32
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7525237520535788
        entropy_coeff: 0.0
        grad_gnorm: 12.212405252456666
        kl: 0.006221757819730556
        policy_loss: -0.145501838127772
        total_loss: 0.02302524745464325
        vf_explained_var: .nan
        vf_loss: 0.15396375676355092
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7290.5
  num_agent_steps_sampled: 244
  num_agent_steps_trained: 244
  num_env_steps_sampled: 244
  num_env_steps_trained: 244
iterations_since_restore: 122
node_ip: 10.27.41.23
num_agent_steps_sampled: 244
num_agent_steps_trained: 244
num_env_steps_sampled: 244
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016463971942966026
num_env_steps_trained: 244
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016463971942966026
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.034682080924856
  ram_util_percent: 47.82138728323699
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 23159.578920841217
time_this_iter_s: 121.47767663002014
time_total_s: 23159.578920841217
timers:
  learn_throughput: 3.228
  learn_time_ms: 619.584
  load_throughput: 5507.227
  load_time_ms: 0.363
  sample_time_ms: 227704.56
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 228325.107
timestamp: 1697336492
timesteps_total: 244
training_iteration: 122
trial_id: default

Last checkpoint 121 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000122
server side epoch loop 122
algo.train executed
agent_timesteps_total: 246
connector_metrics: {}
counters:
  num_agent_steps_sampled: 246
  num_agent_steps_trained: 246
  num_env_steps_sampled: 246
  num_env_steps_trained: 246
custom_metrics: {}
date: 2023-10-15_05-23-32
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7834568599859872
        entropy_coeff: 0.0
        grad_gnorm: 8.882244445880254
        kl: 0.009438932132494908
        policy_loss: -0.2529284636179606
        total_loss: -0.13534711996714274
        vf_explained_var: .nan
        vf_loss: 0.09548753798905334
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7350.5
  num_agent_steps_sampled: 246
  num_agent_steps_trained: 246
  num_env_steps_sampled: 246
  num_env_steps_trained: 246
iterations_since_restore: 123
node_ip: 10.27.41.23
num_agent_steps_sampled: 246
num_agent_steps_trained: 246
num_env_steps_sampled: 246
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0166810593677593
num_env_steps_trained: 246
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0166810593677593
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.904678362573101
  ram_util_percent: 48.49473684210527
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 23279.475664138794
time_this_iter_s: 119.8967432975769
time_total_s: 23279.475664138794
timers:
  learn_throughput: 3.236
  learn_time_ms: 617.985
  load_throughput: 5519.184
  load_time_ms: 0.362
  sample_time_ms: 215746.203
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 216365.126
timestamp: 1697336612
timesteps_total: 246
training_iteration: 123
trial_id: default

Last checkpoint 122 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000123
server side epoch loop 123
algo.train executed
agent_timesteps_total: 248
connector_metrics: {}
counters:
  num_agent_steps_sampled: 248
  num_agent_steps_trained: 248
  num_env_steps_sampled: 248
  num_env_steps_trained: 248
custom_metrics: {}
date: 2023-10-15_05-25-33
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7619995097319285
        entropy_coeff: 0.0
        grad_gnorm: 8.7622064858675
        kl: 0.009032783741956034
        policy_loss: -0.24520919024944304
        total_loss: -0.12896455725034078
        vf_explained_var: .nan
        vf_loss: 0.09510150182565363
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7410.5
  num_agent_steps_sampled: 248
  num_agent_steps_trained: 248
  num_env_steps_sampled: 248
  num_env_steps_trained: 248
iterations_since_restore: 124
node_ip: 10.27.41.23
num_agent_steps_sampled: 248
num_agent_steps_trained: 248
num_env_steps_sampled: 248
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016580995176600496
num_env_steps_trained: 248
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016580995176600496
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.241618497109828
  ram_util_percent: 47.22369942196532
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 23400.0959482193
time_this_iter_s: 120.62028408050537
time_total_s: 23400.0959482193
timers:
  learn_throughput: 3.224
  learn_time_ms: 620.394
  load_throughput: 5493.522
  load_time_ms: 0.364
  sample_time_ms: 191812.491
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 192433.824
timestamp: 1697336733
timesteps_total: 248
training_iteration: 124
trial_id: default

Last checkpoint 123 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000124
server side epoch loop 124
algo.train executed
agent_timesteps_total: 250
connector_metrics: {}
counters:
  num_agent_steps_sampled: 250
  num_agent_steps_trained: 250
  num_env_steps_sampled: 250
  num_env_steps_trained: 250
custom_metrics: {}
date: 2023-10-15_05-32-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.791558180252711
        entropy_coeff: 0.0
        grad_gnorm: 11.468393974502881
        kl: 0.015324717875531253
        policy_loss: -0.2454273670911789
        total_loss: -0.04173989097277323
        vf_explained_var: .nan
        vf_loss: 0.16781673698108837
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7470.5
  num_agent_steps_sampled: 250
  num_agent_steps_trained: 250
  num_env_steps_sampled: 250
  num_env_steps_trained: 250
iterations_since_restore: 125
node_ip: 10.27.41.23
num_agent_steps_sampled: 250
num_agent_steps_trained: 250
num_env_steps_sampled: 250
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.004807478812850844
num_env_steps_trained: 250
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.004807478812850844
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.88735244519393
  ram_util_percent: 47.560370994940975
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 23816.114742279053
time_this_iter_s: 416.0187940597534
time_total_s: 23816.114742279053
timers:
  learn_throughput: 3.11
  learn_time_ms: 643.104
  load_throughput: 5351.925
  load_time_ms: 0.374
  sample_time_ms: 215377.268
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 216021.342
timestamp: 1697337149
timesteps_total: 250
training_iteration: 125
trial_id: default

Last checkpoint 124 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000125
server side epoch loop 125
algo.train executed
agent_timesteps_total: 252
connector_metrics: {}
counters:
  num_agent_steps_sampled: 252
  num_agent_steps_trained: 252
  num_env_steps_sampled: 252
  num_env_steps_trained: 252
custom_metrics: {}
date: 2023-10-15_05-35-29
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7135285973548888
        entropy_coeff: 0.0
        grad_gnorm: 12.242104427019756
        kl: 0.01335376906839277
        policy_loss: -0.23344229360421498
        total_loss: -0.014101427793502808
        vf_explained_var: .nan
        vf_loss: 0.18808354462574547
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7530.5
  num_agent_steps_sampled: 252
  num_agent_steps_trained: 252
  num_env_steps_sampled: 252
  num_env_steps_trained: 252
iterations_since_restore: 126
node_ip: 10.27.41.23
num_agent_steps_sampled: 252
num_agent_steps_trained: 252
num_env_steps_sampled: 252
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011108428652106398
num_env_steps_trained: 252
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011108428652106398
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.016731517509728
  ram_util_percent: 46.4614785992218
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 23996.158539056778
time_this_iter_s: 180.04379677772522
time_total_s: 23996.158539056778
timers:
  learn_throughput: 3.053
  learn_time_ms: 655.015
  load_throughput: 5389.751
  load_time_ms: 0.371
  sample_time_ms: 203409.566
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 204065.546
timestamp: 1697337329
timesteps_total: 252
training_iteration: 126
trial_id: default

Last checkpoint 125 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000126
server side epoch loop 126
algo.train executed
agent_timesteps_total: 254
connector_metrics: {}
counters:
  num_agent_steps_sampled: 254
  num_agent_steps_trained: 254
  num_env_steps_sampled: 254
  num_env_steps_trained: 254
custom_metrics: {}
date: 2023-10-15_05-40-28
done: false
episode_len_mean: 127.0
episode_media: {}
episode_reward_max: 85.9741491984127
episode_reward_mean: 85.9741491984127
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 1
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7651066839694978
        entropy_coeff: 0.0
        grad_gnorm: 11.959361374378204
        kl: 0.009991233920739735
        policy_loss: -0.11306795875231425
        total_loss: 0.07224342972040176
        vf_explained_var: .nan
        vf_loss: 0.16192479370244353
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7590.5
  num_agent_steps_sampled: 254
  num_agent_steps_trained: 254
  num_env_steps_sampled: 254
  num_env_steps_trained: 254
iterations_since_restore: 127
node_ip: 10.27.41.23
num_agent_steps_sampled: 254
num_agent_steps_trained: 254
num_env_steps_sampled: 254
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.006683549202686509
num_env_steps_trained: 254
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.006683549202686509
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.536065573770493
  ram_util_percent: 47.304918032786894
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17099563892071065
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 85112.84075150124
  mean_inference_ms: 2.982524725107046
  mean_raw_obs_processing_ms: 1.6804896868192234
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.0
  episode_media: {}
  episode_reward_max: 85.9741491984127
  episode_reward_mean: 85.9741491984127
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    episode_reward:
    - 85.9741491984127
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17099563892071065
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 85112.84075150124
    mean_inference_ms: 2.982524725107046
    mean_raw_obs_processing_ms: 1.6804896868192234
time_since_restore: 24295.401052713394
time_this_iter_s: 299.2425136566162
time_total_s: 24295.401052713394
timers:
  learn_throughput: 3.055
  learn_time_ms: 654.767
  load_throughput: 5549.856
  load_time_ms: 0.36
  sample_time_ms: 209296.127
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 209951.838
timestamp: 1697337628
timesteps_total: 254
training_iteration: 127
trial_id: default

Last checkpoint 126 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000127
server side epoch loop 127
algo.train executed
agent_timesteps_total: 256
connector_metrics: {}
counters:
  num_agent_steps_sampled: 256
  num_agent_steps_trained: 256
  num_env_steps_sampled: 256
  num_env_steps_trained: 256
custom_metrics: {}
date: 2023-10-15_05-43-28
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 1
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8377835432688394
        entropy_coeff: 0.0
        grad_gnorm: 6.054700996478399
        kl: 0.016617423473023033
        policy_loss: -0.22309724986553192
        total_loss: 4.855220634738604
        vf_explained_var: .nan
        vf_loss: 5.039421366900205
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7650.5
  num_agent_steps_sampled: 256
  num_agent_steps_trained: 256
  num_env_steps_sampled: 256
  num_env_steps_trained: 256
iterations_since_restore: 128
node_ip: 10.27.41.23
num_agent_steps_sampled: 256
num_agent_steps_trained: 256
num_env_steps_sampled: 256
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011096412487336664
num_env_steps_trained: 256
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011096412487336664
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.708949416342412
  ram_util_percent: 47.217898832684824
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 24475.63982272148
time_this_iter_s: 180.23877000808716
time_total_s: 24475.63982272148
timers:
  learn_throughput: 3.188
  learn_time_ms: 627.399
  load_throughput: 5478.096
  load_time_ms: 0.365
  sample_time_ms: 203189.8
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 203818.153
timestamp: 1697337808
timesteps_total: 256
training_iteration: 128
trial_id: default

Last checkpoint 127 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000128
server side epoch loop 128
algo.train executed
agent_timesteps_total: 258
connector_metrics: {}
counters:
  num_agent_steps_sampled: 258
  num_agent_steps_trained: 258
  num_env_steps_sampled: 258
  num_env_steps_trained: 258
custom_metrics: {}
date: 2023-10-15_05-45-29
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.862507563829422
        entropy_coeff: 0.0
        grad_gnorm: 12.361422904332478
        kl: 0.005868728390487377
        policy_loss: -0.15935377279917398
        total_loss: 0.017112799485524497
        vf_explained_var: .nan
        vf_loss: 0.16272956550043696
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7710.5
  num_agent_steps_sampled: 258
  num_agent_steps_trained: 258
  num_env_steps_sampled: 258
  num_env_steps_trained: 258
iterations_since_restore: 129
node_ip: 10.27.41.23
num_agent_steps_sampled: 258
num_agent_steps_trained: 258
num_env_steps_sampled: 258
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016581269893981534
num_env_steps_trained: 258
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016581269893981534
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 6.906395348837209
  ram_util_percent: 47.6110465116279
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 24596.25816178322
time_this_iter_s: 120.61833906173706
time_total_s: 24596.25816178322
timers:
  learn_throughput: 3.182
  learn_time_ms: 628.495
  load_throughput: 5398.422
  load_time_ms: 0.37
  sample_time_ms: 179355.14
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 179984.592
timestamp: 1697337929
timesteps_total: 258
training_iteration: 129
trial_id: default

Last checkpoint 128 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000129
server side epoch loop 129
algo.train executed
agent_timesteps_total: 260
connector_metrics: {}
counters:
  num_agent_steps_sampled: 260
  num_agent_steps_trained: 260
  num_env_steps_sampled: 260
  num_env_steps_trained: 260
custom_metrics: {}
date: 2023-10-15_05-48-28
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8102796951929727
        entropy_coeff: 0.0
        grad_gnorm: 10.448787148793539
        kl: 0.013095207367344604
        policy_loss: -0.18734772503376007
        total_loss: -0.03402495632568995
        vf_explained_var: .nan
        vf_loss: 0.12267067594827191
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7770.5
  num_agent_steps_sampled: 260
  num_agent_steps_trained: 260
  num_env_steps_sampled: 260
  num_env_steps_trained: 260
iterations_since_restore: 130
node_ip: 10.27.41.23
num_agent_steps_sampled: 260
num_agent_steps_trained: 260
num_env_steps_sampled: 260
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.011181942319550624
num_env_steps_trained: 260
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.011181942319550624
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.96156862745098
  ram_util_percent: 47.931372549019606
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 24775.11826992035
time_this_iter_s: 178.86010813713074
time_total_s: 24775.11826992035
timers:
  learn_throughput: 3.204
  learn_time_ms: 624.13
  load_throughput: 5227.198
  load_time_ms: 0.383
  sample_time_ms: 185167.221
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 185792.306
timestamp: 1697338108
timesteps_total: 260
training_iteration: 130
trial_id: default

Last checkpoint 129 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000130
server side epoch loop 130
algo.train executed
agent_timesteps_total: 262
connector_metrics: {}
counters:
  num_agent_steps_sampled: 262
  num_agent_steps_trained: 262
  num_env_steps_sampled: 262
  num_env_steps_trained: 262
custom_metrics: {}
date: 2023-10-15_05-50-28
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7951976120471955
        entropy_coeff: 0.0
        grad_gnorm: 8.91283990542094
        kl: 0.011364586653750544
        policy_loss: -0.20690457224845887
        total_loss: -0.08929869035879771
        vf_explained_var: .nan
        vf_loss: 0.09100466504193415
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7830.5
  num_agent_steps_sampled: 262
  num_agent_steps_trained: 262
  num_env_steps_sampled: 262
  num_env_steps_trained: 262
iterations_since_restore: 131
node_ip: 10.27.41.23
num_agent_steps_sampled: 262
num_agent_steps_trained: 262
num_env_steps_sampled: 262
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016576004653401696
num_env_steps_trained: 262
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016576004653401696
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.675
  ram_util_percent: 46.97848837209303
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 24895.774906396866
time_this_iter_s: 120.65663647651672
time_total_s: 24895.774906396866
timers:
  learn_throughput: 3.131
  learn_time_ms: 638.706
  load_throughput: 5334.907
  load_time_ms: 0.375
  sample_time_ms: 185127.391
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 185767.021
timestamp: 1697338228
timesteps_total: 262
training_iteration: 131
trial_id: default

Last checkpoint 130 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000131
server side epoch loop 131
algo.train executed
agent_timesteps_total: 264
connector_metrics: {}
counters:
  num_agent_steps_sampled: 264
  num_agent_steps_trained: 264
  num_env_steps_sampled: 264
  num_env_steps_trained: 264
custom_metrics: {}
date: 2023-10-15_05-58-29
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8156274120012919
        entropy_coeff: 0.0
        grad_gnorm: 10.105931393305461
        kl: 0.01606393835002867
        policy_loss: -0.23434174060821533
        total_loss: -0.0691090499361356
        vf_explained_var: .nan
        vf_loss: 0.1276316532615359
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7890.5
  num_agent_steps_sampled: 264
  num_agent_steps_trained: 264
  num_env_steps_sampled: 264
  num_env_steps_trained: 264
iterations_since_restore: 132
node_ip: 10.27.41.23
num_agent_steps_sampled: 264
num_agent_steps_trained: 264
num_env_steps_sampled: 264
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0041656518012834565
num_env_steps_trained: 264
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0041656518012834565
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.927883211678832
  ram_util_percent: 47.31284671532847
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 25375.892161369324
time_this_iter_s: 480.1172549724579
time_total_s: 25375.892161369324
timers:
  learn_throughput: 3.533
  learn_time_ms: 566.039
  load_throughput: 5520.637
  load_time_ms: 0.362
  sample_time_ms: 221064.005
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 221630.981
timestamp: 1697338709
timesteps_total: 264
training_iteration: 132
trial_id: default

Last checkpoint 131 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000132
server side epoch loop 132
algo.train executed
agent_timesteps_total: 266
connector_metrics: {}
counters:
  num_agent_steps_sampled: 266
  num_agent_steps_trained: 266
  num_env_steps_sampled: 266
  num_env_steps_trained: 266
custom_metrics: {}
date: 2023-10-15_06-00-30
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3407111722764182
        cur_lr: 5.0000000000000016e-05
        entropy: 1.843438712755839
        entropy_coeff: 0.0
        grad_gnorm: 12.241489704449972
        kl: 0.0010259250720991986
        policy_loss: -0.0037205298741658527
        total_loss: 0.14772004286448162
        vf_explained_var: .nan
        vf_loss: 0.14903917980069917
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 7950.5
  num_agent_steps_sampled: 266
  num_agent_steps_trained: 266
  num_env_steps_sampled: 266
  num_env_steps_trained: 266
iterations_since_restore: 133
node_ip: 10.27.41.23
num_agent_steps_sampled: 266
num_agent_steps_trained: 266
num_env_steps_sampled: 266
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01652290499653246
num_env_steps_trained: 266
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01652290499653246
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 6.547976878612716
  ram_util_percent: 46.79017341040464
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 25496.93660736084
time_this_iter_s: 121.04444599151611
time_total_s: 25496.93660736084
timers:
  learn_throughput: 3.502
  learn_time_ms: 571.149
  load_throughput: 5327.115
  load_time_ms: 0.375
  sample_time_ms: 221173.578
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 221745.743
timestamp: 1697338830
timesteps_total: 266
training_iteration: 133
trial_id: default

Last checkpoint 132 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000133
server side epoch loop 133
algo.train executed
agent_timesteps_total: 268
connector_metrics: {}
counters:
  num_agent_steps_sampled: 268
  num_agent_steps_trained: 268
  num_env_steps_sampled: 268
  num_env_steps_trained: 268
custom_metrics: {}
date: 2023-10-15_06-02-30
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1703555861382091
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8004343152046203
        entropy_coeff: 0.0
        grad_gnorm: 10.437847967942556
        kl: 0.006259886549848185
        policy_loss: -0.2242477873961131
        total_loss: -0.08116209208965301
        vf_explained_var: .nan
        vf_loss: 0.13575940341688691
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8010.5
  num_agent_steps_sampled: 268
  num_agent_steps_trained: 268
  num_env_steps_sampled: 268
  num_env_steps_trained: 268
iterations_since_restore: 134
node_ip: 10.27.41.23
num_agent_steps_sampled: 268
num_agent_steps_trained: 268
num_env_steps_sampled: 268
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01659692112310767
num_env_steps_trained: 268
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01659692112310767
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.966279069767442
  ram_util_percent: 47.002325581395354
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 25617.441258192062
time_this_iter_s: 120.50465083122253
time_total_s: 25617.441258192062
timers:
  learn_throughput: 3.502
  learn_time_ms: 571.177
  load_throughput: 5341.361
  load_time_ms: 0.374
  sample_time_ms: 221161.978
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 221734.167
timestamp: 1697338950
timesteps_total: 268
training_iteration: 134
trial_id: default

Last checkpoint 133 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000134
server side epoch loop 134
algo.train executed
agent_timesteps_total: 270
connector_metrics: {}
counters:
  num_agent_steps_sampled: 270
  num_agent_steps_trained: 270
  num_env_steps_sampled: 270
  num_env_steps_trained: 270
custom_metrics: {}
date: 2023-10-15_06-04-31
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1703555861382091
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7007610936959585
        entropy_coeff: 0.0
        grad_gnorm: 11.680268794298172
        kl: 0.0065704986967224
        policy_loss: -0.1554028550783793
        total_loss: 0.019166520486275356
        vf_explained_var: .nan
        vf_loss: 0.1668795608752892
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8070.5
  num_agent_steps_sampled: 270
  num_agent_steps_trained: 270
  num_env_steps_sampled: 270
  num_env_steps_trained: 270
iterations_since_restore: 135
node_ip: 10.27.41.23
num_agent_steps_sampled: 270
num_agent_steps_trained: 270
num_env_steps_sampled: 270
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0165301292808146
num_env_steps_trained: 270
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0165301292808146
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.363583815028901
  ram_util_percent: 47.56647398843932
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 25738.432819128036
time_this_iter_s: 120.99156093597412
time_total_s: 25738.432819128036
timers:
  learn_throughput: 3.621
  learn_time_ms: 552.39
  load_throughput: 5481.675
  load_time_ms: 0.365
  sample_time_ms: 191678.067
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 192231.44
timestamp: 1697339071
timesteps_total: 270
training_iteration: 135
trial_id: default

Last checkpoint 134 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000135
server side epoch loop 135
algo.train executed
agent_timesteps_total: 272
connector_metrics: {}
counters:
  num_agent_steps_sampled: 272
  num_agent_steps_trained: 272
  num_env_steps_sampled: 272
  num_env_steps_trained: 272
custom_metrics: {}
date: 2023-10-15_06-06-58
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1703555861382091
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7989785114924113
        entropy_coeff: 0.0
        grad_gnorm: 10.42825937072436
        kl: 0.01913166997510416
        policy_loss: -0.22698801855246226
        total_loss: -0.06442898213863373
        vf_explained_var: .nan
        vf_loss: 0.1401681796007324
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8130.5
  num_agent_steps_sampled: 272
  num_agent_steps_trained: 272
  num_env_steps_sampled: 272
  num_env_steps_trained: 272
iterations_since_restore: 136
node_ip: 10.27.41.23
num_agent_steps_sampled: 272
num_agent_steps_trained: 272
num_env_steps_sampled: 272
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013622066345489375
num_env_steps_trained: 272
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013622066345489375
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.944976076555024
  ram_util_percent: 47.01244019138757
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 25885.25386238098
time_this_iter_s: 146.82104325294495
time_total_s: 25885.25386238098
timers:
  learn_throughput: 3.669
  learn_time_ms: 545.169
  load_throughput: 5482.392
  load_time_ms: 0.365
  sample_time_ms: 188362.997
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 188909.152
timestamp: 1697339218
timesteps_total: 272
training_iteration: 136
trial_id: default

Last checkpoint 135 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000136
server side epoch loop 136
algo.train executed
agent_timesteps_total: 274
connector_metrics: {}
counters:
  num_agent_steps_sampled: 274
  num_agent_steps_trained: 274
  num_env_steps_sampled: 274
  num_env_steps_trained: 274
custom_metrics: {}
date: 2023-10-15_06-08-59
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1703555861382091
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7731793026129405
        entropy_coeff: 0.0
        grad_gnorm: 11.081042642891408
        kl: 0.015877613114813963
        policy_loss: -0.2397779921690623
        total_loss: -0.056688805917898814
        vf_explained_var: .nan
        vf_loss: 0.164506733094701
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8190.5
  num_agent_steps_sampled: 274
  num_agent_steps_trained: 274
  num_env_steps_sampled: 274
  num_env_steps_trained: 274
iterations_since_restore: 137
node_ip: 10.27.41.23
num_agent_steps_sampled: 274
num_agent_steps_trained: 274
num_env_steps_sampled: 274
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016599927479722224
num_env_steps_trained: 274
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016599927479722224
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.194767441860465
  ram_util_percent: 47.43313953488372
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 26005.736636400223
time_this_iter_s: 120.48277401924133
time_total_s: 26005.736636400223
timers:
  learn_throughput: 3.678
  learn_time_ms: 543.727
  load_throughput: 5424.604
  load_time_ms: 0.369
  sample_time_ms: 170488.39
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 171033.177
timestamp: 1697339339
timesteps_total: 274
training_iteration: 137
trial_id: default

Last checkpoint 136 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000137
server side epoch loop 137
algo.train executed
agent_timesteps_total: 276
connector_metrics: {}
counters:
  num_agent_steps_sampled: 276
  num_agent_steps_trained: 276
  num_env_steps_sampled: 276
  num_env_steps_trained: 276
custom_metrics: {}
date: 2023-10-15_06-12-28
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1703555861382091
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7138675034046174
        entropy_coeff: 0.0
        grad_gnorm: 9.741465187072754
        kl: 0.02117050384210112
        policy_loss: -0.22949548761049907
        total_loss: -0.08654570678869883
        vf_explained_var: .nan
        vf_loss: 0.11817276487054187
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8250.5
  num_agent_steps_sampled: 276
  num_agent_steps_trained: 276
  num_env_steps_sampled: 276
  num_env_steps_trained: 276
iterations_since_restore: 138
node_ip: 10.27.41.23
num_agent_steps_sampled: 276
num_agent_steps_trained: 276
num_env_steps_sampled: 276
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.009532618542701994
num_env_steps_trained: 276
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.009532618542701994
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.146488294314382
  ram_util_percent: 47.1438127090301
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 26215.54291844368
time_this_iter_s: 209.80628204345703
time_total_s: 26215.54291844368
timers:
  learn_throughput: 3.595
  learn_time_ms: 556.353
  load_throughput: 5530.1
  load_time_ms: 0.362
  sample_time_ms: 173432.517
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 173989.927
timestamp: 1697339548
timesteps_total: 276
training_iteration: 138
trial_id: default

Last checkpoint 137 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000138
server side epoch loop 138
algo.train executed
agent_timesteps_total: 278
connector_metrics: {}
counters:
  num_agent_steps_sampled: 278
  num_agent_steps_trained: 278
  num_env_steps_sampled: 278
  num_env_steps_trained: 278
custom_metrics: {}
date: 2023-10-15_06-17-28
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7555333792073122
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8814468224843344
        entropy_coeff: 0.0
        grad_gnorm: 9.682465316851934
        kl: 0.015876653347550018
        policy_loss: -0.23852399587631226
        total_loss: -0.09167709251244863
        vf_explained_var: .nan
        vf_loss: 0.11897490406263386
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8310.5
  num_agent_steps_sampled: 278
  num_agent_steps_trained: 278
  num_env_steps_sampled: 278
  num_env_steps_trained: 278
iterations_since_restore: 139
node_ip: 10.27.41.23
num_agent_steps_sampled: 278
num_agent_steps_trained: 278
num_env_steps_sampled: 278
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.006681865970953117
num_env_steps_trained: 278
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.006681865970953117
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.147775175644027
  ram_util_percent: 47.34613583138174
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 26514.86079621315
time_this_iter_s: 299.3178777694702
time_total_s: 26514.86079621315
timers:
  learn_throughput: 3.63
  learn_time_ms: 550.988
  load_throughput: 5694.141
  load_time_ms: 0.351
  sample_time_ms: 191307.857
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 191859.885
timestamp: 1697339848
timesteps_total: 278
training_iteration: 139
trial_id: default

Last checkpoint 138 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000139
server side epoch loop 139
algo.train executed
agent_timesteps_total: 280
connector_metrics: {}
counters:
  num_agent_steps_sampled: 280
  num_agent_steps_trained: 280
  num_env_steps_sampled: 280
  num_env_steps_trained: 280
custom_metrics: {}
date: 2023-10-15_06-21-29
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7555333792073122
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6711606621742248
        entropy_coeff: 0.0
        grad_gnorm: 8.953301292657851
        kl: 0.020571070543519455
        policy_loss: -0.24335643152395883
        total_loss: -0.10959160427252451
        vf_explained_var: .nan
        vf_loss: 0.09765163252717078
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8370.5
  num_agent_steps_sampled: 280
  num_agent_steps_trained: 280
  num_env_steps_sampled: 280
  num_env_steps_trained: 280
iterations_since_restore: 140
node_ip: 10.27.41.23
num_agent_steps_sampled: 280
num_agent_steps_trained: 280
num_env_steps_sampled: 280
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.008310708941008433
num_env_steps_trained: 280
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.008310708941008433
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.330232558139535
  ram_util_percent: 46.96831395348838
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 26755.514543294907
time_this_iter_s: 240.6537470817566
time_total_s: 26755.514543294907
timers:
  learn_throughput: 3.692
  learn_time_ms: 541.754
  load_throughput: 5837.584
  load_time_ms: 0.343
  sample_time_ms: 197496.448
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 198039.241
timestamp: 1697340089
timesteps_total: 280
training_iteration: 140
trial_id: default

Last checkpoint 139 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000140
server side epoch loop 140
algo.train executed
agent_timesteps_total: 282
connector_metrics: {}
counters:
  num_agent_steps_sampled: 282
  num_agent_steps_trained: 282
  num_env_steps_sampled: 282
  num_env_steps_trained: 282
custom_metrics: {}
date: 2023-10-15_06-23-29
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6333000688109696
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5971468528111776
        entropy_coeff: 0.0
        grad_gnorm: 12.008321936925253
        kl: 0.00398217266950572
        policy_loss: -0.0985667626063029
        total_loss: 0.017561669150988262
        vf_explained_var: .nan
        vf_loss: 0.10564218062233219
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8430.5
  num_agent_steps_sampled: 282
  num_agent_steps_trained: 282
  num_env_steps_sampled: 282
  num_env_steps_trained: 282
iterations_since_restore: 141
node_ip: 10.27.41.23
num_agent_steps_sampled: 282
num_agent_steps_trained: 282
num_env_steps_sampled: 282
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016591309021922025
num_env_steps_trained: 282
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016591309021922025
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.915116279069768
  ram_util_percent: 47.36279069767442
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 26876.0599193573
time_this_iter_s: 120.54537606239319
time_total_s: 26876.0599193573
timers:
  learn_throughput: 3.773
  learn_time_ms: 530.027
  load_throughput: 5813.714
  load_time_ms: 0.344
  sample_time_ms: 197497.041
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 198028.111
timestamp: 1697340209
timesteps_total: 282
training_iteration: 141
trial_id: default

Last checkpoint 140 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000141
server side epoch loop 141
algo.train executed
agent_timesteps_total: 284
connector_metrics: {}
counters:
  num_agent_steps_sampled: 284
  num_agent_steps_trained: 284
  num_env_steps_sampled: 284
  num_env_steps_trained: 284
custom_metrics: {}
date: 2023-10-15_06-25-30
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3166500344054848
        cur_lr: 5.0000000000000016e-05
        entropy: 1.4850762208302817
        entropy_coeff: 0.0
        grad_gnorm: 10.062175802389781
        kl: 0.029283528895515096
        policy_loss: -0.23234332899252574
        total_loss: -0.07429404060045879
        vf_explained_var: .nan
        vf_loss: 0.11949313087728418
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8490.5
  num_agent_steps_sampled: 284
  num_agent_steps_trained: 284
  num_env_steps_sampled: 284
  num_env_steps_trained: 284
iterations_since_restore: 142
node_ip: 10.27.41.23
num_agent_steps_sampled: 284
num_agent_steps_trained: 284
num_env_steps_sampled: 284
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658230591800913
num_env_steps_trained: 284
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658230591800913
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.375581395348837
  ram_util_percent: 48.42732558139533
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 26996.67075443268
time_this_iter_s: 120.61083507537842
time_total_s: 26996.67075443268
timers:
  learn_throughput: 3.73
  learn_time_ms: 536.151
  load_throughput: 5943.466
  load_time_ms: 0.337
  sample_time_ms: 161540.298
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 162077.465
timestamp: 1697340330
timesteps_total: 284
training_iteration: 142
trial_id: default

Last checkpoint 141 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000142
server side epoch loop 142
algo.train executed
agent_timesteps_total: 286
connector_metrics: {}
counters:
  num_agent_steps_sampled: 286
  num_agent_steps_trained: 286
  num_env_steps_sampled: 286
  num_env_steps_trained: 286
custom_metrics: {}
date: 2023-10-15_06-27-30
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9749750516082272
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8083089371522267
        entropy_coeff: 0.0
        grad_gnorm: 6.551689704259236
        kl: 0.015591976934229024
        policy_loss: -0.2136549820502599
        total_loss: -0.15281785825888317
        vf_explained_var: .nan
        vf_loss: 0.0300433578590552
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8550.5
  num_agent_steps_sampled: 286
  num_agent_steps_trained: 286
  num_env_steps_sampled: 286
  num_env_steps_trained: 286
iterations_since_restore: 143
node_ip: 10.27.41.23
num_agent_steps_sampled: 286
num_agent_steps_trained: 286
num_env_steps_sampled: 286
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01660032384551963
num_env_steps_trained: 286
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01660032384551963
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.287209302325582
  ram_util_percent: 47.64767441860465
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27117.150662899017
time_this_iter_s: 120.47990846633911
time_total_s: 27117.150662899017
timers:
  learn_throughput: 3.77
  learn_time_ms: 530.509
  load_throughput: 4800.623
  load_time_ms: 0.417
  sample_time_ms: 161489.376
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 162021.014
timestamp: 1697340450
timesteps_total: 286
training_iteration: 143
trial_id: default

Last checkpoint 142 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000143
server side epoch loop 143
algo.train executed
agent_timesteps_total: 288
connector_metrics: {}
counters:
  num_agent_steps_sampled: 288
  num_agent_steps_trained: 288
  num_env_steps_sampled: 288
  num_env_steps_trained: 288
custom_metrics: {}
date: 2023-10-15_06-29-31
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9749750516082272
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7908070961634317
        entropy_coeff: 0.0
        grad_gnorm: 4.393382623791695
        kl: 0.015621659012200933
        policy_loss: -0.15644246836503348
        total_loss: -0.11925508777300517
        vf_explained_var: .nan
        vf_loss: 0.006334995742508909
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8610.5
  num_agent_steps_sampled: 288
  num_agent_steps_trained: 288
  num_env_steps_sampled: 288
  num_env_steps_trained: 288
iterations_since_restore: 144
node_ip: 10.27.41.23
num_agent_steps_sampled: 288
num_agent_steps_trained: 288
num_env_steps_sampled: 288
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016524871625847763
num_env_steps_trained: 288
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016524871625847763
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.402906976744186
  ram_util_percent: 48.461046511627906
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27238.18067908287
time_this_iter_s: 121.03001618385315
time_total_s: 27238.18067908287
timers:
  learn_throughput: 3.826
  learn_time_ms: 522.731
  load_throughput: 4788.838
  load_time_ms: 0.418
  sample_time_ms: 161549.694
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 162073.555
timestamp: 1697340571
timesteps_total: 288
training_iteration: 144
trial_id: default

Last checkpoint 143 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000144
server side epoch loop 144
algo.train executed
agent_timesteps_total: 290
connector_metrics: {}
counters:
  num_agent_steps_sampled: 290
  num_agent_steps_trained: 290
  num_env_steps_sampled: 290
  num_env_steps_trained: 290
custom_metrics: {}
date: 2023-10-15_06-31-32
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9749750516082272
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7582481801509857
        entropy_coeff: 0.0
        grad_gnorm: 6.207883987824122
        kl: 0.02823306669477006
        policy_loss: -0.09894470870494843
        total_loss: -0.025732326010862987
        vf_explained_var: .nan
        vf_loss: 0.01745277544274965
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8670.5
  num_agent_steps_sampled: 290
  num_agent_steps_trained: 290
  num_env_steps_sampled: 290
  num_env_steps_trained: 290
iterations_since_restore: 145
node_ip: 10.27.41.23
num_agent_steps_sampled: 290
num_agent_steps_trained: 290
num_env_steps_sampled: 290
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016594748541975204
num_env_steps_trained: 290
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016594748541975204
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.223837209302325
  ram_util_percent: 47.39593023255815
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27358.70108771324
time_this_iter_s: 120.5204086303711
time_total_s: 27358.70108771324
timers:
  learn_throughput: 3.879
  learn_time_ms: 515.532
  load_throughput: 4787.472
  load_time_ms: 0.418
  sample_time_ms: 161509.777
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 162026.442
timestamp: 1697340692
timesteps_total: 290
training_iteration: 145
trial_id: default

Last checkpoint 144 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000145
server side epoch loop 145
algo.train executed
agent_timesteps_total: 292
connector_metrics: {}
counters:
  num_agent_steps_sampled: 292
  num_agent_steps_trained: 292
  num_env_steps_sampled: 292
  num_env_steps_trained: 292
custom_metrics: {}
date: 2023-10-15_06-33-32
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9624625774123405
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7972879310448964
        entropy_coeff: 0.0
        grad_gnorm: 7.523657484849294
        kl: 0.012840727031169763
        policy_loss: -0.23121118744214375
        total_loss: -0.13090825776259105
        vf_explained_var: .nan
        vf_loss: 0.06226275517328759
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8730.5
  num_agent_steps_sampled: 292
  num_agent_steps_trained: 292
  num_env_steps_sampled: 292
  num_env_steps_trained: 292
iterations_since_restore: 146
node_ip: 10.27.41.23
num_agent_steps_sampled: 292
num_agent_steps_trained: 292
num_env_steps_sampled: 292
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016589204867725995
num_env_steps_trained: 292
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016589204867725995
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.38953488372093
  ram_util_percent: 47.666279069767434
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27479.261719703674
time_this_iter_s: 120.56063199043274
time_total_s: 27479.261719703674
timers:
  learn_throughput: 3.902
  learn_time_ms: 512.514
  load_throughput: 4807.776
  load_time_ms: 0.416
  sample_time_ms: 158886.776
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 159400.415
timestamp: 1697340812
timesteps_total: 292
training_iteration: 146
trial_id: default

Last checkpoint 145 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000146
server side epoch loop 146
algo.train executed
agent_timesteps_total: 294
connector_metrics: {}
counters:
  num_agent_steps_sampled: 294
  num_agent_steps_trained: 294
  num_env_steps_sampled: 294
  num_env_steps_trained: 294
custom_metrics: {}
date: 2023-10-15_06-35-58
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9624625774123405
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7902502775192262
        entropy_coeff: 0.0
        grad_gnorm: 6.2087826808293665
        kl: 0.0052578290713427124
        policy_loss: -0.1264294743537903
        total_loss: -0.0954331507285436
        vf_explained_var: .nan
        vf_loss: 0.015420202928847477
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8790.5
  num_agent_steps_sampled: 294
  num_agent_steps_trained: 294
  num_env_steps_sampled: 294
  num_env_steps_trained: 294
iterations_since_restore: 147
node_ip: 10.27.41.23
num_agent_steps_sampled: 294
num_agent_steps_trained: 294
num_env_steps_sampled: 294
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.013744311278471527
num_env_steps_trained: 294
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.013744311278471527
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.757211538461538
  ram_util_percent: 46.852884615384625
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27624.776776075363
time_this_iter_s: 145.51505637168884
time_total_s: 27624.776776075363
timers:
  learn_throughput: 3.921
  learn_time_ms: 510.098
  load_throughput: 4808.603
  load_time_ms: 0.416
  sample_time_ms: 161392.494
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 161903.646
timestamp: 1697340958
timesteps_total: 294
training_iteration: 147
trial_id: default

Last checkpoint 146 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000147
server side epoch loop 147
algo.train executed
agent_timesteps_total: 296
connector_metrics: {}
counters:
  num_agent_steps_sampled: 296
  num_agent_steps_trained: 296
  num_env_steps_sampled: 296
  num_env_steps_trained: 296
custom_metrics: {}
date: 2023-10-15_06-37-59
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9624625774123405
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8200433870156607
        entropy_coeff: 0.0
        grad_gnorm: 8.924196298917135
        kl: 0.00022104817635408834
        policy_loss: -0.001556512713432312
        total_loss: 0.03986052175362905
        vf_explained_var: .nan
        vf_loss: 0.04076218434323285
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8850.5
  num_agent_steps_sampled: 296
  num_agent_steps_trained: 296
  num_env_steps_sampled: 296
  num_env_steps_trained: 296
iterations_since_restore: 148
node_ip: 10.27.41.23
num_agent_steps_sampled: 296
num_agent_steps_trained: 296
num_env_steps_sampled: 296
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016567055969991265
num_env_steps_trained: 296
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016567055969991265
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.390697674418604
  ram_util_percent: 47.70232558139536
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27745.498629808426
time_this_iter_s: 120.72185373306274
time_total_s: 27745.498629808426
timers:
  learn_throughput: 3.961
  learn_time_ms: 504.871
  load_throughput: 4813.57
  load_time_ms: 0.415
  sample_time_ms: 152489.288
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 152995.204
timestamp: 1697341079
timesteps_total: 296
training_iteration: 148
trial_id: default

Last checkpoint 147 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000148
server side epoch loop 148
algo.train executed
agent_timesteps_total: 298
connector_metrics: {}
counters:
  num_agent_steps_sampled: 298
  num_agent_steps_trained: 298
  num_env_steps_sampled: 298
  num_env_steps_trained: 298
custom_metrics: {}
date: 2023-10-15_06-40-00
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4812312887061703
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7610479533672332
        entropy_coeff: 0.0
        grad_gnorm: 5.908960896730423
        kl: 0.016474014329715677
        policy_loss: -0.2325563947359721
        total_loss: -0.1751652995745341
        vf_explained_var: .nan
        vf_loss: 0.03298927332337674
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8910.5
  num_agent_steps_sampled: 298
  num_agent_steps_trained: 298
  num_env_steps_sampled: 298
  num_env_steps_trained: 298
iterations_since_restore: 149
node_ip: 10.27.41.23
num_agent_steps_sampled: 298
num_agent_steps_trained: 298
num_env_steps_sampled: 298
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01655773670928563
num_env_steps_trained: 298
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01655773670928563
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.445086705202312
  ram_util_percent: 47.030635838150296
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27866.288524627686
time_this_iter_s: 120.78989481925964
time_total_s: 27866.288524627686
timers:
  learn_throughput: 3.867
  learn_time_ms: 517.138
  load_throughput: 4576.436
  load_time_ms: 0.437
  sample_time_ms: 134624.03
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 135142.39
timestamp: 1697341200
timesteps_total: 298
training_iteration: 149
trial_id: default

Last checkpoint 148 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000149
server side epoch loop 149
algo.train executed
agent_timesteps_total: 300
connector_metrics: {}
counters:
  num_agent_steps_sampled: 300
  num_agent_steps_trained: 300
  num_env_steps_sampled: 300
  num_env_steps_trained: 300
custom_metrics: {}
date: 2023-10-15_06-42-01
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4812312887061703
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7963113049666086
        entropy_coeff: 0.0
        grad_gnorm: 5.853138130903244
        kl: 0.012448920121058412
        policy_loss: -0.24269948204358419
        total_loss: -0.20287170608838398
        vf_explained_var: .nan
        vf_loss: 0.021388049352147696
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 8970.5
  num_agent_steps_sampled: 300
  num_agent_steps_trained: 300
  num_env_steps_sampled: 300
  num_env_steps_trained: 300
iterations_since_restore: 150
node_ip: 10.27.41.23
num_agent_steps_sampled: 300
num_agent_steps_trained: 300
num_env_steps_sampled: 300
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016540083861059422
num_env_steps_trained: 300
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016540083861059422
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.304069767441861
  ram_util_percent: 46.93895348837209
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 27987.207285642624
time_this_iter_s: 120.91876101493835
time_total_s: 27987.207285642624
timers:
  learn_throughput: 3.773
  learn_time_ms: 530.014
  load_throughput: 4520.211
  load_time_ms: 0.442
  sample_time_ms: 122637.651
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 123168.891
timestamp: 1697341321
timesteps_total: 300
training_iteration: 150
trial_id: default

Last checkpoint 149 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000150
server side epoch loop 150
algo.train executed
agent_timesteps_total: 302
connector_metrics: {}
counters:
  num_agent_steps_sampled: 302
  num_agent_steps_trained: 302
  num_env_steps_sampled: 302
  num_env_steps_trained: 302
custom_metrics: {}
date: 2023-10-15_06-44-02
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4812312887061703
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6930275360743205
        entropy_coeff: 0.0
        grad_gnorm: 21.021177752812704
        kl: 0.022768287228488285
        policy_loss: -0.2562592049439748
        total_loss: 0.4609250195324421
        vf_explained_var: .nan
        vf_loss: 0.6834591379903334
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9030.5
  num_agent_steps_sampled: 302
  num_agent_steps_trained: 302
  num_env_steps_sampled: 302
  num_env_steps_trained: 302
iterations_since_restore: 151
node_ip: 10.27.41.23
num_agent_steps_sampled: 302
num_agent_steps_trained: 302
num_env_steps_sampled: 302
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01645596079162257
num_env_steps_trained: 302
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01645596079162257
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.175287356321839
  ram_util_percent: 46.36436781609195
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28108.744078874588
time_this_iter_s: 121.53679323196411
time_total_s: 28108.744078874588
timers:
  learn_throughput: 3.226
  learn_time_ms: 619.918
  load_throughput: 4535.609
  load_time_ms: 0.441
  sample_time_ms: 122646.895
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 123268.038
timestamp: 1697341442
timesteps_total: 302
training_iteration: 151
trial_id: default

Last checkpoint 150 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000151
server side epoch loop 151
algo.train executed
agent_timesteps_total: 304
connector_metrics: {}
counters:
  num_agent_steps_sampled: 304
  num_agent_steps_trained: 304
  num_env_steps_sampled: 304
  num_env_steps_trained: 304
custom_metrics: {}
date: 2023-10-15_06-46-02
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2218469330592554
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7105807761351268
        entropy_coeff: 0.0
        grad_gnorm: 7.789134905735652
        kl: 0.012452431932645898
        policy_loss: -0.15622374912103018
        total_loss: -0.07208211024602254
        vf_explained_var: .nan
        vf_loss: 0.05647425126104887
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9090.5
  num_agent_steps_sampled: 304
  num_agent_steps_trained: 304
  num_env_steps_sampled: 304
  num_env_steps_trained: 304
iterations_since_restore: 152
node_ip: 10.27.41.23
num_agent_steps_sampled: 304
num_agent_steps_trained: 304
num_env_steps_sampled: 304
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016687394596717652
num_env_steps_trained: 304
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016687394596717652
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.21529411764706
  ram_util_percent: 47.3364705882353
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28228.595314741135
time_this_iter_s: 119.85123586654663
time_total_s: 28228.595314741135
timers:
  learn_throughput: 3.259
  learn_time_ms: 613.77
  load_throughput: 4411.806
  load_time_ms: 0.453
  sample_time_ms: 122577.082
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 123192.083
timestamp: 1697341562
timesteps_total: 304
training_iteration: 152
trial_id: default

Last checkpoint 151 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000152
server side epoch loop 152
algo.train executed
agent_timesteps_total: 306
connector_metrics: {}
counters:
  num_agent_steps_sampled: 306
  num_agent_steps_trained: 306
  num_env_steps_sampled: 306
  num_env_steps_trained: 306
custom_metrics: {}
date: 2023-10-15_06-48-03
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2218469330592554
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8578755776087443
        entropy_coeff: 0.0
        grad_gnorm: 12.256213825941085
        kl: 0.015717597394056308
        policy_loss: -0.24514719943205515
        total_loss: 0.010876696308453877
        vf_explained_var: .nan
        vf_loss: 0.22110180996072207
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9150.5
  num_agent_steps_sampled: 306
  num_agent_steps_trained: 306
  num_env_steps_sampled: 306
  num_env_steps_trained: 306
iterations_since_restore: 153
node_ip: 10.27.41.23
num_agent_steps_sampled: 306
num_agent_steps_trained: 306
num_env_steps_sampled: 306
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016469739113600883
num_env_steps_trained: 306
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016469739113600883
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.158045977011493
  ram_util_percent: 46.72816091954024
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28350.03045654297
time_this_iter_s: 121.4351418018341
time_total_s: 28350.03045654297
timers:
  learn_throughput: 3.288
  learn_time_ms: 608.183
  load_throughput: 5233.721
  load_time_ms: 0.382
  sample_time_ms: 122678.355
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 123287.608
timestamp: 1697341683
timesteps_total: 306
training_iteration: 153
trial_id: default

Last checkpoint 152 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000153
server side epoch loop 153
algo.train executed
agent_timesteps_total: 308
connector_metrics: {}
counters:
  num_agent_steps_sampled: 308
  num_agent_steps_trained: 308
  num_env_steps_sampled: 308
  num_env_steps_trained: 308
custom_metrics: {}
date: 2023-10-15_06-50-04
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2218469330592554
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7707027633984884
        entropy_coeff: 0.0
        grad_gnorm: 30.045584321022034
        kl: 0.027004253951599822
        policy_loss: -0.1706572115421295
        total_loss: 0.8642340780546268
        vf_explained_var: .nan
        vf_loss: 0.9748919682657288
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9210.5
  num_agent_steps_sampled: 308
  num_agent_steps_trained: 308
  num_env_steps_sampled: 308
  num_env_steps_trained: 308
iterations_since_restore: 154
node_ip: 10.27.41.23
num_agent_steps_sampled: 308
num_agent_steps_trained: 308
num_env_steps_sampled: 308
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016567238511621694
num_env_steps_trained: 308
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016567238511621694
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.13779069767442
  ram_util_percent: 47.047093023255805
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28470.750930309296
time_this_iter_s: 120.7204737663269
time_total_s: 28470.750930309296
timers:
  learn_throughput: 3.291
  learn_time_ms: 607.643
  load_throughput: 5303.539
  load_time_ms: 0.377
  sample_time_ms: 122647.955
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 123256.658
timestamp: 1697341804
timesteps_total: 308
training_iteration: 154
trial_id: default

Last checkpoint 153 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000154
server side epoch loop 154
algo.train executed
agent_timesteps_total: 310
connector_metrics: {}
counters:
  num_agent_steps_sampled: 310
  num_agent_steps_trained: 310
  num_env_steps_sampled: 310
  num_env_steps_trained: 310
custom_metrics: {}
date: 2023-10-15_06-52-05
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.332770399588883
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8123866180578867
        entropy_coeff: 0.0
        grad_gnorm: 38.62480419476827
        kl: 0.015618341889542838
        policy_loss: -0.24921854933102924
        total_loss: 1.8085747136423984
        vf_explained_var: .nan
        vf_loss: 2.0057409087215397
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9270.5
  num_agent_steps_sampled: 310
  num_agent_steps_trained: 310
  num_env_steps_sampled: 310
  num_env_steps_trained: 310
iterations_since_restore: 155
node_ip: 10.27.41.23
num_agent_steps_sampled: 310
num_agent_steps_trained: 310
num_env_steps_sampled: 310
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016575078082856763
num_env_steps_trained: 310
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016575078082856763
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.315116279069766
  ram_util_percent: 48.31860465116279
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28591.414327144623
time_this_iter_s: 120.66339683532715
time_total_s: 28591.414327144623
timers:
  learn_throughput: 3.269
  learn_time_ms: 611.87
  load_throughput: 5202.56
  load_time_ms: 0.384
  sample_time_ms: 122658.024
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 123270.962
timestamp: 1697341925
timesteps_total: 310
training_iteration: 155
trial_id: default

Last checkpoint 154 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000155
server side epoch loop 155
algo.train executed
agent_timesteps_total: 312
connector_metrics: {}
counters:
  num_agent_steps_sampled: 312
  num_agent_steps_trained: 312
  num_env_steps_sampled: 312
  num_env_steps_trained: 312
custom_metrics: {}
date: 2023-10-15_06-54-06
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.332770399588883
        cur_lr: 5.0000000000000016e-05
        entropy: 1.813171201944351
        entropy_coeff: 0.0
        grad_gnorm: 26.81821479797363
        kl: 0.013969089860135378
        policy_loss: -0.2531531194845835
        total_loss: 0.44651092290878297
        vf_explained_var: .nan
        vf_loss: 0.6531082763026158
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9330.5
  num_agent_steps_sampled: 312
  num_agent_steps_trained: 312
  num_env_steps_sampled: 312
  num_env_steps_trained: 312
iterations_since_restore: 156
node_ip: 10.27.41.23
num_agent_steps_sampled: 312
num_agent_steps_trained: 312
num_env_steps_sampled: 312
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016568919235351684
num_env_steps_trained: 312
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016568919235351684
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.869186046511626
  ram_util_percent: 47.526744186046514
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28712.122607707977
time_this_iter_s: 120.70828056335449
time_total_s: 28712.122607707977
timers:
  learn_throughput: 3.242
  learn_time_ms: 616.963
  load_throughput: 5018.311
  load_time_ms: 0.399
  sample_time_ms: 122667.672
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 123285.722
timestamp: 1697342046
timesteps_total: 312
training_iteration: 156
trial_id: default

Last checkpoint 155 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000156
server side epoch loop 156
algo.train executed
agent_timesteps_total: 314
connector_metrics: {}
counters:
  num_agent_steps_sampled: 314
  num_agent_steps_trained: 314
  num_env_steps_sampled: 314
  num_env_steps_trained: 314
custom_metrics: {}
date: 2023-10-15_06-56-07
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.332770399588883
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7617907305558522
        entropy_coeff: 0.0
        grad_gnorm: 39.37479375998179
        kl: 0.010889104490676498
        policy_loss: -0.2205200672149658
        total_loss: 1.9544048007577657
        vf_explained_var: .nan
        vf_loss: 2.138633978056411
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9390.5
  num_agent_steps_sampled: 314
  num_agent_steps_trained: 314
  num_env_steps_sampled: 314
  num_env_steps_trained: 314
iterations_since_restore: 157
node_ip: 10.27.41.23
num_agent_steps_sampled: 314
num_agent_steps_trained: 314
num_env_steps_sampled: 314
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016531561069274604
num_env_steps_trained: 314
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016531561069274604
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.522543352601156
  ram_util_percent: 47.908670520231226
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28833.103618383408
time_this_iter_s: 120.9810106754303
time_total_s: 28833.103618383408
timers:
  learn_throughput: 3.069
  learn_time_ms: 651.752
  load_throughput: 5015.31
  load_time_ms: 0.399
  sample_time_ms: 120179.481
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120832.318
timestamp: 1697342167
timesteps_total: 314
training_iteration: 157
trial_id: default

Last checkpoint 156 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000157
server side epoch loop 157
algo.train executed
agent_timesteps_total: 316
connector_metrics: {}
counters:
  num_agent_steps_sampled: 316
  num_agent_steps_trained: 316
  num_env_steps_sampled: 316
  num_env_steps_trained: 316
custom_metrics: {}
date: 2023-10-15_06-58-07
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.332770399588883
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7803601682186128
        entropy_coeff: 0.0
        grad_gnorm: 45.26055618127187
        kl: 0.0044337995374614065
        policy_loss: 0.14065174063046773
        total_loss: 1.3038004290312528
        vf_explained_var: .nan
        vf_loss: 1.1483718652474635
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9450.5
  num_agent_steps_sampled: 316
  num_agent_steps_trained: 316
  num_env_steps_sampled: 316
  num_env_steps_trained: 316
iterations_since_restore: 158
node_ip: 10.27.41.23
num_agent_steps_sampled: 316
num_agent_steps_trained: 316
num_env_steps_sampled: 316
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01659731583476758
num_env_steps_trained: 316
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01659731583476758
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.382558139534883
  ram_util_percent: 46.699999999999996
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 28953.60531449318
time_this_iter_s: 120.50169610977173
time_total_s: 28953.60531449318
timers:
  learn_throughput: 3.08
  learn_time_ms: 649.449
  load_throughput: 5066.502
  load_time_ms: 0.395
  sample_time_ms: 120159.785
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120810.308
timestamp: 1697342287
timesteps_total: 316
training_iteration: 158
trial_id: default

Last checkpoint 157 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000158
server side epoch loop 158
algo.train executed
agent_timesteps_total: 318
connector_metrics: {}
counters:
  num_agent_steps_sampled: 318
  num_agent_steps_trained: 318
  num_env_steps_sampled: 318
  num_env_steps_trained: 318
custom_metrics: {}
date: 2023-10-15_07-00-08
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6663851997944414
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7492495675881703
        entropy_coeff: 0.0
        grad_gnorm: 14.601889993747076
        kl: 0.0187106051866067
        policy_loss: -0.21765771011511484
        total_loss: 5.240660963455836
        vf_explained_var: .nan
        vf_loss: 5.427139690207938
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9510.5
  num_agent_steps_sampled: 318
  num_agent_steps_trained: 318
  num_env_steps_sampled: 318
  num_env_steps_trained: 318
iterations_since_restore: 159
node_ip: 10.27.41.23
num_agent_steps_sampled: 318
num_agent_steps_trained: 318
num_env_steps_sampled: 318
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016575746978440876
num_env_steps_trained: 318
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016575746978440876
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.8383720930232545
  ram_util_percent: 47.368604651162805
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29074.263918161392
time_this_iter_s: 120.65860366821289
time_total_s: 29074.263918161392
timers:
  learn_throughput: 3.144
  learn_time_ms: 636.15
  load_throughput: 5346.809
  load_time_ms: 0.374
  sample_time_ms: 120160.124
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120797.184
timestamp: 1697342408
timesteps_total: 318
training_iteration: 159
trial_id: default

Last checkpoint 158 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000159
server side epoch loop 159
algo.train executed
agent_timesteps_total: 320
connector_metrics: {}
counters:
  num_agent_steps_sampled: 320
  num_agent_steps_trained: 320
  num_env_steps_sampled: 320
  num_env_steps_trained: 320
custom_metrics: {}
date: 2023-10-15_07-02-09
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6663851997944414
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7315539558728537
        entropy_coeff: 0.0
        grad_gnorm: 17.845651199420292
        kl: 0.03916395457696732
        policy_loss: -0.20789177020390828
        total_loss: 0.33475905417775115
        vf_explained_var: .nan
        vf_loss: 0.47738859428621555
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9570.5
  num_agent_steps_sampled: 320
  num_agent_steps_trained: 320
  num_env_steps_sampled: 320
  num_env_steps_trained: 320
iterations_since_restore: 160
node_ip: 10.27.41.23
num_agent_steps_sampled: 320
num_agent_steps_trained: 320
num_env_steps_sampled: 320
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016562853488151962
num_env_steps_trained: 320
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016562853488151962
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.330813953488372
  ram_util_percent: 47.77267441860466
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29195.01635980606
time_this_iter_s: 120.75244164466858
time_total_s: 29195.01635980606
timers:
  learn_throughput: 3.177
  learn_time_ms: 629.528
  load_throughput: 5370.771
  load_time_ms: 0.372
  sample_time_ms: 120150.121
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120780.561
timestamp: 1697342529
timesteps_total: 320
training_iteration: 160
trial_id: default

Last checkpoint 159 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000160
server side epoch loop 160
algo.train executed
agent_timesteps_total: 322
connector_metrics: {}
counters:
  num_agent_steps_sampled: 322
  num_agent_steps_trained: 322
  num_env_steps_sampled: 322
  num_env_steps_trained: 322
custom_metrics: {}
date: 2023-10-15_07-04-09
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4995777996916617
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7204446057478586
        entropy_coeff: 0.0
        grad_gnorm: 9.230359896024067
        kl: 0.016427204143413594
        policy_loss: -0.22315911948680878
        total_loss: 4.929420370856921
        vf_explained_var: .nan
        vf_loss: 5.111518447970351
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9630.5
  num_agent_steps_sampled: 322
  num_agent_steps_trained: 322
  num_env_steps_sampled: 322
  num_env_steps_trained: 322
iterations_since_restore: 161
node_ip: 10.27.41.23
num_agent_steps_sampled: 322
num_agent_steps_trained: 322
num_env_steps_sampled: 322
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016558009610631804
num_env_steps_trained: 322
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016558009610631804
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.2757225433526
  ram_util_percent: 48.001156069364164
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29315.804129362106
time_this_iter_s: 120.78776955604553
time_total_s: 29315.804129362106
timers:
  learn_throughput: 3.742
  learn_time_ms: 534.522
  load_throughput: 5446.087
  load_time_ms: 0.367
  sample_time_ms: 120170.231
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120705.657
timestamp: 1697342649
timesteps_total: 322
training_iteration: 161
trial_id: default

Last checkpoint 160 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000161
server side epoch loop 161
algo.train executed
agent_timesteps_total: 324
connector_metrics: {}
counters:
  num_agent_steps_sampled: 324
  num_agent_steps_trained: 324
  num_env_steps_sampled: 324
  num_env_steps_trained: 324
custom_metrics: {}
date: 2023-10-15_07-06-10
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4995777996916617
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6738283614317575
        entropy_coeff: 0.0
        grad_gnorm: 25.68937889734904
        kl: 0.02484231697805323
        policy_loss: -0.20358416438102722
        total_loss: 0.4749713090558847
        vf_explained_var: .nan
        vf_loss: 0.6164601753155391
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9690.5
  num_agent_steps_sampled: 324
  num_agent_steps_trained: 324
  num_env_steps_sampled: 324
  num_env_steps_trained: 324
iterations_since_restore: 162
node_ip: 10.27.41.23
num_agent_steps_sampled: 324
num_agent_steps_trained: 324
num_env_steps_sampled: 324
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016562246356539783
num_env_steps_trained: 324
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016562246356539783
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.173837209302325
  ram_util_percent: 47.18081395348836
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29436.561069726944
time_this_iter_s: 120.75694036483765
time_total_s: 29436.561069726944
timers:
  learn_throughput: 3.686
  learn_time_ms: 542.552
  load_throughput: 5526.456
  load_time_ms: 0.362
  sample_time_ms: 120252.758
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120796.219
timestamp: 1697342770
timesteps_total: 324
training_iteration: 162
trial_id: default

Last checkpoint 161 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000162
server side epoch loop 162
algo.train executed
agent_timesteps_total: 326
connector_metrics: {}
counters:
  num_agent_steps_sampled: 326
  num_agent_steps_trained: 326
  num_env_steps_sampled: 326
  num_env_steps_trained: 326
custom_metrics: {}
date: 2023-10-15_07-08-11
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7493666995374926
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6553877671559651
        entropy_coeff: 0.0
        grad_gnorm: 14.697901439666747
        kl: 0.012375485368829686
        policy_loss: -0.1416152517000834
        total_loss: 0.1757455221687754
        vf_explained_var: .nan
        vf_loss: 0.27096054088809374
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9750.5
  num_agent_steps_sampled: 326
  num_agent_steps_trained: 326
  num_env_steps_sampled: 326
  num_env_steps_trained: 326
iterations_since_restore: 163
node_ip: 10.27.41.23
num_agent_steps_sampled: 326
num_agent_steps_trained: 326
num_env_steps_sampled: 326
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016576020506552713
num_env_steps_trained: 326
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016576020506552713
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 11.689534883720933
  ram_util_percent: 48.13372093023255
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29557.217612981796
time_this_iter_s: 120.6565432548523
time_total_s: 29557.217612981796
timers:
  learn_throughput: 3.67
  learn_time_ms: 544.914
  load_throughput: 5526.456
  load_time_ms: 0.362
  sample_time_ms: 120172.535
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120718.358
timestamp: 1697342891
timesteps_total: 326
training_iteration: 163
trial_id: default

Last checkpoint 162 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000163
server side epoch loop 163
algo.train executed
agent_timesteps_total: 328
connector_metrics: {}
counters:
  num_agent_steps_sampled: 328
  num_agent_steps_trained: 328
  num_env_steps_sampled: 328
  num_env_steps_trained: 328
custom_metrics: {}
date: 2023-10-15_07-10-12
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7493666995374926
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5779233753681183
        entropy_coeff: 0.0
        grad_gnorm: 25.879780300458272
        kl: 0.02181810112609431
        policy_loss: -0.1951028436422348
        total_loss: 0.689897409081459
        vf_explained_var: .nan
        vf_loss: 0.8031961927811305
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9810.5
  num_agent_steps_sampled: 328
  num_agent_steps_trained: 328
  num_env_steps_sampled: 328
  num_env_steps_trained: 328
iterations_since_restore: 164
node_ip: 10.27.41.23
num_agent_steps_sampled: 328
num_agent_steps_trained: 328
num_env_steps_sampled: 328
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016579940473591754
num_env_steps_trained: 328
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016579940473591754
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.718604651162792
  ram_util_percent: 48.72674418604651
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29677.845599412918
time_this_iter_s: 120.62798643112183
time_total_s: 29677.845599412918
timers:
  learn_throughput: 3.67
  learn_time_ms: 544.943
  load_throughput: 5531.558
  load_time_ms: 0.362
  sample_time_ms: 120163.256
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120709.11
timestamp: 1697343012
timesteps_total: 328
training_iteration: 164
trial_id: default

Last checkpoint 163 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000164
server side epoch loop 164
algo.train executed
agent_timesteps_total: 330
connector_metrics: {}
counters:
  num_agent_steps_sampled: 330
  num_agent_steps_trained: 330
  num_env_steps_sampled: 330
  num_env_steps_trained: 330
custom_metrics: {}
date: 2023-10-15_07-12-12
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8784274240334828
        entropy_coeff: 0.0
        grad_gnorm: 25.590132602055867
        kl: 0.014635091255089112
        policy_loss: -0.2035312056541443
        total_loss: 0.5614527285099029
        vf_explained_var: .nan
        vf_loss: 0.6826754495501518
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9870.5
  num_agent_steps_sampled: 330
  num_agent_steps_trained: 330
  num_env_steps_sampled: 330
  num_env_steps_trained: 330
iterations_since_restore: 165
node_ip: 10.27.41.23
num_agent_steps_sampled: 330
num_agent_steps_trained: 330
num_env_steps_sampled: 330
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016582857907359708
num_env_steps_trained: 330
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016582857907359708
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.712209302325581
  ram_util_percent: 47.22732558139535
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29798.452453136444
time_this_iter_s: 120.606853723526
time_total_s: 29798.452453136444
timers:
  learn_throughput: 3.641
  learn_time_ms: 549.338
  load_throughput: 5393.216
  load_time_ms: 0.371
  sample_time_ms: 120153.182
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120703.448
timestamp: 1697343132
timesteps_total: 330
training_iteration: 165
trial_id: default

Last checkpoint 164 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000165
server side epoch loop 165
algo.train executed
agent_timesteps_total: 332
connector_metrics: {}
counters:
  num_agent_steps_sampled: 332
  num_agent_steps_trained: 332
  num_env_steps_sampled: 332
  num_env_steps_trained: 332
custom_metrics: {}
date: 2023-10-15_07-14-13
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7922211607297263
        entropy_coeff: 0.0
        grad_gnorm: 13.467639859517416
        kl: 0.01732851749305458
        policy_loss: -0.1765732208887736
        total_loss: 0.16702381496628124
        vf_explained_var: .nan
        vf_loss: 0.2461405823601429
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9930.5
  num_agent_steps_sampled: 332
  num_agent_steps_trained: 332
  num_env_steps_sampled: 332
  num_env_steps_trained: 332
iterations_since_restore: 166
node_ip: 10.27.41.23
num_agent_steps_sampled: 332
num_agent_steps_trained: 332
num_env_steps_sampled: 332
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016580000770554434
num_env_steps_trained: 332
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016580000770554434
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.763005780346822
  ram_util_percent: 46.95953757225433
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 29919.080047130585
time_this_iter_s: 120.62759399414062
time_total_s: 29919.080047130585
timers:
  learn_throughput: 3.632
  learn_time_ms: 550.675
  load_throughput: 5345.446
  load_time_ms: 0.374
  sample_time_ms: 120143.781
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120695.38
timestamp: 1697343253
timesteps_total: 332
training_iteration: 166
trial_id: default

Last checkpoint 165 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000166
server side epoch loop 166
algo.train executed
agent_timesteps_total: 334
connector_metrics: {}
counters:
  num_agent_steps_sampled: 334
  num_agent_steps_trained: 334
  num_env_steps_sampled: 334
  num_env_steps_trained: 334
custom_metrics: {}
date: 2023-10-15_07-16-14
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7625449339548747
        entropy_coeff: 0.0
        grad_gnorm: 10.281940476099651
        kl: 0.009562745184424178
        policy_loss: -0.24535754521687825
        total_loss: -0.07520641485850016
        vf_explained_var: .nan
        vf_loss: 0.11636976628457583
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 9990.5
  num_agent_steps_sampled: 334
  num_agent_steps_trained: 334
  num_env_steps_sampled: 334
  num_env_steps_trained: 334
iterations_since_restore: 167
node_ip: 10.27.41.23
num_agent_steps_sampled: 334
num_agent_steps_trained: 334
num_env_steps_sampled: 334
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016581483165111113
num_env_steps_trained: 334
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016581483165111113
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.086627906976743
  ram_util_percent: 47.21395348837209
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30039.696816444397
time_this_iter_s: 120.61676931381226
time_total_s: 30039.696816444397
timers:
  learn_throughput: 3.903
  learn_time_ms: 512.468
  load_throughput: 5293.165
  load_time_ms: 0.378
  sample_time_ms: 120145.553
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120658.957
timestamp: 1697343374
timesteps_total: 334
training_iteration: 167
trial_id: default

Last checkpoint 166 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000167
server side epoch loop 167
algo.train executed
agent_timesteps_total: 336
connector_metrics: {}
counters:
  num_agent_steps_sampled: 336
  num_agent_steps_trained: 336
  num_env_steps_sampled: 336
  num_env_steps_trained: 336
custom_metrics: {}
date: 2023-10-15_07-18-14
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7505839546521504
        entropy_coeff: 0.0
        grad_gnorm: 9.182350889841716
        kl: 0.013210295652970671
        policy_loss: -0.2432822773853938
        total_loss: -0.07536245584487915
        vf_explained_var: .nan
        vf_loss: 0.09362446467033199
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10050.5
  num_agent_steps_sampled: 336
  num_agent_steps_trained: 336
  num_env_steps_sampled: 336
  num_env_steps_trained: 336
iterations_since_restore: 168
node_ip: 10.27.41.23
num_agent_steps_sampled: 336
num_agent_steps_trained: 336
num_env_steps_sampled: 336
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016580369345272464
num_env_steps_trained: 336
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016580369345272464
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.597093023255814
  ram_util_percent: 48.2906976744186
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30160.321707963943
time_this_iter_s: 120.62489151954651
time_total_s: 30160.321707963943
timers:
  learn_throughput: 3.884
  learn_time_ms: 514.884
  load_throughput: 5312.944
  load_time_ms: 0.376
  sample_time_ms: 120155.455
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120671.272
timestamp: 1697343494
timesteps_total: 336
training_iteration: 168
trial_id: default

Last checkpoint 167 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000168
server side epoch loop 168
algo.train executed
agent_timesteps_total: 338
connector_metrics: {}
counters:
  num_agent_steps_sampled: 338
  num_agent_steps_trained: 338
  num_env_steps_sampled: 338
  num_env_steps_trained: 338
custom_metrics: {}
date: 2023-10-15_07-20-15
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7828420003255208
        entropy_coeff: 0.0
        grad_gnorm: 10.744492713610331
        kl: 0.009305425735995717
        policy_loss: -0.21696498195330302
        total_loss: -0.04230908950169881
        vf_explained_var: .nan
        vf_loss: 0.12232170496912052
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10110.5
  num_agent_steps_sampled: 338
  num_agent_steps_trained: 338
  num_env_steps_sampled: 338
  num_env_steps_trained: 338
iterations_since_restore: 169
node_ip: 10.27.41.23
num_agent_steps_sampled: 338
num_agent_steps_trained: 338
num_env_steps_sampled: 338
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016573429934200568
num_env_steps_trained: 338
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016573429934200568
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.422674418604652
  ram_util_percent: 48.4639534883721
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30280.997131347656
time_this_iter_s: 120.67542338371277
time_total_s: 30280.997131347656
timers:
  learn_throughput: 3.871
  learn_time_ms: 516.64
  load_throughput: 5291.496
  load_time_ms: 0.378
  sample_time_ms: 120155.393
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 120672.959
timestamp: 1697343615
timesteps_total: 338
training_iteration: 169
trial_id: default

Last checkpoint 168 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000169
server side epoch loop 169
algo.train executed
agent_timesteps_total: 340
connector_metrics: {}
counters:
  num_agent_steps_sampled: 340
  num_agent_steps_trained: 340
  num_env_steps_sampled: 340
  num_env_steps_trained: 340
custom_metrics: {}
date: 2023-10-15_07-22-15
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5981206119060516
        entropy_coeff: 0.0
        grad_gnorm: 9.122869336605072
        kl: 0.013065711497559581
        policy_loss: -0.18838490347067516
        total_loss: -0.030438359578450522
        vf_explained_var: .nan
        vf_loss: 0.08446431872434915
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10170.5
  num_agent_steps_sampled: 340
  num_agent_steps_trained: 340
  num_env_steps_sampled: 340
  num_env_steps_trained: 340
iterations_since_restore: 170
node_ip: 10.27.41.23
num_agent_steps_sampled: 340
num_agent_steps_trained: 340
num_env_steps_sampled: 340
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016604248710965438
num_env_steps_trained: 340
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016604248710965438
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.966860465116278
  ram_util_percent: 48.05523255813954
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30401.448564767838
time_this_iter_s: 120.45143342018127
time_total_s: 30401.448564767838
timers:
  learn_throughput: 3.948
  learn_time_ms: 506.628
  load_throughput: 5336.604
  load_time_ms: 0.375
  sample_time_ms: 120135.308
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120642.855
timestamp: 1697343735
timesteps_total: 340
training_iteration: 170
trial_id: default

Last checkpoint 169 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000170
server side epoch loop 170
algo.train executed
agent_timesteps_total: 342
connector_metrics: {}
counters:
  num_agent_steps_sampled: 342
  num_agent_steps_trained: 342
  num_env_steps_sampled: 342
  num_env_steps_trained: 342
custom_metrics: {}
date: 2023-10-15_07-24-16
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8421613852183023
        entropy_coeff: 0.0
        grad_gnorm: 11.944295569260914
        kl: 0.010007700666513604
        policy_loss: -0.1996605137983958
        total_loss: 0.02462449073791504
        vf_explained_var: .nan
        vf_loss: 0.16800120111875003
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10230.5
  num_agent_steps_sampled: 342
  num_agent_steps_trained: 342
  num_env_steps_sampled: 342
  num_env_steps_trained: 342
iterations_since_restore: 171
node_ip: 10.27.41.23
num_agent_steps_sampled: 342
num_agent_steps_trained: 342
num_env_steps_sampled: 342
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016580246845695343
num_env_steps_trained: 342
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016580246845695343
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.537209302325582
  ram_util_percent: 47.52790697674419
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30522.074352264404
time_this_iter_s: 120.62578749656677
time_total_s: 30522.074352264404
timers:
  learn_throughput: 3.995
  learn_time_ms: 500.685
  load_throughput: 5247.8
  load_time_ms: 0.381
  sample_time_ms: 120125.043
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120626.655
timestamp: 1697343856
timesteps_total: 342
training_iteration: 171
trial_id: default

Last checkpoint 170 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000171
server side epoch loop 171
algo.train executed
agent_timesteps_total: 344
connector_metrics: {}
counters:
  num_agent_steps_sampled: 344
  num_agent_steps_trained: 344
  num_env_steps_sampled: 344
  num_env_steps_trained: 344
custom_metrics: {}
date: 2023-10-15_07-26-17
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7735897819201152
        entropy_coeff: 0.0
        grad_gnorm: 8.921724931399028
        kl: 0.013393099454212158
        policy_loss: -0.20927558243274688
        total_loss: -0.04944471617539724
        vf_explained_var: .nan
        vf_loss: 0.08450739678082755
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10290.5
  num_agent_steps_sampled: 344
  num_agent_steps_trained: 344
  num_env_steps_sampled: 344
  num_env_steps_trained: 344
iterations_since_restore: 172
node_ip: 10.27.41.23
num_agent_steps_sampled: 344
num_agent_steps_trained: 344
num_env_steps_sampled: 344
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.0165867687776543
num_env_steps_trained: 344
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.0165867687776543
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.095930232558139
  ram_util_percent: 48.01511627906977
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30642.652718305588
time_this_iter_s: 120.57836604118347
time_total_s: 30642.652718305588
timers:
  learn_throughput: 4.06
  learn_time_ms: 492.651
  load_throughput: 5365.619
  load_time_ms: 0.373
  sample_time_ms: 120115.241
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120608.802
timestamp: 1697343977
timesteps_total: 344
training_iteration: 172
trial_id: default

Last checkpoint 171 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000172
server side epoch loop 172
algo.train executed
agent_timesteps_total: 346
connector_metrics: {}
counters:
  num_agent_steps_sampled: 346
  num_agent_steps_trained: 346
  num_env_steps_sampled: 346
  num_env_steps_trained: 346
custom_metrics: {}
date: 2023-10-15_07-28-17
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.624050049306239
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8007850110530854
        entropy_coeff: 0.0
        grad_gnorm: 12.982603669166565
        kl: 0.002130312747158314
        policy_loss: -0.05172318716843923
        total_loss: 0.12709085444609325
        vf_explained_var: .nan
        vf_loss: 0.16683306510894907
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10350.5
  num_agent_steps_sampled: 346
  num_agent_steps_trained: 346
  num_env_steps_sampled: 346
  num_env_steps_trained: 346
iterations_since_restore: 173
node_ip: 10.27.41.23
num_agent_steps_sampled: 346
num_agent_steps_trained: 346
num_env_steps_sampled: 346
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01659113067472654
num_env_steps_trained: 346
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01659113067472654
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 6.40639534883721
  ram_util_percent: 47.76918604651162
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30763.199379444122
time_this_iter_s: 120.54666113853455
time_total_s: 30763.199379444122
timers:
  learn_throughput: 4.071
  learn_time_ms: 491.238
  load_throughput: 5566.429
  load_time_ms: 0.359
  sample_time_ms: 120105.68
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120597.814
timestamp: 1697344097
timesteps_total: 346
training_iteration: 173
trial_id: default

Last checkpoint 172 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000173
server side epoch loop 173
algo.train executed
agent_timesteps_total: 348
connector_metrics: {}
counters:
  num_agent_steps_sampled: 348
  num_agent_steps_trained: 348
  num_env_steps_sampled: 348
  num_env_steps_trained: 348
custom_metrics: {}
date: 2023-10-15_07-30-18
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8120250246531193
        cur_lr: 5.0000000000000016e-05
        entropy: 1.4993218580881755
        entropy_coeff: 0.0
        grad_gnorm: 7.450732723871867
        kl: 0.011813547858885916
        policy_loss: -0.2118354727824529
        total_loss: -0.1359448790550232
        vf_explained_var: .nan
        vf_loss: 0.04267060426136595
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10410.5
  num_agent_steps_sampled: 348
  num_agent_steps_trained: 348
  num_env_steps_sampled: 348
  num_env_steps_trained: 348
iterations_since_restore: 174
node_ip: 10.27.41.23
num_agent_steps_sampled: 348
num_agent_steps_trained: 348
num_env_steps_sampled: 348
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016571032616909433
num_env_steps_trained: 348
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016571032616909433
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.633139534883721
  ram_util_percent: 47.6406976744186
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 30883.892215013504
time_this_iter_s: 120.69283556938171
time_total_s: 30883.892215013504
timers:
  learn_throughput: 4.095
  learn_time_ms: 488.361
  load_throughput: 5550.591
  load_time_ms: 0.36
  sample_time_ms: 120115.042
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120604.298
timestamp: 1697344218
timesteps_total: 348
training_iteration: 174
trial_id: default

Last checkpoint 173 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000174
server side epoch loop 174
algo.train executed
agent_timesteps_total: 350
connector_metrics: {}
counters:
  num_agent_steps_sampled: 350
  num_agent_steps_trained: 350
  num_env_steps_sampled: 350
  num_env_steps_trained: 350
custom_metrics: {}
date: 2023-10-15_07-32-19
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8120250246531193
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7484225511550904
        entropy_coeff: 0.0
        grad_gnorm: 12.206436775128047
        kl: 0.013538703516496753
        policy_loss: -0.24262267748514812
        total_loss: -0.019499885042508443
        vf_explained_var: .nan
        vf_loss: 0.18505161615272905
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10470.5
  num_agent_steps_sampled: 350
  num_agent_steps_trained: 350
  num_env_steps_sampled: 350
  num_env_steps_trained: 350
iterations_since_restore: 175
node_ip: 10.27.41.23
num_agent_steps_sampled: 350
num_agent_steps_trained: 350
num_env_steps_sampled: 350
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658296080913543
num_env_steps_trained: 350
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658296080913543
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.929651162790696
  ram_util_percent: 47.39476744186047
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31004.498298168182
time_this_iter_s: 120.60608315467834
time_total_s: 31004.498298168182
timers:
  learn_throughput: 4.18
  learn_time_ms: 478.445
  load_throughput: 5823.804
  load_time_ms: 0.343
  sample_time_ms: 120124.904
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120604.223
timestamp: 1697344339
timesteps_total: 350
training_iteration: 175
trial_id: default

Last checkpoint 174 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000175
server side epoch loop 175
algo.train executed
agent_timesteps_total: 352
connector_metrics: {}
counters:
  num_agent_steps_sampled: 352
  num_agent_steps_trained: 352
  num_env_steps_sampled: 352
  num_env_steps_trained: 352
custom_metrics: {}
date: 2023-10-15_07-34-19
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8120250246531193
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7300121307373046
        entropy_coeff: 0.0
        grad_gnorm: 6.75806516011556
        kl: 0.00103487567200015
        policy_loss: 0.005545379718144735
        total_loss: 0.011030045151710511
        vf_explained_var: .nan
        vf_loss: 0.00257456606074508
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10530.5
  num_agent_steps_sampled: 352
  num_agent_steps_trained: 352
  num_env_steps_sampled: 352
  num_env_steps_trained: 352
iterations_since_restore: 176
node_ip: 10.27.41.23
num_agent_steps_sampled: 352
num_agent_steps_trained: 352
num_env_steps_sampled: 352
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01657093474053647
num_env_steps_trained: 352
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01657093474053647
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 6.4633720930232545
  ram_util_percent: 47.5203488372093
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31125.191850423813
time_this_iter_s: 120.6935522556305
time_total_s: 31125.191850423813
timers:
  learn_throughput: 4.205
  learn_time_ms: 475.622
  load_throughput: 6078.261
  load_time_ms: 0.329
  sample_time_ms: 120134.342
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120610.821
timestamp: 1697344459
timesteps_total: 352
training_iteration: 176
trial_id: default

Last checkpoint 175 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000176
server side epoch loop 176
algo.train executed
agent_timesteps_total: 354
connector_metrics: {}
counters:
  num_agent_steps_sampled: 354
  num_agent_steps_trained: 354
  num_env_steps_sampled: 354
  num_env_steps_trained: 354
custom_metrics: {}
date: 2023-10-15_07-36-20
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4060125123265597
        cur_lr: 5.0000000000000016e-05
        entropy: 1.4682246327400208
        entropy_coeff: 0.0
        grad_gnorm: 5.937696139017741
        kl: 0.024095756665155933
        policy_loss: -0.22139919300874075
        total_loss: -0.15327773292859395
        vf_explained_var: .nan
        vf_loss: 0.034242528735436886
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10590.5
  num_agent_steps_sampled: 354
  num_agent_steps_trained: 354
  num_env_steps_sampled: 354
  num_env_steps_trained: 354
iterations_since_restore: 177
node_ip: 10.27.41.23
num_agent_steps_sampled: 354
num_agent_steps_trained: 354
num_env_steps_sampled: 354
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01660700035770961
num_env_steps_trained: 354
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01660700035770961
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.39593023255814
  ram_util_percent: 47.79360465116279
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31245.623368263245
time_this_iter_s: 120.43151783943176
time_total_s: 31245.623368263245
timers:
  learn_throughput: 4.214
  learn_time_ms: 474.599
  load_throughput: 6164.015
  load_time_ms: 0.324
  sample_time_ms: 120116.836
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120592.288
timestamp: 1697344580
timesteps_total: 354
training_iteration: 177
trial_id: default

Last checkpoint 176 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000177
server side epoch loop 177
algo.train executed
agent_timesteps_total: 356
connector_metrics: {}
counters:
  num_agent_steps_sampled: 356
  num_agent_steps_trained: 356
  num_env_steps_sampled: 356
  num_env_steps_trained: 356
custom_metrics: {}
date: 2023-10-15_07-38-20
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.10901876848984
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8380680978298187
        entropy_coeff: 0.0
        grad_gnorm: 25.466805678606033
        kl: 0.014465612011069122
        policy_loss: -0.2347896248102188
        total_loss: 0.6461626068378489
        vf_explained_var: .nan
        vf_loss: 0.8504439923912287
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10650.5
  num_agent_steps_sampled: 356
  num_agent_steps_trained: 356
  num_env_steps_sampled: 356
  num_env_steps_trained: 356
iterations_since_restore: 178
node_ip: 10.27.41.23
num_agent_steps_sampled: 356
num_agent_steps_trained: 356
num_env_steps_sampled: 356
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016588818316550225
num_env_steps_trained: 356
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016588818316550225
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 6.948255813953488
  ram_util_percent: 47.40988372093023
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31366.186866044998
time_this_iter_s: 120.56349778175354
time_total_s: 31366.186866044998
timers:
  learn_throughput: 4.181
  learn_time_ms: 478.359
  load_throughput: 6154.518
  load_time_ms: 0.325
  sample_time_ms: 120106.927
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120586.144
timestamp: 1697344700
timesteps_total: 356
training_iteration: 178
trial_id: default

Last checkpoint 177 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000178
server side epoch loop 178
algo.train executed
agent_timesteps_total: 358
connector_metrics: {}
counters:
  num_agent_steps_sampled: 358
  num_agent_steps_trained: 358
  num_env_steps_sampled: 358
  num_env_steps_trained: 358
custom_metrics: {}
date: 2023-10-15_07-40-21
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.10901876848984
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6153296113014222
        entropy_coeff: 0.0
        grad_gnorm: 2.8189551055431368
        kl: 0.026177688656995695
        policy_loss: -0.21446906328201293
        total_loss: -0.15629740953445434
        vf_explained_var: .nan
        vf_loss: 0.0029624195720922824
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10710.5
  num_agent_steps_sampled: 358
  num_agent_steps_trained: 358
  num_env_steps_sampled: 358
  num_env_steps_trained: 358
iterations_since_restore: 179
node_ip: 10.27.41.23
num_agent_steps_sampled: 358
num_agent_steps_trained: 358
num_env_steps_sampled: 358
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658729548086955
num_env_steps_trained: 358
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658729548086955
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.356395348837207
  ram_util_percent: 47.33895348837211
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31486.761371850967
time_this_iter_s: 120.57450580596924
time_total_s: 31486.761371850967
timers:
  learn_throughput: 4.178
  learn_time_ms: 478.717
  load_throughput: 6093.715
  load_time_ms: 0.328
  sample_time_ms: 120096.472
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120576.057
timestamp: 1697344821
timesteps_total: 358
training_iteration: 179
trial_id: default

Last checkpoint 178 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000179
server side epoch loop 179
algo.train executed
agent_timesteps_total: 360
connector_metrics: {}
counters:
  num_agent_steps_sampled: 360
  num_agent_steps_trained: 360
  num_env_steps_sampled: 360
  num_env_steps_trained: 360
custom_metrics: {}
date: 2023-10-15_07-42-21
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.7864543239275614
        entropy_coeff: 0.0
        grad_gnorm: 7.026296669244767
        kl: 0.012709882590085424
        policy_loss: -0.22685051163037617
        total_loss: -0.13517706195513407
        vf_explained_var: .nan
        vf_loss: 0.051465381534702224
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10770.5
  num_agent_steps_sampled: 360
  num_agent_steps_trained: 360
  num_env_steps_sampled: 360
  num_env_steps_trained: 360
iterations_since_restore: 180
node_ip: 10.27.41.23
num_agent_steps_sampled: 360
num_agent_steps_trained: 360
num_env_steps_sampled: 360
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016603559405386183
num_env_steps_trained: 360
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016603559405386183
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.838953488372093
  ram_util_percent: 47.841279069767445
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31607.217772245407
time_this_iter_s: 120.4564003944397
time_total_s: 31607.217772245407
timers:
  learn_throughput: 4.168
  learn_time_ms: 479.79
  load_throughput: 6173.087
  load_time_ms: 0.324
  sample_time_ms: 120095.909
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120576.558
timestamp: 1697344941
timesteps_total: 360
training_iteration: 180
trial_id: default

Last checkpoint 179 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000180
server side epoch loop 180
algo.train executed
agent_timesteps_total: 362
connector_metrics: {}
counters:
  num_agent_steps_sampled: 362
  num_agent_steps_trained: 362
  num_env_steps_sampled: 362
  num_env_steps_trained: 362
custom_metrics: {}
date: 2023-10-15_07-44-22
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5105824808279673
        entropy_coeff: 0.0
        grad_gnorm: 7.905203076203664
        kl: 0.015516690700557471
        policy_loss: -0.23148317337036134
        total_loss: -0.11261735260486602
        vf_explained_var: .nan
        vf_loss: 0.06977832619692587
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10830.5
  num_agent_steps_sampled: 362
  num_agent_steps_trained: 362
  num_env_steps_sampled: 362
  num_env_steps_trained: 362
iterations_since_restore: 181
node_ip: 10.27.41.23
num_agent_steps_sampled: 362
num_agent_steps_trained: 362
num_env_steps_sampled: 362
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01654913264337724
num_env_steps_trained: 362
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01654913264337724
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.40057803468208
  ram_util_percent: 46.783236994219656
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31728.070358753204
time_this_iter_s: 120.85258650779724
time_total_s: 31728.070358753204
timers:
  learn_throughput: 4.147
  learn_time_ms: 482.314
  load_throughput: 6226.698
  load_time_ms: 0.321
  sample_time_ms: 120116.054
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120599.236
timestamp: 1697345062
timesteps_total: 362
training_iteration: 181
trial_id: default

Last checkpoint 180 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000181
server side epoch loop 181
algo.train executed
agent_timesteps_total: 364
connector_metrics: {}
counters:
  num_agent_steps_sampled: 364
  num_agent_steps_trained: 364
  num_env_steps_sampled: 364
  num_env_steps_trained: 364
custom_metrics: {}
date: 2023-10-15_07-46-23
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.1844149351119995
        entropy_coeff: 0.0
        grad_gnorm: 5.757001302639643
        kl: 0.006789160675301294
        policy_loss: -0.14720210433006287
        total_loss: -0.08946778078873953
        vf_explained_var: .nan
        vf_loss: 0.03625661686285942
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10890.5
  num_agent_steps_sampled: 364
  num_agent_steps_trained: 364
  num_env_steps_sampled: 364
  num_env_steps_trained: 364
iterations_since_restore: 182
node_ip: 10.27.41.23
num_agent_steps_sampled: 364
num_agent_steps_trained: 364
num_env_steps_sampled: 364
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016589870376330546
num_env_steps_trained: 364
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016589870376330546
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.095348837209302
  ram_util_percent: 47.27209302325582
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31848.626138448715
time_this_iter_s: 120.55577969551086
time_total_s: 31848.626138448715
timers:
  learn_throughput: 4.188
  learn_time_ms: 477.596
  load_throughput: 6219.773
  load_time_ms: 0.322
  sample_time_ms: 120118.513
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120596.982
timestamp: 1697345183
timesteps_total: 364
training_iteration: 182
trial_id: default

Last checkpoint 181 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000182
server side epoch loop 182
algo.train executed
agent_timesteps_total: 366
connector_metrics: {}
counters:
  num_agent_steps_sampled: 366
  num_agent_steps_trained: 366
  num_env_steps_sampled: 366
  num_env_steps_trained: 366
custom_metrics: {}
date: 2023-10-15_07-48-24
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.2393328775962194
        entropy_coeff: 0.0
        grad_gnorm: 4.4485917568206785
        kl: 0.007565482270426097
        policy_loss: -0.09386336902777354
        total_loss: -0.06959389746189118
        vf_explained_var: .nan
        vf_loss: 0.0003358619488911548
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 10950.5
  num_agent_steps_sampled: 366
  num_agent_steps_trained: 366
  num_env_steps_sampled: 366
  num_env_steps_trained: 366
iterations_since_restore: 183
node_ip: 10.27.41.23
num_agent_steps_sampled: 366
num_agent_steps_trained: 366
num_env_steps_sampled: 366
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01658525622109578
num_env_steps_trained: 366
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01658525622109578
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.971511627906976
  ram_util_percent: 47.679651162790684
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 31969.2155585289
time_this_iter_s: 120.58942008018494
time_total_s: 31969.2155585289
timers:
  learn_throughput: 4.146
  learn_time_ms: 482.423
  load_throughput: 6369.966
  load_time_ms: 0.314
  sample_time_ms: 120117.965
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120601.251
timestamp: 1697345304
timesteps_total: 366
training_iteration: 183
trial_id: default

Last checkpoint 182 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000183
server side epoch loop 183
algo.train executed
agent_timesteps_total: 368
connector_metrics: {}
counters:
  num_agent_steps_sampled: 368
  num_agent_steps_trained: 368
  num_env_steps_sampled: 368
  num_env_steps_trained: 368
custom_metrics: {}
date: 2023-10-15_07-50-24
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.639521833260854
        entropy_coeff: 0.0
        grad_gnorm: 5.410555742184321
        kl: 0.007834766081426399
        policy_loss: -0.13607372740904491
        total_loss: -0.09461108148097992
        vf_explained_var: .nan
        vf_loss: 0.016677138950277972
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11010.5
  num_agent_steps_sampled: 368
  num_agent_steps_trained: 368
  num_env_steps_sampled: 368
  num_env_steps_trained: 368
iterations_since_restore: 184
node_ip: 10.27.41.23
num_agent_steps_sampled: 368
num_agent_steps_trained: 368
num_env_steps_sampled: 368
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016571316464929528
num_env_steps_trained: 368
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016571316464929528
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 7.566279069767442
  ram_util_percent: 47.53662790697675
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32089.906351566315
time_this_iter_s: 120.69079303741455
time_total_s: 32089.906351566315
timers:
  learn_throughput: 4.064
  learn_time_ms: 492.116
  load_throughput: 6262.96
  load_time_ms: 0.319
  sample_time_ms: 120108.057
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120601.043
timestamp: 1697345424
timesteps_total: 368
training_iteration: 184
trial_id: default

Last checkpoint 183 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000184
server side epoch loop 184
algo.train executed
agent_timesteps_total: 370
connector_metrics: {}
counters:
  num_agent_steps_sampled: 370
  num_agent_steps_trained: 370
  num_env_steps_sampled: 370
  num_env_steps_trained: 370
custom_metrics: {}
date: 2023-10-15_07-52-25
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5189868807792664
        entropy_coeff: 0.0
        grad_gnorm: 6.395906122525533
        kl: 0.009374117458052448
        policy_loss: -0.2549061586459478
        total_loss: -0.19143321414788564
        vf_explained_var: .nan
        vf_loss: 0.033817661554348885
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11070.5
  num_agent_steps_sampled: 370
  num_agent_steps_trained: 370
  num_env_steps_sampled: 370
  num_env_steps_trained: 370
iterations_since_restore: 185
node_ip: 10.27.41.23
num_agent_steps_sampled: 370
num_agent_steps_trained: 370
num_env_steps_sampled: 370
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016567748366893523
num_env_steps_trained: 370
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016567748366893523
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.82267441860465
  ram_util_percent: 47.83372093023256
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32210.623123168945
time_this_iter_s: 120.71677160263062
time_total_s: 32210.623123168945
timers:
  learn_throughput: 3.978
  learn_time_ms: 502.749
  load_throughput: 6254.088
  load_time_ms: 0.32
  sample_time_ms: 120108.505
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120612.118
timestamp: 1697345545
timesteps_total: 370
training_iteration: 185
trial_id: default

Last checkpoint 184 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000185
server side epoch loop 185
algo.train executed
agent_timesteps_total: 372
connector_metrics: {}
counters:
  num_agent_steps_sampled: 372
  num_agent_steps_trained: 372
  num_env_steps_sampled: 372
  num_env_steps_trained: 372
custom_metrics: {}
date: 2023-10-15_07-54-26
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.4693175752957661
        entropy_coeff: 0.0
        grad_gnorm: 3.995626002550125
        kl: 0.017137744008505252
        policy_loss: -0.2213317225376765
        total_loss: -0.1534416437149048
        vf_explained_var: .nan
        vf_loss: 0.013674332501492852
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11130.5
  num_agent_steps_sampled: 372
  num_agent_steps_trained: 372
  num_env_steps_sampled: 372
  num_env_steps_trained: 372
iterations_since_restore: 186
node_ip: 10.27.41.23
num_agent_steps_sampled: 372
num_agent_steps_trained: 372
num_env_steps_sampled: 372
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01657685781875073
num_env_steps_trained: 372
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01657685781875073
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.98779069767442
  ram_util_percent: 48.13139534883722
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32331.273606300354
time_this_iter_s: 120.65048313140869
time_total_s: 32331.273606300354
timers:
  learn_throughput: 4.013
  learn_time_ms: 498.34
  load_throughput: 6386.455
  load_time_ms: 0.313
  sample_time_ms: 120108.607
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120607.807
timestamp: 1697345666
timesteps_total: 372
training_iteration: 186
trial_id: default

Last checkpoint 185 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000186
server side epoch loop 186
algo.train executed
agent_timesteps_total: 374
connector_metrics: {}
counters:
  num_agent_steps_sampled: 374
  num_agent_steps_trained: 374
  num_env_steps_sampled: 374
  num_env_steps_trained: 374
custom_metrics: {}
date: 2023-10-15_07-56-26
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5785689493020376
        entropy_coeff: 0.0
        grad_gnorm: 6.372175598144532
        kl: 0.01986707294054213
        policy_loss: -0.22779692908128102
        total_loss: -0.12484490772088369
        vf_explained_var: .nan
        vf_loss: 0.04010197413493491
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11190.5
  num_agent_steps_sampled: 374
  num_agent_steps_trained: 374
  num_env_steps_sampled: 374
  num_env_steps_trained: 374
iterations_since_restore: 187
node_ip: 10.27.41.23
num_agent_steps_sampled: 374
num_agent_steps_trained: 374
num_env_steps_sampled: 374
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01657030223826355
num_env_steps_trained: 374
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01657030223826355
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.442441860465118
  ram_util_percent: 47.50348837209302
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32451.97175884247
time_this_iter_s: 120.69815254211426
time_total_s: 32451.97175884247
timers:
  learn_throughput: 3.948
  learn_time_ms: 506.608
  load_throughput: 6431.009
  load_time_ms: 0.311
  sample_time_ms: 120127.02
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120634.479
timestamp: 1697345786
timesteps_total: 374
training_iteration: 187
trial_id: default

Last checkpoint 186 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000187
server side epoch loop 187
algo.train executed
agent_timesteps_total: 376
connector_metrics: {}
counters:
  num_agent_steps_sampled: 376
  num_agent_steps_trained: 376
  num_env_steps_sampled: 376
  num_env_steps_trained: 376
custom_metrics: {}
date: 2023-10-15_07-58-28
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5677425960699718
        entropy_coeff: 0.0
        grad_gnorm: 5.805699149767558
        kl: 0.017059125546074937
        policy_loss: -0.25109840631484986
        total_loss: -0.1717053731282552
        vf_explained_var: .nan
        vf_loss: 0.025426009201813333
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11250.5
  num_agent_steps_sampled: 376
  num_agent_steps_trained: 376
  num_env_steps_sampled: 376
  num_env_steps_trained: 376
iterations_since_restore: 188
node_ip: 10.27.41.23
num_agent_steps_sampled: 376
num_agent_steps_trained: 376
num_env_steps_sampled: 376
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016510884372774617
num_env_steps_trained: 376
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016510884372774617
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.978612716763006
  ram_util_percent: 47.290751445086705
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32573.104266405106
time_this_iter_s: 121.13250756263733
time_total_s: 32573.104266405106
timers:
  learn_throughput: 4.055
  learn_time_ms: 493.168
  load_throughput: 6307.224
  load_time_ms: 0.317
  sample_time_ms: 120197.369
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 120691.387
timestamp: 1697345908
timesteps_total: 376
training_iteration: 188
trial_id: default

Last checkpoint 187 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000188
server side epoch loop 188
algo.train executed
agent_timesteps_total: 378
connector_metrics: {}
counters:
  num_agent_steps_sampled: 378
  num_agent_steps_trained: 378
  num_env_steps_sampled: 378
  num_env_steps_trained: 378
custom_metrics: {}
date: 2023-10-15_08-00-39
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6125791509946188
        entropy_coeff: 0.0
        grad_gnorm: 5.286457637945811
        kl: 0.018598226617905313
        policy_loss: -0.20452515582243602
        total_loss: -0.12100787858168284
        vf_explained_var: .nan
        vf_loss: 0.024681262336525834
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11310.5
  num_agent_steps_sampled: 378
  num_agent_steps_trained: 378
  num_env_steps_sampled: 378
  num_env_steps_trained: 378
iterations_since_restore: 189
node_ip: 10.27.41.23
num_agent_steps_sampled: 378
num_agent_steps_trained: 378
num_env_steps_sampled: 378
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.015203982400690328
num_env_steps_trained: 378
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.015203982400690328
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.000000000000002
  ram_util_percent: 46.43191489361703
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32704.64910006523
time_this_iter_s: 131.54483366012573
time_total_s: 32704.64910006523
timers:
  learn_throughput: 4.015
  learn_time_ms: 498.094
  load_throughput: 6381.111
  load_time_ms: 0.313
  sample_time_ms: 121289.476
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 121788.414
timestamp: 1697346039
timesteps_total: 378
training_iteration: 189
trial_id: default

Last checkpoint 188 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000189
server side epoch loop 189
algo.train executed
agent_timesteps_total: 380
connector_metrics: {}
counters:
  num_agent_steps_sampled: 380
  num_agent_steps_trained: 380
  num_env_steps_sampled: 380
  num_env_steps_trained: 380
custom_metrics: {}
date: 2023-10-15_08-02-57
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8668368379275004
        entropy_coeff: 0.0
        grad_gnorm: 5.879781208435694
        kl: 0.008937646998189545
        policy_loss: -0.23034522235393523
        total_loss: -0.16829188366731007
        vf_explained_var: .nan
        vf_loss: 0.033778845825630316
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11370.5
  num_agent_steps_sampled: 380
  num_agent_steps_trained: 380
  num_env_steps_sampled: 380
  num_env_steps_trained: 380
iterations_since_restore: 190
node_ip: 10.27.41.23
num_agent_steps_sampled: 380
num_agent_steps_trained: 380
num_env_steps_sampled: 380
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.014495447136908965
num_env_steps_trained: 380
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.014495447136908965
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.348730964467004
  ram_util_percent: 48.24314720812182
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32842.6237578392
time_this_iter_s: 137.97465777397156
time_total_s: 32842.6237578392
timers:
  learn_throughput: 4.021
  learn_time_ms: 497.373
  load_throughput: 6274.202
  load_time_ms: 0.319
  sample_time_ms: 123042.012
  synch_weights_time_ms: 0.01
  training_iteration_time_ms: 123540.24
timestamp: 1697346177
timesteps_total: 380
training_iteration: 190
trial_id: default

Last checkpoint 189 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000190
server side epoch loop 190
algo.train executed
agent_timesteps_total: 382
connector_metrics: {}
counters:
  num_agent_steps_sampled: 382
  num_agent_steps_trained: 382
  num_env_steps_sampled: 382
  num_env_steps_trained: 382
custom_metrics: {}
date: 2023-10-15_08-04-58
done: false
episode_len_mean: 127.5
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 94.9752391309524
episode_reward_min: 85.9741491984127
episodes_this_iter: 0
episodes_total: 2
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8660167018572489
        entropy_coeff: 0.0
        grad_gnorm: 5.639499835173289
        kl: 0.013561751544330036
        policy_loss: -0.17968420187632242
        total_loss: -0.11487711171309153
        vf_explained_var: .nan
        vf_loss: 0.021904107080141937
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11430.5
  num_agent_steps_sampled: 382
  num_agent_steps_trained: 382
  num_env_steps_sampled: 382
  num_env_steps_trained: 382
iterations_since_restore: 191
node_ip: 10.27.41.23
num_agent_steps_sampled: 382
num_agent_steps_trained: 382
num_env_steps_sampled: 382
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016536323243441748
num_env_steps_trained: 382
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016536323243441748
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.457558139534884
  ram_util_percent: 49.04244186046511
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17255926330471238
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 89623.0182705628
  mean_inference_ms: 3.057178099926551
  mean_raw_obs_processing_ms: 1.754417619826994
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.5
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 94.9752391309524
  episode_reward_min: 85.9741491984127
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17255926330471238
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 89623.0182705628
    mean_inference_ms: 3.057178099926551
    mean_raw_obs_processing_ms: 1.754417619826994
time_since_restore: 32963.56995391846
time_this_iter_s: 120.94619607925415
time_total_s: 32963.56995391846
timers:
  learn_throughput: 4.028
  learn_time_ms: 496.58
  load_throughput: 6304.854
  load_time_ms: 0.317
  sample_time_ms: 123052.182
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123549.602
timestamp: 1697346298
timesteps_total: 382
training_iteration: 191
trial_id: default

Last checkpoint 190 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000191
server side epoch loop 191
algo.train executed
agent_timesteps_total: 384
connector_metrics: {}
counters:
  num_agent_steps_sampled: 384
  num_agent_steps_trained: 384
  num_env_steps_sampled: 384
  num_env_steps_trained: 384
custom_metrics: {}
date: 2023-10-15_08-06-59
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 1
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8539085288842518
        entropy_coeff: 0.0
        grad_gnorm: 6.865389495094617
        kl: 0.011923073316696293
        policy_loss: -0.22208056350549063
        total_loss: 4.877712171276411
        vf_explained_var: .nan
        vf_loss: 5.062073805493613
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11490.5
  num_agent_steps_sampled: 384
  num_agent_steps_trained: 384
  num_env_steps_sampled: 384
  num_env_steps_trained: 384
iterations_since_restore: 192
node_ip: 10.27.41.23
num_agent_steps_sampled: 384
num_agent_steps_trained: 384
num_env_steps_sampled: 384
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01655990137473365
num_env_steps_trained: 384
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01655990137473365
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.295375722543353
  ram_util_percent: 48.63121387283236
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 1
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33084.34395503998
time_this_iter_s: 120.774001121521
time_total_s: 33084.34395503998
timers:
  learn_throughput: 3.95
  learn_time_ms: 506.272
  load_throughput: 5995.717
  load_time_ms: 0.334
  sample_time_ms: 123064.284
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123571.418
timestamp: 1697346419
timesteps_total: 384
training_iteration: 192
trial_id: default

Last checkpoint 191 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000192
server side epoch loop 192
algo.train executed
agent_timesteps_total: 386
connector_metrics: {}
counters:
  num_agent_steps_sampled: 386
  num_agent_steps_trained: 386
  num_env_steps_sampled: 386
  num_env_steps_trained: 386
custom_metrics: {}
date: 2023-10-15_08-09-00
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6575643877188364
        entropy_coeff: 0.0
        grad_gnorm: 10.010637708504994
        kl: 0.016744421412719628
        policy_loss: -0.2586823066075643
        total_loss: -0.08512347241242726
        vf_explained_var: .nan
        vf_loss: 0.12058739256390254
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11550.5
  num_agent_steps_sampled: 386
  num_agent_steps_trained: 386
  num_env_steps_sampled: 386
  num_env_steps_trained: 386
iterations_since_restore: 193
node_ip: 10.27.41.23
num_agent_steps_sampled: 386
num_agent_steps_trained: 386
num_env_steps_sampled: 386
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016561892451327775
num_env_steps_trained: 386
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016561892451327775
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.529069767441861
  ram_util_percent: 47.45116279069768
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33205.10340023041
time_this_iter_s: 120.75944519042969
time_total_s: 33205.10340023041
timers:
  learn_throughput: 3.894
  learn_time_ms: 513.603
  load_throughput: 6023.702
  load_time_ms: 0.332
  sample_time_ms: 123073.97
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123588.43
timestamp: 1697346540
timesteps_total: 386
training_iteration: 193
trial_id: default

Last checkpoint 192 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000193
server side epoch loop 193
algo.train executed
agent_timesteps_total: 388
connector_metrics: {}
counters:
  num_agent_steps_sampled: 388
  num_agent_steps_trained: 388
  num_env_steps_sampled: 388
  num_env_steps_trained: 388
custom_metrics: {}
date: 2023-10-15_08-11-00
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.692306403319041
        entropy_coeff: 0.0
        grad_gnorm: 9.199751247962316
        kl: 0.011870288318944707
        policy_loss: -0.2445300668478012
        total_loss: -0.10428854326407115
        vf_explained_var: .nan
        vf_loss: 0.10268952801222136
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11610.5
  num_agent_steps_sampled: 388
  num_agent_steps_trained: 388
  num_env_steps_sampled: 388
  num_env_steps_trained: 388
iterations_since_restore: 194
node_ip: 10.27.41.23
num_agent_steps_sampled: 388
num_agent_steps_trained: 388
num_env_steps_sampled: 388
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016569507317213353
num_env_steps_trained: 388
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016569507317213353
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 8.864534883720932
  ram_util_percent: 48.42151162790697
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33325.80736708641
time_this_iter_s: 120.70396685600281
time_total_s: 33325.80736708641
timers:
  learn_throughput: 3.885
  learn_time_ms: 514.814
  load_throughput: 6055.882
  load_time_ms: 0.33
  sample_time_ms: 123074.074
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123589.749
timestamp: 1697346660
timesteps_total: 388
training_iteration: 194
trial_id: default

Last checkpoint 193 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000194
server side epoch loop 194
algo.train executed
agent_timesteps_total: 390
connector_metrics: {}
counters:
  num_agent_steps_sampled: 390
  num_agent_steps_trained: 390
  num_env_steps_sampled: 390
  num_env_steps_trained: 390
custom_metrics: {}
date: 2023-10-15_08-13-01
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.73087384502093
        entropy_coeff: 0.0
        grad_gnorm: 7.682274522384008
        kl: 0.01495832810239032
        policy_loss: -0.24433419505755108
        total_loss: -0.1319963494936625
        vf_explained_var: .nan
        vf_loss: 0.06501674542814725
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11670.5
  num_agent_steps_sampled: 390
  num_agent_steps_trained: 390
  num_env_steps_sampled: 390
  num_env_steps_trained: 390
iterations_since_restore: 195
node_ip: 10.27.41.23
num_agent_steps_sampled: 390
num_agent_steps_trained: 390
num_env_steps_sampled: 390
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016567017165280525
num_env_steps_trained: 390
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016567017165280525
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.894219653179192
  ram_util_percent: 47.08959537572256
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33446.52955722809
time_this_iter_s: 120.72219014167786
time_total_s: 33446.52955722809
timers:
  learn_throughput: 3.881
  learn_time_ms: 515.271
  load_throughput: 5860.013
  load_time_ms: 0.341
  sample_time_ms: 123074.122
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123590.282
timestamp: 1697346781
timesteps_total: 390
training_iteration: 195
trial_id: default

Last checkpoint 194 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000195
server side epoch loop 195
algo.train executed
agent_timesteps_total: 392
connector_metrics: {}
counters:
  num_agent_steps_sampled: 392
  num_agent_steps_trained: 392
  num_env_steps_sampled: 392
  num_env_steps_trained: 392
custom_metrics: {}
date: 2023-10-15_08-15-02
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.8655283013979593
        entropy_coeff: 0.0
        grad_gnorm: 7.572262442111969
        kl: 0.012784378611831926
        policy_loss: -0.21651428639888765
        total_loss: -0.10804732441902161
        vf_explained_var: .nan
        vf_loss: 0.068023220194785
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11730.5
  num_agent_steps_sampled: 392
  num_agent_steps_trained: 392
  num_env_steps_sampled: 392
  num_env_steps_trained: 392
iterations_since_restore: 196
node_ip: 10.27.41.23
num_agent_steps_sampled: 392
num_agent_steps_trained: 392
num_env_steps_sampled: 392
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016562481604129486
num_env_steps_trained: 392
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016562481604129486
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.396511627906976
  ram_util_percent: 48.004651162790694
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33567.284695863724
time_this_iter_s: 120.75513863563538
time_total_s: 33567.284695863724
timers:
  learn_throughput: 3.807
  learn_time_ms: 525.409
  load_throughput: 5768.141
  load_time_ms: 0.347
  sample_time_ms: 123074.452
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123600.755
timestamp: 1697346902
timesteps_total: 392
training_iteration: 196
trial_id: default

Last checkpoint 195 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000196
server side epoch loop 196
algo.train executed
agent_timesteps_total: 394
connector_metrics: {}
counters:
  num_agent_steps_sampled: 394
  num_agent_steps_trained: 394
  num_env_steps_sampled: 394
  num_env_steps_trained: 394
custom_metrics: {}
date: 2023-10-15_08-17-03
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.6842753609021506
        entropy_coeff: 0.0
        grad_gnorm: 7.436249669392904
        kl: 0.0062049496153425325
        policy_loss: -0.11645262440045674
        total_loss: -0.06408634285132091
        vf_explained_var: .nan
        vf_loss: 0.03273674482285666
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11790.5
  num_agent_steps_sampled: 394
  num_agent_steps_trained: 394
  num_env_steps_sampled: 394
  num_env_steps_trained: 394
iterations_since_restore: 197
node_ip: 10.27.41.23
num_agent_steps_sampled: 394
num_agent_steps_trained: 394
num_env_steps_sampled: 394
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016552285437278
num_env_steps_trained: 394
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016552285437278
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 10.917441860465114
  ram_util_percent: 47.77151162790697
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33688.11423134804
time_this_iter_s: 120.82953548431396
time_total_s: 33688.11423134804
timers:
  learn_throughput: 3.696
  learn_time_ms: 541.098
  load_throughput: 5735.8
  load_time_ms: 0.349
  sample_time_ms: 123071.894
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123613.893
timestamp: 1697347023
timesteps_total: 394
training_iteration: 197
trial_id: default

Last checkpoint 196 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000197
server side epoch loop 197
algo.train executed
agent_timesteps_total: 396
connector_metrics: {}
counters:
  num_agent_steps_sampled: 396
  num_agent_steps_trained: 396
  num_env_steps_sampled: 396
  num_env_steps_trained: 396
custom_metrics: {}
date: 2023-10-15_08-19-03
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.5532389342784882
        entropy_coeff: 0.0
        grad_gnorm: 4.28409602244695
        kl: 0.0114694597577909
        policy_loss: -0.2291275312503179
        total_loss: -0.18741381963094075
        vf_explained_var: .nan
        vf_loss: 0.005429744421659658
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11850.5
  num_agent_steps_sampled: 396
  num_agent_steps_trained: 396
  num_env_steps_sampled: 396
  num_env_steps_trained: 396
iterations_since_restore: 198
node_ip: 10.27.41.23
num_agent_steps_sampled: 396
num_agent_steps_trained: 396
num_env_steps_sampled: 396
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016617042334888924
num_env_steps_trained: 396
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016617042334888924
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.33313953488372
  ram_util_percent: 47.48139534883722
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33808.47289156914
time_this_iter_s: 120.35866022109985
time_total_s: 33808.47289156914
timers:
  learn_throughput: 3.677
  learn_time_ms: 543.94
  load_throughput: 5412.005
  load_time_ms: 0.37
  sample_time_ms: 122991.628
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 123536.508
timestamp: 1697347143
timesteps_total: 396
training_iteration: 198
trial_id: default

Last checkpoint 197 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000198
server side epoch loop 198
algo.train executed
agent_timesteps_total: 398
connector_metrics: {}
counters:
  num_agent_steps_sampled: 398
  num_agent_steps_trained: 398
  num_env_steps_sampled: 398
  num_env_steps_trained: 398
custom_metrics: {}
date: 2023-10-15_08-21-04
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.3575348218282064
        entropy_coeff: 0.0
        grad_gnorm: 7.555410206317902
        kl: 0.012711948547075736
        policy_loss: -0.1846157987912496
        total_loss: -0.10222627719243367
        vf_explained_var: .nan
        vf_loss: 0.04217490910499085
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11910.5
  num_agent_steps_sampled: 398
  num_agent_steps_trained: 398
  num_env_steps_sampled: 398
  num_env_steps_trained: 398
iterations_since_restore: 199
node_ip: 10.27.41.23
num_agent_steps_sampled: 398
num_agent_steps_trained: 398
num_env_steps_sampled: 398
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.01659011408689755
num_env_steps_trained: 398
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.01659011408689755
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.11046511627907
  ram_util_percent: 46.68430232558139
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 33929.02691698074
time_this_iter_s: 120.55402541160583
time_total_s: 33929.02691698074
timers:
  learn_throughput: 3.726
  learn_time_ms: 536.728
  load_throughput: 5377.313
  load_time_ms: 0.372
  sample_time_ms: 121899.766
  synch_weights_time_ms: 0.005
  training_iteration_time_ms: 122437.433
timestamp: 1697347264
timesteps_total: 398
training_iteration: 199
trial_id: default

Last checkpoint 198 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000199
server side epoch loop 199
algo.train executed
agent_timesteps_total: 400
connector_metrics: {}
counters:
  num_agent_steps_sampled: 400
  num_agent_steps_trained: 400
  num_env_steps_sampled: 400
  num_env_steps_trained: 400
custom_metrics: {}
date: 2023-10-15_08-23-04
done: false
episode_len_mean: 127.66666666666667
episode_media: {}
episode_reward_max: 103.9763290634921
episode_reward_mean: 86.89556692063496
episode_reward_min: 70.73622250000007
episodes_this_iter: 0
episodes_total: 3
hostname: rancher-rd-bastion
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163528152734759
        cur_lr: 5.0000000000000016e-05
        entropy: 1.3970204492410023
        entropy_coeff: 0.0
        grad_gnorm: 4.218932364384333
        kl: 0.014874419820747184
        policy_loss: -0.25085959732532503
        total_loss: -0.1978437413771947
        vf_explained_var: .nan
        vf_loss: 0.005960205125302309
      model: {}
      num_agent_steps_trained: 1.0
      num_grad_updates_lifetime: 11970.5
  num_agent_steps_sampled: 400
  num_agent_steps_trained: 400
  num_env_steps_sampled: 400
  num_env_steps_trained: 400
iterations_since_restore: 200
node_ip: 10.27.41.23
num_agent_steps_sampled: 400
num_agent_steps_trained: 400
num_env_steps_sampled: 400
num_env_steps_sampled_this_iter: 2
num_env_steps_sampled_throughput_per_sec: 0.016583304141182927
num_env_steps_trained: 400
num_env_steps_trained_this_iter: 2
num_env_steps_trained_throughput_per_sec: 0.016583304141182927
num_faulty_episodes: 0
num_healthy_workers: 0
num_in_flight_async_reqs: 0
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2
perf:
  cpu_util_percent: 9.740697674418605
  ram_util_percent: 47.77325581395347
pid: 3910393
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.17165081248111794
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 88090.60844950099
  mean_inference_ms: 3.0422335315385314
  mean_raw_obs_processing_ms: 1.7720006610594279
sampler_results:
  connector_metrics: {}
  custom_metrics: {}
  episode_len_mean: 127.66666666666667
  episode_media: {}
  episode_reward_max: 103.9763290634921
  episode_reward_mean: 86.89556692063496
  episode_reward_min: 70.73622250000007
  episodes_this_iter: 0
  hist_stats:
    episode_lengths:
    - 127
    - 128
    - 128
    episode_reward:
    - 85.9741491984127
    - 103.9763290634921
    - 70.73622250000007
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17165081248111794
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 88090.60844950099
    mean_inference_ms: 3.0422335315385314
    mean_raw_obs_processing_ms: 1.7720006610594279
time_since_restore: 34049.630472421646
time_this_iter_s: 120.60355544090271
time_total_s: 34049.630472421646
timers:
  learn_throughput: 3.76
  learn_time_ms: 531.901
  load_throughput: 5462.045
  load_time_ms: 0.366
  sample_time_ms: 120167.491
  synch_weights_time_ms: 0.004
  training_iteration_time_ms: 120700.319
timestamp: 1697347384
timesteps_total: 400
training_iteration: 200
trial_id: default

Last checkpoint 199 /root/ray_results/PPO_None_2023-10-14_22-55-28jzr5oilz/checkpoint_000200
server side epoch loop 200
