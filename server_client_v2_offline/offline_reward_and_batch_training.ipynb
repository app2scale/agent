{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "ALPHA = 0.8\n",
    "eps = 1e-4\n",
    "data = pd.read_csv(\"/Users/hasan.nayir/Projects/Payten/app2scale_reinforcement_learning/server_client_v4/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward\n",
    "\n",
    "ALPHA = 0.8\n",
    "\n",
    "data[\"utilization\"] = np.minimum(data[\"cpu_usage\"]/(data[\"cpu\"]/10),1)\n",
    "data[\"performance_request\"] = np.minimum(round(data['num_request'] /  (data['expected_tps']),6),1)\n",
    "\n",
    "for i in range(0, data.shape[0]):\n",
    "    temp = data.iloc[i,:]\n",
    "    if temp[\"response_time\"] >= 20:\n",
    "        data.loc[i, \"performance_response\"] = 20/temp[\"response_time\"]\n",
    "    else:\n",
    "        data.loc[i, \"performance_response\"] = 1\n",
    "\n",
    "data[\"performance\"] = 0.5*data[\"performance_request\"] + 0.5*data[\"performance_response\"]\n",
    "data[\"reward\"] = data[\"performance\"] * ALPHA + data[\"utilization\"] * (1-ALPHA)\n",
    "\n",
    "\n",
    "drop_rows = (data[\"cpu_usage\"] != 0) | (data[\"memory_usage\"] != 0)\n",
    "hybrid_data = data[drop_rows].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import ray._private.utils\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder\n",
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def convert_data_to_batch(df, writer, eps_id_list):\n",
    "    number_episodes = int(df.shape[0]/episode_length)\n",
    "    remained_steps = df.shape[0]-number_episodes*episode_length\n",
    "    for eps_id,idx in zip(eps_id_list, range(len(eps_id_list))):\n",
    "        print(eps_id)\n",
    "        try:\n",
    "            data = df.iloc[idx*episode_length:(idx+1)*episode_length]\n",
    "        except:\n",
    "            data = df.iloc[idx*episode_length:(idx+1)*remained_steps]\n",
    "\n",
    "        first_row = data.iloc[0,:]\n",
    "        obs = np.array([first_row['replica'], first_row['cpu'], first_row['heap'], first_row[\"cpu_usage\"],first_row[\"memory_usage\"],first_row[\"response_time\"], first_row[\"expected_tps\"]], dtype=np.float32)\n",
    "        info = {}\n",
    "        possible_state_value = np.array([first_row['replica'], first_row['cpu'], first_row['heap']])\n",
    "        equal_rows = np.all(POSSIBLE_STATES == possible_state_value, axis=1)\n",
    "        prev_action = np.where(equal_rows)[0][0]\n",
    "        prev_reward = 0\n",
    "        terminated = truncated = False\n",
    "        # print(\"aaa\",data.shape[0])\n",
    "        for i in range(0, data.shape[0]):\n",
    "            truncated = True if i == data.shape[0]-1 else False\n",
    "            terminated = truncated\n",
    "            selected_row = data.iloc[i,:]\n",
    "            possible_state_value = np.array([selected_row['replica'], selected_row['cpu'], selected_row['heap']])\n",
    "            equal_rows = np.all(POSSIBLE_STATES == possible_state_value, axis=1)\n",
    "            action = np.where(equal_rows)[0][0]\n",
    "            new_obs = np.array([selected_row['replica'], selected_row['cpu'], selected_row['heap'], selected_row[\"cpu_usage\"],selected_row[\"memory_usage\"],selected_row[\"response_time\"], selected_row[\"expected_tps\"]], dtype=np.float32)\n",
    "            rew = selected_row[\"reward\"]\n",
    "            obs = obs/np.array([3,9,9,0.899,1.094, 835.78, 168])\n",
    "            new_obs = new_obs/np.array([3,9,9,0.899,1.094, 835.78, 168])\n",
    "\n",
    "            batch_builder.add_values(\n",
    "                t=i,\n",
    "                eps_id=eps_id,\n",
    "                agent_index=0,\n",
    "                # obs=prep.transform(obs),\n",
    "                obs = obs,\n",
    "                actions=action,\n",
    "                action_prob=1.0, \n",
    "                action_logp=0.0,\n",
    "                rewards=rew,\n",
    "                prev_actions=prev_action,\n",
    "                prev_rewards=prev_reward,\n",
    "                terminateds=terminated,\n",
    "                truncateds=truncated,\n",
    "                infos=info,\n",
    "                # new_obs=prep.transform(new_obs),\n",
    "                new_obs = new_obs,\n",
    "\n",
    "            )\n",
    "            obs = new_obs\n",
    "            prev_action = action\n",
    "            prev_reward = rew\n",
    "        writer.write(batch_builder.build_and_reset())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_builder = SampleBatchBuilder()\n",
    "    training_writer = JsonWriter(\n",
    "        os.path.join(ray._private.utils.get_user_temp_dir(), \"training-out\")\n",
    "    )\n",
    "    eval_writer = JsonWriter(\n",
    "        os.path.join(ray._private.utils.get_user_temp_dir(), \"eval-out\")\n",
    "    )\n",
    "    \n",
    "    action_space = Discrete(108) #6*6*6\n",
    "    observation_space = Box(low=np.array([1, 4, 4, 0,0,0,0]), high=np.array([3, 9, 9, 2,2, 1000,1000]), dtype=np.float32)\n",
    "    replica = [1, 2, 3]\n",
    "    cpu = [4, 5, 6, 7, 8, 9]\n",
    "    heap = [4, 5, 6, 7, 8, 9]\n",
    "    POSSIBLE_STATES = np.array(list(product(replica, cpu, heap)))\n",
    "\n",
    "\n",
    "    prep = get_preprocessor(observation_space)(observation_space)\n",
    "    print(\"The preprocessor is\", prep)\n",
    "    episode_length = 100\n",
    "    full_data = hybrid_data\n",
    "    training_split_ratio = 0.8\n",
    "    train_df = full_data.sample(frac=training_split_ratio, random_state=42)  # EÄŸitim verisi\n",
    "    eval_df = full_data.drop(train_df.index) \n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    eval_df = eval_df.reset_index(drop=True)\n",
    "\n",
    "    number_episodes_training = int(train_df.shape[0]/episode_length)\n",
    "    remained_steps_training = train_df.shape[0]-number_episodes_training*episode_length\n",
    "    if remained_steps_training > 0:\n",
    "        number_episodes_training += 1\n",
    "\n",
    "    # episode_length_eval = eval_df.shape[0]\n",
    "    number_episodes_eval= int(eval_df.shape[0]/episode_length)\n",
    "    remained_steps_eval = eval_df.shape[0]-number_episodes_eval*episode_length\n",
    "    if remained_steps_eval > 0:\n",
    "        number_episodes_eval += 1\n",
    "        \n",
    "    eps_id_training = list(range(number_episodes_training))\n",
    "    eps_id_eval = list(range(len(eps_id_training), number_episodes_eval+ len(eps_id_training)))\n",
    "    convert_data_to_batch(train_df, training_writer, eps_id_training)\n",
    "    convert_data_to_batch(eval_df, eval_writer, eps_id_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.offline.estimators import ImportanceSampling\n",
    "from ray.rllib.offline.estimators.fqe_torch_model import FQETorchModel\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "from ray.rllib.offline.estimators import ImportanceSampling, WeightedImportanceSampling\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "\n",
    "def generate_config(train_path, eval_path, hyper_params):\n",
    "    learning_rate = hyper_params[0]\n",
    "    fcnet_hiddens = hyper_params[1]\n",
    "    config = (\n",
    "        DQNConfig()\n",
    "        .environment(env=None,\n",
    "                    action_space=Discrete(108), \n",
    "                    observation_space=Box(low=np.array([1, 4, 4, 0,0,0,0]), high=np.array([3, 9, 9, 2,2, 1000,1000]), dtype=np.float32)\n",
    "                    )\n",
    "        .training(model={\"fcnet_hiddens\": fcnet_hiddens},\n",
    "                  gamma=0,\n",
    "                  lr=learning_rate,\n",
    "                  train_batch_size=256\n",
    "        )\n",
    "                \n",
    "        .offline_data(input_=train_path)\n",
    "        .exploration(explore=False)\n",
    "        .evaluation(evaluation_parallel_to_training=False,\n",
    "                    evaluation_interval=2,\n",
    "                    evaluation_duration=10,\n",
    "                    evaluation_duration_unit=\"episodes\",\n",
    "                    evaluation_config={\"input\": eval_path},\n",
    "                    off_policy_estimation_methods={\"is\": {\"type\": ImportanceSampling},\n",
    "                                                   \"wis\": {\"type\": WeightedImportanceSampling}\n",
    "                    }\n",
    "        )\n",
    "                    \n",
    "\n",
    "    )\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "def generate_plots(filename,  mean_q_list, is_v_gain_list,wis_v_gain_list):\n",
    "    fig, ax = plt.subplots(nrows=len(inspect.signature(generate_plots).parameters)-1,ncols=1,figsize=(12,36))\n",
    "    ax[0].plot(mean_q_list)\n",
    "    ax[0].set_xlabel('step')\n",
    "    ax[0].set_ylabel('mean_q')\n",
    "    ax[1].plot(is_v_gain_list)\n",
    "    ax[1].set_xlabel('step')\n",
    "    ax[1].set_ylabel('is_v_gain')\n",
    "    ax[2].plot(wis_v_gain_list)\n",
    "    ax[2].set_xlabel('step')\n",
    "    ax[2].set_ylabel('wis_v_gain')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "hyperparameters = {\"learning_rate\": [1e-05],\n",
    "                   \"fcnet_hiddens\": [[64,64]]\n",
    "                   }\n",
    "parameter_combinations = list(product(*hyperparameters.values())) # This variable includes all combinations of the hyperparameters. ex. (1e-05, [32, 32])\n",
    "\n",
    "\n",
    "train_path = \"/tmp/training-out\"\n",
    "eval_path = \"/tmp/eval-out\"\n",
    "epoch_number = 10000\n",
    "\n",
    "for comb in parameter_combinations:\n",
    "    config = generate_config(train_path, eval_path, comb)\n",
    "    print(f\"Started training with lr: {comb[0]} and fcnet: {comb[1]}\")\n",
    "    mean_q_list = []\n",
    "    is_v_gain_list = []\n",
    "    wis_v_gain_list = []\n",
    "\n",
    "    # config['observation_filter'] = \"MeanStdFilter\"\n",
    "    algo = config.build()\n",
    "    debug_dir = \"{}checkpoints/\".format(algo.logdir)\n",
    "    filename = f\"./server_client_v2_offline/results/{algo.logdir.split('/')[-2]}_lr_{comb[0]}_fcnet_{comb[1][0]}_{comb[1][1]}.pdf\"\n",
    "    for i in range(epoch_number):\n",
    "        print(\"------------- Iteration\", i+1, \"-------------\")\n",
    "        results = algo.train()\n",
    "        print(\"timesteps_total:\", results['timesteps_total'])\n",
    "        print(\"training_iteration_time_ms:\", results['timers']['training_iteration_time_ms'])\n",
    "        if 'evaluation' in results.keys():\n",
    "            print(\"== Evaluation ==\")\n",
    "            if 'off_policy_estimator' in results['evaluation'].keys():\n",
    "                print(results['evaluation']['off_policy_estimator'])\n",
    "                is_v_gain_list.append(results['evaluation']['off_policy_estimator'][\"is\"][\"v_gain\"])\n",
    "                wis_v_gain_list.append(results['evaluation']['off_policy_estimator'][\"wis\"][\"v_gain\"])\n",
    "\n",
    "\n",
    "        if (i+1) % 5000 == 0:\n",
    "            ma_checkpoint_dir = algo.save(checkpoint_dir=debug_dir)\n",
    "            print(\n",
    "                \"An Algorithm checkpoint has been created inside directory: \"\n",
    "                f\"'{ma_checkpoint_dir}'\"\n",
    "            )\n",
    "        mean_q_list.append(results[\"info\"][\"learner\"][\"default_policy\"][\"learner_stats\"][\"mean_q\"])\n",
    "\n",
    "    algo.stop()\n",
    "    generate_plots(filename, mean_q_list,is_v_gain_list,wis_v_gain_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan.nayir/miniconda3/envs/rl_lib/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/hasan.nayir/miniconda3/envs/rl_lib/lib/python3.9/site-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "2024-02-28 00:00:33,206\tINFO policy.py:1285 -- Policy (worker=local) running on CPU.\n",
      "2024-02-28 00:00:33,209\tINFO torch_policy.py:184 -- Found 0 visible cuda devices.\n",
      "2024-02-28 00:00:33,216\tINFO util.py:118 -- Using connectors:\n",
      "2024-02-28 00:00:33,217\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2024-02-28 00:00:33,217\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2024-02-28 00:00:33,217\tINFO rollout_worker.py:2000 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2024-02-28 00:00:33,217\tINFO rollout_worker.py:2001 -- Built preprocessor map: {'default_policy': None}\n",
      "2024-02-28 00:00:33,218\tINFO rollout_worker.py:761 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "Install gputil for GPU system monitoring.\n",
      "2024-02-28 00:00:33,223\tWARNING checkpoints.py:109 -- No `rllib_checkpoint.json` file found in checkpoint directory /Users/hasan.nayir/ray_results/DQN_None_2024-02-27_22-50-28ooarimlb/checkpoints/checkpoint_010000/.! Trying to extract checkpoint info from other files found in that dir.\n",
      "2024-02-28 00:00:38,910\tINFO worker.py:1625 -- Started a local Ray instance.\n",
      "2024-02-28 00:00:41,842\tINFO trainable.py:913 -- Restored on 127.0.0.1 from checkpoint: /Users/hasan.nayir/ray_results/DQN_None_2024-02-27_22-50-28ooarimlb/checkpoints/checkpoint_010000\n",
      "2024-02-28 00:00:41,847\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 10000, '_timesteps_total': None, '_time_total': 3725.153131008148, '_episodes_total': 0}\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "from gevent import monkey\n",
    "monkey.patch_all(thread=False, select=False)\n",
    "from locust.env import Environment\n",
    "from kubernetes import client, config\n",
    "\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "\n",
    "from ray.rllib.env.policy_client import PolicyClient\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from gymnasium.spaces import Discrete, Dict, MultiDiscrete, Tuple, Box\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "import ssl\n",
    "import random\n",
    "import logging\n",
    "import ray\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.env.policy_server_input import PolicyServerInput\n",
    "from locust import HttpUser, task, constant, constant_throughput, events\n",
    "from locust.shape import LoadTestShape\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "\n",
    "DEPLOYMENT_NAME = \"teastore-webui\"\n",
    "NAMESPACE = \"app2scale-test\"\n",
    "\n",
    "OBSERVATION_SPACE =Box(low=np.array([1, 4, 4, 0,0,0,0]), high=np.array([3, 9, 9, 2,2, 1000,1000]), dtype=np.float32)\n",
    "\n",
    "\"\"\"\n",
    "    # replica : 1,2,3,4,5,6 -> 0,1,2,3,4,5 + 1\n",
    "    # cpu : 4,5,6,7,8,9 -> 0,1,2,3,4,5   +   4\n",
    "    # heap : 4,5,6,7,8,9 -> 0,1,2,3,4,5   +   4\n",
    "\"\"\"\n",
    "\n",
    "ACTION_SPACE = Discrete(108) # index of the possible states\n",
    "replica = [1, 2, 3]\n",
    "cpu = [4, 5, 6, 7, 8, 9]\n",
    "heap = [4, 5, 6, 7, 8, 9]\n",
    "\n",
    "config_dqn = (DQNConfig()\n",
    "          .environment(\n",
    "              env=None,\n",
    "              action_space=ACTION_SPACE,\n",
    "              observation_space=OBSERVATION_SPACE)\n",
    "\n",
    "          .training(model={\"fcnet_hiddens\": [64,64]},\n",
    "              gamma=0.99,\n",
    "              lr=1e-05,\n",
    "              train_batch_size=256)\n",
    "\n",
    "          .debugging(log_level=\"INFO\")\n",
    "          .evaluation(off_policy_estimation_methods={})\n",
    "          \n",
    "          )\n",
    "\n",
    "\n",
    "config_dqn.rl_module(_enable_rl_module_api=False)\n",
    "config_dqn.training(_enable_learner_api=False)\n",
    "algo = config_dqn.build() \n",
    "\n",
    "\n",
    "path_to_checkpoint = \"/Users/hasan.nayir/ray_results/DQN_None_2024-02-27_22-50-28ooarimlb/checkpoints/checkpoint_010000\"\n",
    "algo.restore(path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.4114044,   5.6831293,   6.913543 ,   1.9389706,   1.8907213,\n",
       "       506.15012  , 560.51086  ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBSERVATION_SPACE.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obs = np.array([first_row['replica'], first_row['cpu'], first_row['heap'], first_row[\"cpu_usage\"],first_row[\"memory_usage\"],first_row[\"response_time\"], first_row[\"expected_tps\"]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "[3 7 6]\n",
      "60\n",
      "[2 8 4]\n",
      "42\n",
      "[2 5 4]\n",
      "85\n",
      "[3 6 5]\n",
      "42\n",
      "[2 5 4]\n",
      "0\n",
      "[1 4 4]\n",
      "42\n",
      "[2 5 4]\n",
      "92\n",
      "[3 7 6]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "93\n",
      "[3 7 7]\n",
      "92\n",
      "[3 7 6]\n",
      "0\n",
      "[1 4 4]\n",
      "60\n",
      "[2 8 4]\n",
      "85\n",
      "[3 6 5]\n",
      "85\n",
      "[3 6 5]\n",
      "85\n",
      "[3 6 5]\n",
      "26\n",
      "[1 8 6]\n",
      "26\n",
      "[1 8 6]\n",
      "26\n",
      "[1 8 6]\n",
      "93\n",
      "[3 7 7]\n",
      "92\n",
      "[3 7 6]\n",
      "60\n",
      "[2 8 4]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "69\n",
      "[2 9 7]\n",
      "42\n",
      "[2 5 4]\n",
      "60\n",
      "[2 8 4]\n",
      "7\n",
      "[1 5 5]\n",
      "85\n",
      "[3 6 5]\n",
      "85\n",
      "[3 6 5]\n",
      "42\n",
      "[2 5 4]\n",
      "60\n",
      "[2 8 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "93\n",
      "[3 7 7]\n",
      "42\n",
      "[2 5 4]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "60\n",
      "[2 8 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "85\n",
      "[3 6 5]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "92\n",
      "[3 7 6]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "60\n",
      "[2 8 4]\n",
      "85\n",
      "[3 6 5]\n",
      "26\n",
      "[1 8 6]\n",
      "60\n",
      "[2 8 4]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "60\n",
      "[2 8 4]\n",
      "0\n",
      "[1 4 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "85\n",
      "[3 6 5]\n",
      "42\n",
      "[2 5 4]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "69\n",
      "[2 9 7]\n",
      "69\n",
      "[2 9 7]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "60\n",
      "[2 8 4]\n",
      "85\n",
      "[3 6 5]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n",
      "42\n",
      "[2 5 4]\n",
      "93\n",
      "[3 7 7]\n",
      "69\n",
      "[2 9 7]\n",
      "60\n",
      "[2 8 4]\n",
      "42\n",
      "[2 5 4]\n",
      "85\n",
      "[3 6 5]\n",
      "92\n",
      "[3 7 6]\n",
      "42\n",
      "[2 5 4]\n"
     ]
    }
   ],
   "source": [
    "ACTION_SPACE = Discrete(108) # index of the possible states\n",
    "replica = [1, 2, 3]\n",
    "cpu = [4, 5, 6, 7, 8, 9]\n",
    "heap = [4, 5, 6, 7, 8, 9]\n",
    "\n",
    "POSSIBLE_STATES = np.array(list(product(replica, cpu, heap)))\n",
    "\n",
    "\n",
    "test_step = 100\n",
    "for i in range(test_step):\n",
    "    obs = OBSERVATION_SPACE.sample()/np.array([3,9,9,0.899,1.094, 835.78, 168])\n",
    "    action = algo.compute_single_action(obs)\n",
    "    print(action)\n",
    "    print(POSSIBLE_STATES[action])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
