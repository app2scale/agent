{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided script defines a custom dataset class named OfflineDataset designed for handling offline data. \n",
    "\n",
    "- The script reads data from multiple CSV files, concatenates them into a single pandas DataFrame, and performs preprocessing steps.\n",
    "- Preprocessing includes filtering out rows with non-positive 'memory_usage' and 'cpu_usage', and computing the logarithm of 'response_time', storing it as 'response_time_log'.\n",
    "- Replica, cpu and heap features are discrete values indicating the type or configuration of the server being used\n",
    "- The feature 'expected_tps' represents the expected number of transactions per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.df = self.read_data()\n",
    "        self.replica_set = pd.unique(self.df['replica'])\n",
    "        self.cpu_set = pd.unique(self.df['cpu'])\n",
    "        self.heap_set = pd.unique(self.df['heap'])\n",
    "        self.cpu_min = self.df['cpu'].min()\n",
    "        self.heap_min = self.df['heap'].min()\n",
    "        self.replica_min = self.df['replica'].min()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Related to actions\n",
    "        x_1 = torch.tensor([row['replica']])\n",
    "        x_2 = torch.tensor([row['cpu']])\n",
    "        x_3 = torch.tensor([row['heap']])\n",
    "        \n",
    "        # Related to incoming load\n",
    "        x_4 = torch.tensor([row['expected_tps']])\n",
    "        \n",
    "        # Target metrics\n",
    "        y_1 = torch.tensor([row['response_time_log']])\n",
    "        y_2 = torch.tensor([row['num_request']])\n",
    "        y_3 = torch.tensor([row['cpu_usage']])\n",
    "        y_4 = torch.tensor([row['memory_usage']])\n",
    "\n",
    "        input = torch.concat([x_1, x_2, x_3, x_4]).to(torch.float)\n",
    "        output = torch.concat([y_1, y_2, y_3, y_4]).to(torch.float)\n",
    "        \n",
    "        return input, output\n",
    "\n",
    "    def getshapes(self):\n",
    "        x, y = self.__getitem__(0)\n",
    "        input_size = (len(self.df), len(x))\n",
    "        output_size = (len(self.df), len(y))\n",
    "        return input_size, output_size\n",
    "    \n",
    "    def getoutputnames(self):\n",
    "        return ['response_time_log', 'num_request','cpu_usage','memory_usage']\n",
    "    \n",
    "    def read_data(self) -> pd.DataFrame:\n",
    "        df = pd.read_csv('server_client_v4/output_300_1.csv')\n",
    "        for i in ['2','3','4']:\n",
    "            df_tmp = pd.read_csv(f'server_client_v4/output_300_{i}.csv')\n",
    "            df = pd.concat([df, df_tmp])\n",
    "\n",
    "        for i in ['1','2','3']:\n",
    "            df_tmp = pd.read_csv(f'server_client_v4/output_browse_300_data_{i}.csv')\n",
    "            df = pd.concat([df, df_tmp])\n",
    "\n",
    "\n",
    "        df = df[(df['memory_usage'] > 0) & (df['cpu_usage'] > 0)]\n",
    "        df['response_time_log'] = np.log10(df['response_time'])\n",
    "\n",
    "        #df=(df-df.mean())/df.std()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricEstimator(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, \n",
    "                 loss_fn=torch.nn.functional.mse_loss):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        layer_size = [40,50]\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=in_features)\n",
    "        )\n",
    "\n",
    "        self.layer_0 = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=layer_size[0]),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(num_features=layer_size[0]),\n",
    "        )\n",
    "\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=layer_size[0], out_features=layer_size[1]),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(num_features=layer_size[1]),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=layer_size[-1],out_features=out_features),\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.input_layer(x)\n",
    "        out = self.layer_0(out)\n",
    "        out = self.layer_1(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.empty(0, self.out_features)\n",
    "            targets = torch.empty(predictions.shape)\n",
    "            for x, y in dataloader:\n",
    "                y_pred = self.forward(x)\n",
    "                predictions = torch.cat([predictions, y_pred], dim=0)\n",
    "                targets = torch.cat([targets, y], dim=0)\n",
    "            return predictions, targets\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        with torch.no_grad():\n",
    "            avg_loss = 0\n",
    "            for x, y in dataloader:\n",
    "                y_pred = self.forward(x)\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "                avg_loss += loss.item()\n",
    "            avg_loss = avg_loss / len(dataloader) \n",
    "            return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_loss_fn(output, target):\n",
    "    return torch.mean(torch.abs((target - output) / target))\n",
    "\n",
    "def scatter(pred, target):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "    for ax, i, title in zip(axes.flatten(), range(4), ds.getoutputnames()):\n",
    "        ax.scatter(target[:,i], pred[:,i])\n",
    "        ax.plot(target[:,i], target[:,i])\n",
    "        # if title == 'response_time':\n",
    "            # ax.set_xscale('log')\n",
    "            # ax.set_yscale('log')\n",
    "        ax.set_xlabel('target')\n",
    "        ax.set_ylabel('prediction')\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "trn_ratio = 0.8\n",
    "batch_size_trn = 16\n",
    "batch_size_val = 16\n",
    "max_epoch = 100\n",
    "loss_fn = torch.nn.functional.mse_loss\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "ds = OfflineDataset()\n",
    "trn_size = int(len(ds)*trn_ratio)\n",
    "val_size = len(ds) - trn_size\n",
    "ds_trn, ds_val = torch.utils.data.random_split(ds, [trn_size, val_size])\n",
    "dl_trn = DataLoader(ds_trn, batch_size=batch_size_trn, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=batch_size_val, shuffle=True)\n",
    "\n",
    "x, y = ds.getshapes()\n",
    "_, in_features = x\n",
    "_, out_features = y\n",
    "\n",
    "metric_estimator = MetricEstimator(in_features=in_features, out_features=out_features, loss_fn=loss_fn)\n",
    "optim = torch.optim.Adam(metric_estimator.parameters())\n",
    "\n",
    "\n",
    "epochbar = tqdm(range(max_epoch))\n",
    "for ep in epochbar:\n",
    "    metric_estimator.train()\n",
    "    pbar = tqdm(dl_trn)\n",
    "    for x, y in pbar:\n",
    "        optim.zero_grad()\n",
    "        y_pred = metric_estimator(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    val_loss = metric_estimator.evaluate(dl_val)\n",
    "    epochbar.set_postfix(epoch=ep,loss=loss.item(),val_loss=val_loss)\n",
    "\n",
    "pred_trn, target_trn = metric_estimator.predict(dl_trn)\n",
    "pred_val, target_val = metric_estimator.predict(dl_val)\n",
    "\n",
    "\n",
    "\n",
    "scatter(pred_trn, target_trn)\n",
    "scatter(pred_val, target_val)\n",
    "\n",
    "\n",
    "trn_loss = metric_estimator.evaluate(dl_trn)\n",
    "val_loss = metric_estimator.evaluate(dl_val)\n",
    "print(f'Training   loss: {trn_loss}')\n",
    "print(f'Validation loss: {val_loss}')\n",
    "\n",
    "\n",
    "metric_estimator.eval()\n",
    "df = ds.read_data()\n",
    "\n",
    "def reward(response_time, num_request, cpu, memory, expected_tps, cpu_limit, memory_limit):\n",
    "    #print(response_time, num_request, cpu, memory, expected_tps, cpu_limit, memory_limit)\n",
    "    speed = 1 if response_time < 20 else 20/response_time\n",
    "    performance = 0.5 * min(num_request/expected_tps, 1) + 0.5 * speed \n",
    "    utilization = 0.5 * min(cpu / (cpu_limit / 10), 1) + 0.5 * min(memory / (memory_limit / 10), 1)\n",
    "    #print(performance, utilization)\n",
    "    r = 0.8 * performance  + 0.2 * utilization\n",
    "    return r\n",
    "\n",
    "\n",
    "def policy(expected_tps):\n",
    "    max_reward = -1\n",
    "\n",
    "    for replica in range(1,10): #pd.unique(df['replica']):\n",
    "        for cpu in range(1,10): #pd.unique(df['cpu']):\n",
    "            for heap in range(1,10): #pd.unique(df['heap']):\n",
    "                state = torch.from_numpy(np.array([replica, cpu, heap, expected_tps])).to(torch.float)\n",
    "                estimated_metrics = metric_estimator(state[None, :]).squeeze()\n",
    "                r = reward(estimated_metrics[0], estimated_metrics[1], estimated_metrics[2], estimated_metrics[3], expected_tps, cpu, heap)\n",
    "                if r > max_reward:\n",
    "                    max_reward = r\n",
    "                    best_action = (replica, cpu, heap)\n",
    "                    best_metrics = estimated_metrics\n",
    "\n",
    "                    df_avg = df.query(f'expected_tps == {expected_tps} and replica=={replica} and cpu == {cpu} and heap == {heap}').mean()\n",
    "                    real_metrics = df_avg[ds.getoutputnames()].to_numpy()\n",
    "\n",
    "    return best_action, best_metrics, real_metrics\n",
    "\n",
    "\n",
    "for tps in sorted(pd.unique(df['expected_tps'])):\n",
    "    chosen_action, best_metrics, real_metrics = policy(tps)\n",
    "    print(tps, chosen_action, best_metrics.detach().numpy(), real_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
